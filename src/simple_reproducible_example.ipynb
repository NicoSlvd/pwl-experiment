{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01de14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "016c6199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.498831</td>\n",
       "      <td>0.498391</td>\n",
       "      <td>0.496947</td>\n",
       "      <td>0.496930</td>\n",
       "      <td>-2.108865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.281851</td>\n",
       "      <td>0.285530</td>\n",
       "      <td>0.291431</td>\n",
       "      <td>0.290942</td>\n",
       "      <td>0.352325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-3.035120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.253412</td>\n",
       "      <td>0.245748</td>\n",
       "      <td>0.245258</td>\n",
       "      <td>0.235843</td>\n",
       "      <td>-2.376458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500025</td>\n",
       "      <td>0.500173</td>\n",
       "      <td>0.500075</td>\n",
       "      <td>0.500059</td>\n",
       "      <td>-2.134583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.743091</td>\n",
       "      <td>0.745965</td>\n",
       "      <td>0.747204</td>\n",
       "      <td>0.750282</td>\n",
       "      <td>-1.854362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.998907</td>\n",
       "      <td>0.999978</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>0.999854</td>\n",
       "      <td>-1.129851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f0           f1           f2           f3       choice\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000\n",
       "mean      0.498831     0.498391     0.496947     0.496930    -2.108865\n",
       "std       0.281851     0.285530     0.291431     0.290942     0.352325\n",
       "min       0.000036     0.000184     0.000332     0.000037    -3.035120\n",
       "25%       0.253412     0.245748     0.245258     0.235843    -2.376458\n",
       "50%       0.500025     0.500173     0.500075     0.500059    -2.134583\n",
       "75%       0.743091     0.745965     0.747204     0.750282    -1.854362\n",
       "max       0.998907     0.999978     0.999739     0.999854    -1.129851"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from rumboost.metrics import cross_entropy\n",
    "np.random.seed(0)\n",
    "\n",
    "def compute_levels(sp, betas):\n",
    "    levels = np.zeros(len(sp))\n",
    "    for i in range(len(sp) - 1):\n",
    "        levels[i + 1] = levels[i] + (sp[i + 1] - sp[i]) * betas[i]\n",
    "    return levels\n",
    "\n",
    "\n",
    "def create_pw_linear_feature(x, sp, betas, intercept):\n",
    "    indices = np.searchsorted(sp, x) - 1\n",
    "    levels = compute_levels(sp, betas) + intercept\n",
    "    f_x = levels[indices] + betas[indices] * (x - sp[indices])\n",
    "\n",
    "    return f_x\n",
    "\n",
    "\n",
    "def apply_linear_feature(x_arr, sp, betas, feature_names, intercept):\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            feature_names[i]: np.array(\n",
    "                create_pw_linear_feature(x, sp[i], betas[i], intercept[i])\n",
    "            )\n",
    "            for i, x in enumerate(x_arr.T)\n",
    "        }\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_dataset(x_arr, feature_names):\n",
    "    return pd.DataFrame({feature_names[i]: x_arr[:, i] for i in range(x_arr.shape[1])})\n",
    "\n",
    "\n",
    "def generate_x(n, k, props_low=0.5, cut=0.5, deterministic=False):\n",
    "    n_low = int(props_low * n)\n",
    "    n_high = n - n_low\n",
    "    if deterministic:\n",
    "        x_low = np.linspace([0]*n_utility, [cut]*n_utility, n_low)\n",
    "        x_high = np.linspace([cut]*n_utility, [1]*n_utility, n_high)\n",
    "    else:\n",
    "        x_low = np.random.uniform(0, cut, (n_low, k))\n",
    "        x_high = np.random.uniform(cut, 1, (n_high, k))\n",
    "\n",
    "    return np.concatenate([x_low, x_high], axis=0)\n",
    "\n",
    "\n",
    "def add_choice(row, u_idx, regression=False):\n",
    "    u = np.array([np.sum(row[u_idx[i][0] : u_idx[i][1]]) for i in range(len(u_idx))])\n",
    "    if regression:\n",
    "        return u[0]\n",
    "    else:\n",
    "        return np.random.choice(u.shape[0], size=1, p=softmax(u))[0]\n",
    "\n",
    "n_utility = 1 \n",
    "regression = n_utility == 1\n",
    "f_per_utility = 4\n",
    "x_arr = generate_x(2000, n_utility * f_per_utility, 0.5, 0.5)\n",
    "# x_arr[:, 1] = x_arr[:, 0]\n",
    "# x_arr[:, 2] = x_arr[:, 0]\n",
    "x_arr_test = generate_x(1000, n_utility * f_per_utility, 0.5, 0.5)\n",
    "# x_arr_test[:, 1] = x_arr_test[:, 0]\n",
    "# x_arr_test[:, 2] = x_arr_test[:, 0]\n",
    "sp = np.array(\n",
    "    [\n",
    "        [0, 0.25, 0.5, 0.75, 1],\n",
    "        [0, 0.4, 0.5, 0.6, 1],\n",
    "        [0, 0.3, 0.5, 0.7, 1],\n",
    "        [0, 0.2, 0.4, 0.5, 1],\n",
    "        [0, 0.25, 0.5, 0.75, 1],\n",
    "        [0, 0.4, 0.5, 0.6, 1],\n",
    "        [0, 0.34, 0.5, 0.55, 1],\n",
    "        [0, 0.2, 0.4, 0.8, 1],\n",
    "        [0, 0.1, 0.55, 0.6, 1],\n",
    "        [0, 0.3, 0.5, 0.7, 1],\n",
    "        [0, 0.25, 0.5, 0.75, 1],\n",
    "        [0, 0.4, 0.5, 0.6, 1],\n",
    "        [0, 0.3, 0.5, 0.7, 1],\n",
    "        [0, 0.2, 0.4, 0.5, 1],\n",
    "        [0, 0.25, 0.5, 0.75, 1],\n",
    "        [0, 0.4, 0.5, 0.6, 1],\n",
    "    ]\n",
    ")\n",
    "betas = np.array(\n",
    "    [\n",
    "        [1, 2, 0.5, 1],\n",
    "        [0.5, 0, 0.5, 1],\n",
    "        [-0.5, -0.5, -2, -3],\n",
    "        [0, 0, 0, 0],\n",
    "        [3, -2, 0.5, -1],\n",
    "        [0.5, 0, 0.5, 1],\n",
    "        [-0.5, -0.5, -2, -3],\n",
    "        [0, 1, 1, 0],\n",
    "        [1, -2, 0.5, -1],\n",
    "        [-0.5, 0, 0.5, 1],\n",
    "        [-0.5, -0.5, -2, -3],\n",
    "        [0, 0, 5, 0],\n",
    "        [1, 1, 0.9, -1],\n",
    "        [0.5, -1, 0.5, 1],\n",
    "        [-0.5, -0.5, 2, -3],\n",
    "        [0, -1, 0, 0],\n",
    "    ]\n",
    ")\n",
    "intercept = [-0.5, -1.5, 0.5, -1, 0, -2, 1, -2, 1, -1, -0.5, 0.5, -2, 0, 0, 0.5]\n",
    "\n",
    "ind = [i for i in range(4 * n_utility) if i % 4 < f_per_utility]\n",
    "sp = [sp[i] for i in ind]\n",
    "betas = [betas[i] for i in ind]\n",
    "intercept = [intercept[i] for i in ind]\n",
    "\n",
    "feature_names = [f\"f{i}\" for i in range(f_per_utility*n_utility)]\n",
    "\n",
    "u_idx = [(i * f_per_utility, (i+1) * f_per_utility) for i in range(n_utility)]\n",
    "\n",
    "x_plot = np.linspace(1e-4, 1, 1000).reshape(-1, 1)\n",
    "for i, (sp_i, beta_i, inter_i) in enumerate(zip(sp, betas, intercept)):\n",
    "    y_plot = apply_linear_feature(\n",
    "        x_plot, sp_i.reshape(1, -1), beta_i.reshape(1, -1), feature_names, [inter_i]\n",
    "    ).values\n",
    "    plt.plot(x_plot, y_plot, label=feature_names[i], linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "dataset = create_dataset(x_arr, feature_names)\n",
    "dataset_transf = apply_linear_feature(\n",
    "    x_arr, sp, betas, feature_names, intercept=intercept\n",
    ")\n",
    "dataset[\"choice\"] = dataset_transf.apply(add_choice, axis=1, u_idx=u_idx, regression=regression)\n",
    "dataset_test = create_dataset(x_arr_test, feature_names)\n",
    "dataset_test_transf = apply_linear_feature(\n",
    "    x_arr_test, sp, betas, feature_names, intercept=intercept\n",
    ")\n",
    "dataset_test[\"choice\"] = dataset_test_transf.apply(add_choice, axis=1, u_idx=u_idx, regression=regression)\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade30c7",
   "metadata": {},
   "source": [
    "## Fake gradient boosting iteration with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702987b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def compute_preds(X):\n",
    "    preds = softmax(X, axis=1)\n",
    "    return preds\n",
    "\n",
    "def compute_grads_and_hess(preds, labels, num_classes=3):\n",
    "    grads = preds - labels\n",
    "    factor = num_classes / (num_classes - 1)\n",
    "    hess = factor * preds * (1 - preds)\n",
    "    hess = np.maximum(hess, 1e-6)\n",
    "    return grads, hess\n",
    "\n",
    "def compute_split_gain(grads, hess, split, feature):\n",
    "    left_mask = feature <= split\n",
    "    right_mask = feature > split\n",
    "    left_grads = grads[left_mask]\n",
    "    right_grads = grads[right_mask]\n",
    "    left_hess = hess[left_mask]\n",
    "    right_hess = hess[right_mask]\n",
    "\n",
    "    no_split_gain = np.sum(grads) ** 2 / np.sum(hess)\n",
    "    left_gain = np.sum(left_grads) ** 2 / np.sum(left_hess)\n",
    "    right_gain = np.sum(right_grads) ** 2 / np.sum(right_hess)\n",
    "    bigger_gain_left = left_gain > right_gain\n",
    "\n",
    "    gain = left_gain + right_gain - no_split_gain\n",
    "    return gain, bigger_gain_left\n",
    "\n",
    "def find_best_split(X, grads, hess, linear_trees=False, from_split_point=False):\n",
    "    best_gain = -np.inf\n",
    "    best_split = None\n",
    "    best_feature = None\n",
    "    best_bigger_gain_left = None\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    for feature in range(X.shape[1]):\n",
    "        unique_values = np.unique(X[:, feature])\n",
    "        for split in unique_values:\n",
    "            if linear_trees and from_split_point:\n",
    "                grads_ = grads * (X[:, feature] - split)\n",
    "                hess_ = hess * (X[:, feature] - split)**2\n",
    "            elif linear_trees:\n",
    "                grads_ = grads * (X[:, feature])\n",
    "                hess_ = hess * (X[:, feature])**2\n",
    "            else:\n",
    "                grads_ = grads\n",
    "                hess_ = hess\n",
    "            gain, bigger_gain_left = compute_split_gain(grads_, hess_, split, X[:, feature])\n",
    "            if gain > best_gain:\n",
    "                best_gain = copy.deepcopy(gain)\n",
    "                best_split = copy.deepcopy(split)\n",
    "                best_feature = copy.deepcopy(feature)\n",
    "                best_bigger_gain_left = copy.deepcopy(bigger_gain_left)\n",
    "\n",
    "    return best_feature, best_split, best_gain, best_bigger_gain_left\n",
    "\n",
    "def compute_leaf_value(grads, hess, feature, split, linear_trees=False, from_split_point=False):\n",
    "    no_split_value = - np.sum(grads) / np.sum(hess)\n",
    "    left_mask = feature <= split\n",
    "    right_mask = feature > split\n",
    "\n",
    "    left_grads = grads[left_mask]\n",
    "    right_grads = grads[right_mask]\n",
    "    left_hess = hess[left_mask]\n",
    "    right_hess = hess[right_mask]\n",
    "\n",
    "    left_intercept = - np.sum(left_grads) / np.sum(left_hess)\n",
    "    right_intercept = - np.sum(right_grads) / np.sum(right_hess)\n",
    "\n",
    "    if linear_trees and from_split_point:\n",
    "        grads = grads * (feature - split)\n",
    "        hess = hess * (feature - split)**2\n",
    "    elif linear_trees:\n",
    "        grads = grads * (feature)\n",
    "        hess = hess * (feature)**2\n",
    "\n",
    "    \n",
    "    left_grads = grads[left_mask]\n",
    "    right_grads = grads[right_mask]\n",
    "    left_hess = hess[left_mask]\n",
    "    right_hess = hess[right_mask]\n",
    "\n",
    "    left_value = - np.sum(left_grads) / np.sum(left_hess)\n",
    "    right_value = - np.sum(right_grads) / np.sum(right_hess)\n",
    "\n",
    "    return left_value, right_value, no_split_value, left_intercept, right_intercept\n",
    "\n",
    "def boost(X, y, preds, num_classes=3, linear_trees=False, from_split_point=False):\n",
    "    grads, hess = compute_grads_and_hess(preds, y, num_classes=num_classes)\n",
    "    feature, split, gain, bigger_left = find_best_split(X, grads, hess, linear_trees=linear_trees, from_split_point=from_split_point)\n",
    "    left_value, right_value, no_split_value, left_intercept, right_intercept = compute_leaf_value(grads, hess, X[:, feature].reshape(-1), split, linear_trees=linear_trees, from_split_point=from_split_point)\n",
    "\n",
    "    return feature, split, left_value, right_value, gain, no_split_value, bigger_left, left_intercept, right_intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2de4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, linear_trees=False, from_split_point=False):\n",
    "        self.linear_trees = linear_trees\n",
    "        self.from_split_point = from_split_point\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, preds, num_classes=3):\n",
    "        self.tree = self._build_tree(X, y, preds, num_classes=num_classes)\n",
    "\n",
    "    def _build_tree(self, X, y, preds, num_classes=3):\n",
    "\n",
    "        feature, split, left_value, right_value, gain, no_split_value, bigger_left, left_intercept, right_intercept = boost(X, y, preds, num_classes=num_classes, linear_trees=self.linear_trees, from_split_point=self.from_split_point)\n",
    "        if gain <= 0:\n",
    "            return None\n",
    "        \n",
    "        # if self.from_split_point:\n",
    "        #     if bigger_left:\n",
    "        #         right_value = 0\n",
    "        #     else:\n",
    "        #         left_value = 0\n",
    "\n",
    "        return {\n",
    "            \"feature\": feature,\n",
    "            \"split\": split,\n",
    "            \"left_value\": -left_value if (self.linear_trees and not self.from_split_point) else left_value,\n",
    "            # \"left_value\": left_value,\n",
    "            \"right_value\": right_value,\n",
    "            \"gain\": gain,\n",
    "            \"left_constant\": split * right_value  if not self.from_split_point else None,\n",
    "            # \"left_constant\": split * right_value + no_split_value if not self.from_split_point else None,\n",
    "            \"right_constant\": split * -left_value if not self.from_split_point else None,\n",
    "            # \"right_constant\": split * -left_value + no_split_value if not self.from_split_point else None,\n",
    "            \"left_intercept\": left_intercept,\n",
    "            \"right_intercept\": right_intercept,\n",
    "            \"full_intercept\": no_split_value,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c0f13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self, linear_trees=False, from_split_point=False):\n",
    "        self.linear_trees = linear_trees\n",
    "        self.from_split_point = from_split_point\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y, preds, num_classes=3):\n",
    "        tree = Tree(linear_trees=self.linear_trees, from_split_point=self.from_split_point)\n",
    "        tree.fit(X, y, preds, num_classes=num_classes)\n",
    "        self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0]))\n",
    "        for tree in self.trees:\n",
    "            if tree.tree is not None:\n",
    "                split = tree.tree[\"split\"]\n",
    "                feature = tree.tree[\"feature\"]\n",
    "                left_value = tree.tree[\"left_value\"]\n",
    "                right_value = tree.tree[\"right_value\"]\n",
    "                left_constant = tree.tree[\"left_constant\"]\n",
    "                right_constant = tree.tree[\"right_constant\"]\n",
    "                left_intercept = tree.tree[\"left_intercept\"]\n",
    "                right_intercept = tree.tree[\"right_intercept\"]\n",
    "                full_intercept = tree.tree[\"full_intercept\"]\n",
    "\n",
    "                x_f = X[:, feature]\n",
    "\n",
    "                if self.linear_trees and self.from_split_point:\n",
    "                    preds[x_f <= split] += left_value * (x_f[x_f <= split] - split) + full_intercept\n",
    "                    preds[x_f > split] += right_value * (x_f[x_f > split] - split) + full_intercept\n",
    "                elif self.linear_trees:\n",
    "                    # preds[x_f <= split] += left_value * x_f[x_f <= split] + left_intercept + right_intercept\n",
    "                    preds[x_f <= split] += left_value * x_f[x_f <= split] - right_constant + full_intercept\n",
    "                    # preds[x_f <= split] += left_value * x_f[x_f <= split] \n",
    "                    # preds[x_f > split] += right_value * x_f[x_f > split] + right_intercept + right_intercept\n",
    "                    preds[x_f > split] += right_value * x_f[x_f > split] - left_constant + full_intercept\n",
    "                    # preds = preds - left_constant - right_constant\n",
    "                    # preds[x_f > split] += right_value * x_f[x_f > split] \n",
    "                else:\n",
    "                    preds[x_f <= split] += left_value\n",
    "                    preds[x_f > split] += right_value\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7376bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingModel:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, num_classes=3, linear_trees=False, from_split_point=False, feature_indices=None):\n",
    "        self.linear_trees = linear_trees\n",
    "        self.from_split_point = from_split_point\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.ensemble = [Ensemble(linear_trees=self.linear_trees, from_split_point=self.from_split_point) for _ in range(num_classes)]\n",
    "        self.feature_indices = feature_indices\n",
    "        if self.feature_indices is None:\n",
    "            self.feature_indices = [(i, i+1) for i in range(num_classes)]\n",
    "\n",
    "    def fit(self, X, y, X_test=None, y_test=None):\n",
    "        self.initial_preds = np.zeros((1, self.num_classes))\n",
    "        for j in range(self.num_classes):\n",
    "            self.initial_preds[:, j] = np.log(np.mean(y==j))\n",
    "        raw_preds = np.zeros((X.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "        preds = softmax(raw_preds, axis=1)\n",
    "        print(\"Initial train cel:\", cross_entropy(preds, y))\n",
    "        if X_test is not None and y_test is not None:\n",
    "            test_preds = np.zeros((X_test.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "            test_preds = softmax(test_preds, axis=1)\n",
    "            print(\"Initial test cel:\", cross_entropy(test_preds, y_test))\n",
    "            \n",
    "        for i in range(self.n_estimators):\n",
    "            raw_preds = np.zeros((X.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "            for j in range(self.num_classes):\n",
    "                ensemble = self.ensemble[j]\n",
    "                ensemble.fit(X[:, self.feature_indices[j][0]:self.feature_indices[j][1]], y == j, preds[:, j], num_classes=self.num_classes)\n",
    "                raw_preds[:, j] += self.learning_rate * ensemble.predict(X[:, self.feature_indices[j][0]:self.feature_indices[j][1]])\n",
    "            preds = softmax(raw_preds, axis=1)\n",
    "            print(\"Train cel:\", cross_entropy(preds, y))\n",
    "\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_preds = np.zeros((X_test.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "                for j in range(self.num_classes):\n",
    "                    ensemble = self.ensemble[j]\n",
    "                    test_preds[:, j] += self.learning_rate * ensemble.predict(X_test[:, self.feature_indices[j][0]:self.feature_indices[j][1]])\n",
    "                test_preds = softmax(test_preds, axis=1)\n",
    "                print(\"Test cel:\", cross_entropy(test_preds, y_test))\n",
    "\n",
    "    def predict(self, X, utilities=False):\n",
    "        preds = np.zeros((X.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "\n",
    "        for j in range(self.num_classes):\n",
    "            ensemble = self.ensemble[j]\n",
    "            preds[:, j] += self.learning_rate * ensemble.predict(X[:, self.feature_indices[j][0]:self.feature_indices[j][1]])\n",
    "\n",
    "        return softmax(preds, axis=1) if not utilities else preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "912c3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train cel: 0.8665065688364659\n",
      "Initial test cel: 0.8179526293862074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucesnjs\\AppData\\Local\\Temp\\ipykernel_31572\\4037606457.py:23: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  right_gain = np.sum(right_grads) ** 2 / np.sum(right_hess)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cel: 0.8586408065346729\n",
      "Test cel: 0.8125263353251223\n",
      "Train cel: 0.8523639655593002\n",
      "Test cel: 0.8083794207170204\n",
      "Train cel: 0.847280512675607\n",
      "Test cel: 0.8050015149765648\n",
      "Train cel: 0.8431538809787837\n",
      "Test cel: 0.8023803655439451\n",
      "Train cel: 0.8397119144720798\n",
      "Test cel: 0.7999995552187615\n",
      "Train cel: 0.8368135424462635\n",
      "Test cel: 0.7980951998264676\n",
      "Train cel: 0.8342793483545331\n",
      "Test cel: 0.7966571332645913\n",
      "Train cel: 0.8320779907906701\n",
      "Test cel: 0.7952192486876686\n",
      "Train cel: 0.8300300132646146\n",
      "Test cel: 0.7941294766824756\n",
      "Train cel: 0.8281205559662473\n",
      "Test cel: 0.7927274112202821\n",
      "Train cel: 0.8263153079026708\n",
      "Test cel: 0.7919492960716722\n",
      "Train cel: 0.8245967808901562\n",
      "Test cel: 0.7912053538381256\n",
      "Train cel: 0.8229733320498367\n",
      "Test cel: 0.7901872576378166\n",
      "Train cel: 0.8214634501011021\n",
      "Test cel: 0.7893390767181747\n",
      "Train cel: 0.8200138980747725\n",
      "Test cel: 0.7884761270258568\n",
      "Train cel: 0.8186649711126729\n",
      "Test cel: 0.7882034685279373\n",
      "Train cel: 0.817387553859441\n",
      "Test cel: 0.7874613954666191\n",
      "Train cel: 0.8161505324683526\n",
      "Test cel: 0.7867574857411767\n",
      "Train cel: 0.8150172733234096\n",
      "Test cel: 0.786237687022101\n",
      "Train cel: 0.8139037371963386\n",
      "Test cel: 0.7860134100310141\n",
      "Feature: 0, Split: 0.5291639301601847, Gain: 63.96877004337776, Left value: -0.33154287920277, Right value: 0.3723698154952555\n",
      "Feature: 0, Split: 0.5291639301601847, Gain: 50.933193212984385, Left value: -0.305771131464412, Right value: 0.3215574164302337\n",
      "Feature: 1, Split: 0.5961620698985872, Gain: 41.6220878504838, Left value: -0.24440170546315979, Right value: 0.3294847900301036\n",
      "Feature: 0, Split: 0.43203373416307134, Gain: 33.96524940976863, Left value: -0.3177459999269976, Right value: 0.20649224582653114\n",
      "Feature: 1, Split: 0.5961620698985872, Gain: 28.326968361552527, Left value: -0.21109905097853243, Right value: 0.2607872239001003\n",
      "Feature: 0, Split: 0.43203373416307134, Gain: 23.426771615602895, Left value: -0.2761186732937255, Right value: 0.1642177626278554\n",
      "Feature: 1, Split: 0.5961620698985872, Gain: 19.92610529795469, Left value: -0.18370991453129737, Right value: 0.21184696029669456\n",
      "Feature: 0, Split: 0.4234399319519584, Gain: 16.730684413643463, Left value: -0.2460695638882612, Right value: 0.1317430678562112\n",
      "Feature: 2, Split: 0.863330653458873, Gain: 15.111050051208421, Left value: 0.0715444127868101, Right value: -0.4007926443747122\n",
      "Feature: 1, Split: 0.7204474166810597, Gain: 13.953850876031247, Left value: -0.11510762132976207, Right value: 0.24107834042572512\n",
      "Feature: 2, Split: 0.863330653458873, Gain: 14.365187276211612, Left value: 0.06897252191781478, Right value: -0.3931928425939252\n",
      "Feature: 2, Split: 0.863330653458873, Gain: 12.844241784046005, Left value: 0.0646205952780978, Right value: -0.37550366707811955\n",
      "Feature: 2, Split: 0.863330653458873, Gain: 11.69234592394766, Left value: 0.06074350350523318, Right value: -0.36153808052694153\n",
      "Feature: 0, Split: 0.17573435387374253, Gain: 11.173542607309477, Left value: -0.3635064556468088, Right value: 0.05747134466785537\n",
      "Feature: 2, Split: 0.863330653458873, Gain: 11.124351293066583, Left value: 0.058801902711935326, Right value: -0.35449874605754217\n",
      "Feature: 2, Split: 0.863330653458873, Gain: 10.137567171329506, Left value: 0.0553370212525065, Right value: -0.3413321025856356\n",
      "Feature: 1, Split: 0.7204474166810597, Gain: 9.561166359601293, Left value: -0.09930692490753278, Right value: 0.19429211137494698\n",
      "Feature: 2, Split: 0.863330653458873, Gain: 9.54546815362961, Left value: 0.05347740146652384, Right value: -0.333067838159165\n",
      "Feature: 0, Split: 0.17573435387374253, Gain: 8.72144860493252, Left value: -0.33151017287494583, Right value: 0.0486622395819234\n",
      "Feature: 2, Split: 0.863330653458873, Gain: 9.112000699359951, Left value: 0.05180930378923096, Right value: -0.32700979048870643\n",
      "Feature: 3, Split: 0.8247902960182967, Gain: 14.988891797777507, Left value: -0.09981788037679805, Right value: 0.46894497077304953\n",
      "Feature: 3, Split: 0.8247902960182967, Gain: 12.104774554045948, Left value: -0.09217719304170084, Right value: 0.41019984209924587\n",
      "Feature: 3, Split: 0.8247902960182967, Gain: 9.835377226935268, Left value: -0.08474457453244447, Right value: 0.36163236292599743\n",
      "Feature: 3, Split: 0.8247902960182967, Gain: 8.006428056177409, Left value: -0.07757105490191274, Right value: 0.3202464673987729\n",
      "Feature: 2, Split: 0.8507685106183086, Gain: 6.948917164700491, Left value: 0.06590607910095406, Right value: -0.3366603574672795\n",
      "Feature: 3, Split: 0.8247902960182967, Gain: 6.536115575593388, Left value: -0.07006885278835608, Right value: 0.28578442228756934\n",
      "Feature: 2, Split: 0.8507685106183086, Gain: 6.588297679147128, Left value: 0.06505908372364935, Right value: -0.32927005898983575\n",
      "Feature: 2, Split: 0.026806536806251058, Gain: 6.164955592226057, Left value: 0.824401547831697, Right value: -0.020134961897252085\n",
      "Feature: 2, Split: 0.8507685106183086, Gain: 6.079300488256412, Left value: 0.06343456268147878, Right value: -0.3184025087732664\n",
      "Feature: 2, Split: 0.7916123906457255, Gain: 5.614733437585259, Left value: 0.07428318742856023, Right value: -0.2471073827782931\n",
      "Feature: 3, Split: 0.8247902960182967, Gain: 5.379337819944167, Left value: -0.06224116915125272, Right value: 0.2581631740497744\n",
      "Feature: 2, Split: 0.7916123906457255, Gain: 5.44034215791642, Left value: 0.07333002597245998, Right value: -0.2437718741862914\n",
      "Feature: 2, Split: 0.026806536806251058, Gain: 5.35887271311056, Left value: 0.752796342554221, Right value: -0.018238366227043147\n",
      "Feature: 2, Split: 0.8507685106183086, Gain: 5.245353901050513, Left value: 0.058512795952458445, Right value: -0.30083670335989643\n",
      "Feature: 2, Split: 0.7916123906457255, Gain: 4.822342590215194, Left value: 0.06892644058011275, Right value: -0.23173864903614969\n",
      "Feature: 0, Split: 0.9718229798936142, Gain: 4.58324273047124, Left value: 0.02598503398471427, Right value: -0.6716812432049252\n",
      "Feature: 2, Split: 0.026806536806251058, Gain: 4.633363221593684, Left value: 0.6856746702173139, Right value: -0.016851307046439702\n",
      "Feature: 2, Split: 0.7916123906457255, Gain: 4.605129940751339, Left value: 0.06742378941566601, Right value: -0.22722451231518334\n",
      "Feature: 2, Split: 0.8507685106183086, Gain: 4.289778768489078, Left value: 0.05269584920618507, Right value: -0.27775021538858863\n",
      "Feature: 0, Split: 0.9718229798936142, Gain: 4.266982759909636, Left value: 0.025298955055523247, Right value: -0.6621567455392252\n",
      "Feature: 2, Split: 0.6007646046985831, Gain: 86.89614394011326, Left value: 0.2799836649179436, Right value: -0.45875951428507333\n",
      "Feature: 2, Split: 0.6007646046985831, Gain: 69.30646809175825, Left value: 0.25497580945121684, Right value: -0.40226437442969554\n",
      "Feature: 2, Split: 0.6007646046985831, Gain: 55.45416580447977, Left value: 0.2319781762515578, Right value: -0.354557232939304\n",
      "Feature: 2, Split: 0.6007646046985831, Gain: 44.92055914330929, Left value: 0.21176482130554805, Right value: -0.31548808983749405\n",
      "Feature: 2, Split: 0.6393708394372497, Gain: 36.77604951985246, Left value: 0.17771757680101297, Right value: -0.3088839198546382\n",
      "Feature: 2, Split: 0.7887746201708652, Gain: 30.624237885703383, Left value: 0.11069365159348515, Right value: -0.4151175313605232\n",
      "Feature: 0, Split: 0.6495625629201283, Gain: 26.519447617578106, Left value: 0.15573436931775028, Right value: -0.2555913701711042\n",
      "Feature: 2, Split: 0.6393708394372497, Gain: 23.22159237014253, Left value: 0.1445490028619758, Right value: -0.24178641345778548\n",
      "Feature: 2, Split: 0.8581460991320728, Gain: 20.981722831747266, Left value: 0.07154662497429831, Right value: -0.4437327902458823\n",
      "Feature: 0, Split: 0.3610647747750148, Gain: 20.18608843920712, Left value: 0.2468187007509041, Right value: -0.12323407583014115\n",
      "Feature: 2, Split: 0.8586082338089687, Gain: 17.582448549082503, Left value: 0.06588283391724922, Right value: -0.4062967560907463\n",
      "Feature: 0, Split: 0.6495625629201283, Gain: 16.930384209420346, Left value: 0.12734597194605451, Right value: -0.20165836598094533\n",
      "Feature: 0, Split: 0.3610647747750148, Gain: 16.288431191754423, Left value: 0.22459771537502843, Right value: -0.10966184190805497\n",
      "Feature: 2, Split: 0.7887746201708652, Gain: 14.913932482863144, Left value: 0.08012118209460296, Right value: -0.2844682930492978\n",
      "Feature: 0, Split: 0.3610647747750148, Gain: 13.737952742463941, Left value: 0.20826025492187175, Right value: -0.10001734172221209\n",
      "Feature: 0, Split: 0.6495625629201283, Gain: 12.999630444686215, Left value: 0.11322388157547725, Right value: -0.17535673323956324\n",
      "Feature: 2, Split: 0.8586082338089687, Gain: 12.320425338536344, Left value: 0.056298195365611856, Right value: -0.33828791201953357\n",
      "Feature: 0, Split: 0.3610647747750148, Gain: 11.182192237173975, Left value: 0.18991394828319547, Right value: -0.08961121436978652\n",
      "Feature: 2, Split: 0.6393708394372497, Gain: 10.532746736547569, Left value: 0.10104518302594669, Right value: -0.16004066856698942\n",
      "Feature: 2, Split: 0.8586082338089687, Gain: 9.560279252817791, Left value: 0.04998732875648878, Right value: -0.29754576123786525\n",
      "Cross-entropy loss: 0.7860134100310141\n"
     ]
    }
   ],
   "source": [
    "features = [f for f in dataset.columns if f not in [\"choice\"]]\n",
    "X, y = dataset[features].values, dataset[\"choice\"].values\n",
    "X_test, y_test = dataset_test[features].values, dataset_test[\"choice\"].values\n",
    "\n",
    "feature_indices = [(i * f_per_utility, (i + 1) * f_per_utility) for i in range(n_utility)]\n",
    "\n",
    "model = BoostingModel(n_estimators=20, learning_rate=0.1, feature_indices=feature_indices)\n",
    "\n",
    "model.fit(X, y, X_test=X_test, y_test=y_test)\n",
    "for ensemble in model.ensemble:\n",
    "    for tree in ensemble.trees:\n",
    "        feature, split, left_value, right_value, gain, _, _, _, _, _= tree.tree.values()\n",
    "        print(f\"Feature: {feature}, Split: {split}, Gain: {gain}, Left value: {left_value}, Right value: {right_value}\")\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "cel = cross_entropy(preds, y_test)\n",
    "print(f\"Cross-entropy loss: {cel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1c6af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "md = 1\n",
    "verbose = 2\n",
    "mono = False \n",
    "# monotonicity_0 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1] if mono else [0]*17\n",
    "monotonicity_0 = [0, 1, -1, 0] if mono else [0] * 17\n",
    "# monotonicity_0 = [1] if mono else [0] * 17\n",
    "l1 = 0\n",
    "l2 = 0\n",
    "min_data_in_leaf = 1 \n",
    "min_sum_hessian_in_leaf = 0\n",
    "min_data_in_bin = 1\n",
    "mb = 2000\n",
    "variables_0 = feature_names[:f_per_utility]\n",
    "dico1 = []\n",
    "for i, v in enumerate(variables_0):\n",
    "    dico1.append({\n",
    "        \"utility\": [0],\n",
    "        \"variables\": [v],\n",
    "        \"boosting_params\": {\n",
    "            'boosting': 'gbdt',\n",
    "            'monotone_constraints_method': 'advanced',\n",
    "            \"max_depth\": md,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"verbose\": verbose,\n",
    "            \"monotone_constraints\": [monotonicity_0[i]],\n",
    "            \"lambda_l2\": l2,\n",
    "            \"lambda_l1\": l1,\n",
    "            \"min_data_in_leaf\": min_data_in_leaf,\n",
    "            \"min_sum_hessian_in_leaf\": min_sum_hessian_in_leaf,\n",
    "            \"max_bin\": mb,\n",
    "            \"min_data_in_bin\": min_data_in_bin,\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    })\n",
    "\n",
    "# monotonicity_1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1] if mono else [0]*17\n",
    "monotonicity_1 = [0, 1, -1, 1] if mono else [0]*17\n",
    "# monotonicity_1 = [1] if mono else [0]*17\n",
    "variables_1 = feature_names[f_per_utility:2*f_per_utility]\n",
    "dico2 = []\n",
    "for i, v in enumerate(variables_1):\n",
    "    dico2.append({\n",
    "        \"utility\": [1],\n",
    "        \"variables\": [v],\n",
    "        \"boosting_params\": {\n",
    "            'boosting': 'gbdt',\n",
    "            'monotone_constraints_method': 'advanced',\n",
    "            \"max_depth\": md,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"verbose\": verbose,\n",
    "            \"monotone_constraints\": [monotonicity_1[i]],\n",
    "            \"lambda_l2\": l2,\n",
    "            \"lambda_l1\": l1,\n",
    "            \"min_data_in_leaf\": min_data_in_leaf,\n",
    "            \"min_sum_hessian_in_leaf\": min_sum_hessian_in_leaf,\n",
    "            \"max_bin\": mb,\n",
    "            \"min_data_in_bin\": min_data_in_bin,\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    })\n",
    "    \n",
    "# monotonicity_3 = [0 ,0, -1, 1] if mono else [0]*23\n",
    "# monotonicity_3 = [1] if mono else [0]*23\n",
    "monotonicity_3 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1] if mono else [0]*23\n",
    "variables_3 = feature_names[2*f_per_utility:3*f_per_utility]\n",
    "dico3 = []\n",
    "for i, v in enumerate(variables_3):\n",
    "\n",
    "    dico3.append({\n",
    "        \"utility\": [2],\n",
    "        \"variables\": [v],\n",
    "        \"boosting_params\": {\n",
    "            'boosting': 'gbdt',\n",
    "            'monotone_constraints_method': 'advanced',\n",
    "            \"max_depth\": md,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"verbose\": verbose,\n",
    "            \"monotone_constraints\": [monotonicity_3[i]],\n",
    "            \"lambda_l2\": l2,\n",
    "            \"lambda_l1\": l1,\n",
    "            \"min_data_in_leaf\": min_data_in_leaf,\n",
    "            \"min_sum_hessian_in_leaf\": min_sum_hessian_in_leaf,\n",
    "            \"max_bin\": mb,\n",
    "            \"min_data_in_bin\": min_data_in_bin,\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    })\n",
    "\n",
    "monotonicity_4 = [0, 0, 0, -1] if mono else [0]*20\n",
    "# monotonicity_4 = [1] if mono else [0]*20\n",
    "variables_4 = feature_names[3*f_per_utility:]\n",
    "dico4 = []\n",
    "for i, v in enumerate(variables_4):\n",
    "    dico4.append({\n",
    "        \"utility\": [3],\n",
    "        \"variables\": [v],\n",
    "        \"boosting_params\": {\n",
    "            'boosting': 'gbdt',\n",
    "            'monotone_constraints_method': 'advanced',\n",
    "            \"max_depth\": md,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"verbose\": verbose,\n",
    "            \"monotone_constraints\": [monotonicity_4[i]],\n",
    "            \"lambda_l2\": l2,\n",
    "            \"lambda_l1\": l1,\n",
    "            \"min_data_in_leaf\": min_data_in_leaf,\n",
    "            \"min_sum_hessian_in_leaf\": min_sum_hessian_in_leaf,\n",
    "            \"max_bin\": mb,\n",
    "            \"min_data_in_bin\": min_data_in_bin,\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    })\n",
    "\n",
    "# rum_structure = dico1 + dico2 + dico3 #+ dico4\n",
    "if n_utility == 4:\n",
    "    rum_structure = dico1 + dico2 + dico3 + dico4\n",
    "elif n_utility == 3:\n",
    "    rum_structure = dico1 + dico2 + dico3\n",
    "else:\n",
    "    rum_structure = dico1\n",
    "boost_from_param_space = [False] * len(rum_structure)\n",
    "# boost_from_param_space = [True] * len(rum_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82e146c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "general_params = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_classes\": n_utility,  # important\n",
    "    \"verbosity\": 2,  # specific RUMBoost parameter\n",
    "    \"num_iterations\": 20,\n",
    "    # \"early_stopping_round\": 10,\n",
    "    # \"max_booster_to_update\": 23 * 4,\n",
    "    # \"max_booster_to_update\": 17 * 4,\n",
    "    # \"max_booster_to_update\": 8,\n",
    "    # \"max_booster_to_update\": n_utility * f_per_utility,\n",
    "    # \"max_booster_to_update\": 1,\n",
    "    \"max_booster_to_update\": n_utility,\n",
    "    # \"boost_from_parameter_space\": boost_from_param_space + boost_from_param_space2,\n",
    "    \"boost_from_parameter_space\": boost_from_param_space, #+ [False]*len(rum_structure2),\n",
    "    \"verbose_interval\": 1,\n",
    "    \"optim_interval\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93d90852",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensors = {\"device\":\"cuda\"}\n",
    "# torch_tensors = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c18c1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_specification = {\n",
    "    \"general_params\": general_params,\n",
    "    \"rum_structure\": rum_structure,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08d01433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm\n",
    "from rumboost.rumboost import rum_train\n",
    "# features and label column names\n",
    "features = [f for f in dataset.columns if f != \"choice\"]\n",
    "label = \"choice\"\n",
    "# create lightgbm dataset\n",
    "lgb_train_set = lightgbm.Dataset(\n",
    "    dataset, label=dataset[label], free_raw_data=False\n",
    ")\n",
    "lgb_test_set = lightgbm.Dataset(\n",
    "    dataset_test, label=dataset_test[label], free_raw_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2802ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ucesnjs\\OneDrive - University College London\\Documents\\PhD - UCL\\pwl-experiment\\src\\rumboost\\rumboost.py:2731: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "c:\\Users\\ucesnjs\\OneDrive - University College London\\Documents\\PhD - UCL\\pwl-experiment\\src\\rumboost\\rumboost.py:3259: UserWarning: Assuming the learning rate is 0.1 for all boosters\n",
      "  warnings.warn(f\"Assuming the learning rate is {lr} for all boosters\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch tensors on cuda\n",
      "[LightGBM] [Warning] Unknown parameter: boost_from_parameter_space\n",
      "[LightGBM] [Warning] Unknown parameter: max_booster_to_update\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_interval\n",
      "[LightGBM] [Warning] Unknown parameter: optim_interval\n",
      "[LightGBM] [Warning] Unknown parameter: boost_from_parameter_space\n",
      "[LightGBM] [Warning] Unknown parameter: max_booster_to_update\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_interval\n",
      "[LightGBM] [Warning] Unknown parameter: optim_interval\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000288 seconds, init for row-wise cost 0.000330 seconds\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Debug] Using Dense Multi-Val Bin\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000081 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000048 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000042 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000053 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000065 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000031 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000000 seconds, init for row-wise cost 0.000031 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000000 seconds, init for row-wise cost 0.000030 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000000 seconds, init for row-wise cost 0.000028 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000000 seconds, init for row-wise cost 0.000032 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000000 seconds, init for row-wise cost 0.000048 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[1]------NCE value on train set : 0.8586\n",
      "---------NCE value on test set 1: 0.8125\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[2]------NCE value on train set : 0.8524\n",
      "---------NCE value on test set 1: 0.8083\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[3]------NCE value on train set : 0.8473\n",
      "---------NCE value on test set 1: 0.8049\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[4]------NCE value on train set : 0.8432\n",
      "---------NCE value on test set 1: 0.8023\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[5]------NCE value on train set : 0.8397\n",
      "---------NCE value on test set 1: 0.7999\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[6]------NCE value on train set : 0.8368\n",
      "---------NCE value on test set 1: 0.7980\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[7]------NCE value on train set : 0.8343\n",
      "---------NCE value on test set 1: 0.7966\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[8]------NCE value on train set : 0.8321\n",
      "---------NCE value on test set 1: 0.7951\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[9]------NCE value on train set : 0.8300\n",
      "---------NCE value on test set 1: 0.7940\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[10]-----NCE value on train set : 0.8281\n",
      "---------NCE value on test set 1: 0.7926\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[11]-----NCE value on train set : 0.8263\n",
      "---------NCE value on test set 1: 0.7918\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[12]-----NCE value on train set : 0.8246\n",
      "---------NCE value on test set 1: 0.7911\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[13]-----NCE value on train set : 0.8230\n",
      "---------NCE value on test set 1: 0.7901\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[14]-----NCE value on train set : 0.8215\n",
      "---------NCE value on test set 1: 0.7892\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[15]-----NCE value on train set : 0.8200\n",
      "---------NCE value on test set 1: 0.7884\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[16]-----NCE value on train set : 0.8187\n",
      "---------NCE value on test set 1: 0.7881\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[17]-----NCE value on train set : 0.8174\n",
      "---------NCE value on test set 1: 0.7873\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[18]-----NCE value on train set : 0.8161\n",
      "---------NCE value on test set 1: 0.7866\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[19]-----NCE value on train set : 0.8150\n",
      "---------NCE value on test set 1: 0.7861\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[20]-----NCE value on train set : 0.8139\n",
      "---------NCE value on test set 1: 0.7859\n"
     ]
    }
   ],
   "source": [
    "LPMC_model_fully_trained = rum_train(lgb_train_set, model_specification, valid_sets=[lgb_test_set], torch_tensors=torch_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9249d7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "15           5           1       5-S0       5-L0        5-L1         None   \n",
      "16           5           2       5-L0       None        None         5-S0   \n",
      "17           5           2       5-L1       None        None         5-S0   \n",
      "18           6           1       6-S0       6-L0        6-L1         None   \n",
      "19           6           2       6-L0       None        None         6-S0   \n",
      "20           6           2       6-L1       None        None         6-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0             f0   63.968800   0.529970            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3             f0   50.933201   0.529970            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6             f0   33.988998   0.432606            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9             f0   23.451799   0.432606            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12            f0   16.753300   0.423655            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "15            f0   11.208700   0.176709            <=              left   \n",
      "16          None         NaN        NaN          None              None   \n",
      "17          None         NaN        NaN          None              None   \n",
      "18            f0    8.749950   0.176709            <=              left   \n",
      "19          None         NaN        NaN          None              None   \n",
      "20          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None -0.033154  274.100295   1058  \n",
      "2          None  0.037237  244.047710    942  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None -0.030577  266.010690   1058  \n",
      "5          None  0.032156  252.056384    942  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None -0.031710  204.599658    862  \n",
      "8          None  0.020752  311.524640   1138  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None -0.027505  193.865682    862  \n",
      "11         None  0.016580  319.611215   1138  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None -0.024466  181.517704    844  \n",
      "14         None  0.013372  329.316342   1156  \n",
      "15         None  0.000000    0.000000   2000  \n",
      "16         None -0.036145   73.516863    359  \n",
      "17         None  0.006085  433.155191   1641  \n",
      "18         None  0.000000    0.000000   2000  \n",
      "19         None -0.032913   69.814278    359  \n",
      "20         None  0.005235  433.252974   1641  \n",
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0             f1    41.64220   0.596457            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3             f1    28.35000   0.596457            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6             f1    19.95030   0.596457            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9             f1    13.97800   0.720621            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12            f1     9.58326   0.720621            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None -0.024392  297.815979   1198  \n",
      "2          None  0.033025  219.349485    802  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None -0.021012  286.264125   1198  \n",
      "5          None  0.026218  228.562112    802  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None -0.018215  276.907883   1198  \n",
      "8          None  0.021394  235.152753    802  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None -0.011278  348.760147   1468  \n",
      "11         None  0.024404  160.216357    532  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None -0.009594  340.780090   1468  \n",
      "14         None  0.019838  163.819039    532  \n",
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "15           5           1       5-S0       5-L0        5-L1         None   \n",
      "16           5           2       5-L0       None        None         5-S0   \n",
      "17           5           2       5-L1       None        None         5-S0   \n",
      "18           6           1       6-S0       6-L0        6-L1         None   \n",
      "19           6           2       6-L0       None        None         6-S0   \n",
      "20           6           2       6-L1       None        None         6-S0   \n",
      "21           7           1       7-S0       7-L0        7-L1         None   \n",
      "22           7           2       7-L0       None        None         7-S0   \n",
      "23           7           2       7-L1       None        None         7-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0             f2    15.13650   0.863541            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3             f2    14.39170   0.863541            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6             f2    12.86730   0.863541            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9             f2    11.70900   0.863541            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12            f2    11.14200   0.863541            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "15            f2    10.15140   0.863541            <=              left   \n",
      "16          None         NaN        NaN          None              None   \n",
      "17          None         NaN        NaN          None              None   \n",
      "18            f2     9.55856   0.863541            <=              left   \n",
      "19          None         NaN        NaN          None              None   \n",
      "20          None         NaN        NaN          None              None   \n",
      "21            f2     9.12355   0.863541            <=              left   \n",
      "22          None         NaN        NaN          None              None   \n",
      "23          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None  0.007400  428.917362   1730  \n",
      "2          None -0.039911   80.280171    270  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None  0.007195  428.179878   1730  \n",
      "5          None -0.039110   79.597109    270  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None  0.006788  429.220966   1730  \n",
      "8          None -0.037314   78.210395    270  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None  0.006377  430.027795   1730  \n",
      "11         None -0.035928   77.164670    270  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None  0.006227  429.076742   1730  \n",
      "14         None -0.035189   76.546733    270  \n",
      "15         None  0.000000    0.000000   2000  \n",
      "16         None  0.005885  429.627561   1730  \n",
      "17         None -0.033862   75.556584    270  \n",
      "18         None  0.000000    0.000000   2000  \n",
      "19         None  0.005728  428.680028   1730  \n",
      "20         None -0.033010   74.813677    270  \n",
      "21         None  0.000000    0.000000   2000  \n",
      "22         None  0.005575  427.547757   1730  \n",
      "23         None -0.032389   74.301798    270  \n",
      "   tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0           0           1       0-S0       0-L0        0-L1         None   \n",
      "1           0           2       0-L0       None        None         0-S0   \n",
      "2           0           2       0-L1       None        None         0-S0   \n",
      "3           1           1       1-S0       1-L0        1-L1         None   \n",
      "4           1           2       1-L0       None        None         1-S0   \n",
      "5           1           2       1-L1       None        None         1-S0   \n",
      "\n",
      "  split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0            f4     4.56489   0.971921            <=              left   \n",
      "1          None         NaN        NaN          None              None   \n",
      "2          None         NaN        NaN          None              None   \n",
      "3            f4     4.25133   0.971921            <=              left   \n",
      "4          None         NaN        NaN          None              None   \n",
      "5          None         NaN        NaN          None              None   \n",
      "\n",
      "  missing_type     value      weight  count  \n",
      "0         None  0.000000    0.000000   2000  \n",
      "1         None  0.002174  308.759824   1944  \n",
      "2         None -0.067287    9.760236     56  \n",
      "3         None  0.000000    0.000000   2000  \n",
      "4         None  0.002094  308.643105   1944  \n",
      "5         None -0.066345    9.351448     56  \n",
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "15           5           1       5-S0       5-L0        5-L1         None   \n",
      "16           5           2       5-L0       None        None         5-S0   \n",
      "17           5           2       5-L1       None        None         5-S0   \n",
      "18           6           1       6-S0       6-L0        6-L1         None   \n",
      "19           6           2       6-L0       None        None         6-S0   \n",
      "20           6           2       6-L1       None        None         6-S0   \n",
      "21           7           1       7-S0       7-L0        7-L1         None   \n",
      "22           7           2       7-L0       None        None         7-S0   \n",
      "23           7           2       7-L1       None        None         7-S0   \n",
      "24           8           1       8-S0       8-L0        8-L1         None   \n",
      "25           8           2       8-L0       None        None         8-S0   \n",
      "26           8           2       8-L1       None        None         8-S0   \n",
      "27           9           1       9-S0       9-L0        9-L1         None   \n",
      "28           9           2       9-L0       None        None         9-S0   \n",
      "29           9           2       9-L1       None        None         9-S0   \n",
      "30          10           1      10-S0      10-L0       10-L1         None   \n",
      "31          10           2      10-L0       None        None        10-S0   \n",
      "32          10           2      10-L1       None        None        10-S0   \n",
      "33          11           1      11-S0      11-L0       11-L1         None   \n",
      "34          11           2      11-L0       None        None        11-S0   \n",
      "35          11           2      11-L1       None        None        11-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0             f6     6.94486   0.850841            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3             f6     6.57712   0.850841            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6             f6     6.14904   0.026853            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9             f6     6.06392   0.850841            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12            f6     5.59928   0.791639            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "15            f6     5.42660   0.791639            <=              left   \n",
      "16          None         NaN        NaN          None              None   \n",
      "17          None         NaN        NaN          None              None   \n",
      "18            f6     5.34207   0.026853            <=              left   \n",
      "19          None         NaN        NaN          None              None   \n",
      "20          None         NaN        NaN          None              None   \n",
      "21            f6     5.23472   0.850841            <=              left   \n",
      "22          None         NaN        NaN          None              None   \n",
      "23          None         NaN        NaN          None              None   \n",
      "24            f6     4.81657   0.791639            <=              left   \n",
      "25          None         NaN        NaN          None              None   \n",
      "26          None         NaN        NaN          None              None   \n",
      "27            f6     4.62403   0.026853            <=              left   \n",
      "28          None         NaN        NaN          None              None   \n",
      "29          None         NaN        NaN          None              None   \n",
      "30            f6     4.60215   0.791639            <=              left   \n",
      "31          None         NaN        NaN          None              None   \n",
      "32          None         NaN        NaN          None              None   \n",
      "33            f6     4.28527   0.850841            <=              left   \n",
      "34          None         NaN        NaN          None              None   \n",
      "35          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None  0.006515  268.344152   1699  \n",
      "2          None -0.033713   51.083460    301  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None  0.006303  268.764273   1699  \n",
      "5          None -0.033053   50.430379    301  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None  0.081970    8.916965     60  \n",
      "8          None -0.002257  310.208261   1940  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None  0.006023  269.464293   1699  \n",
      "11         None -0.032045   49.536156    301  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None  0.007035  249.351126   1575  \n",
      "14         None -0.024991   69.893034    425  \n",
      "15         None  0.000000    0.000000   2000  \n",
      "16         None  0.006906  249.487045   1575  \n",
      "17         None -0.024689   69.504793    425  \n",
      "18         None  0.000000    0.000000   2000  \n",
      "19         None  0.074619    9.323748     60  \n",
      "20         None -0.002206  309.513967   1940  \n",
      "21         None  0.000000    0.000000   2000  \n",
      "22         None  0.005449  270.644537   1699  \n",
      "23         None -0.030364   48.059818    301  \n",
      "24         None  0.000000    0.000000   2000  \n",
      "25         None  0.006483  250.360607   1575  \n",
      "26         None -0.023494   68.199850    425  \n",
      "27         None  0.000000    0.000000   2000  \n",
      "28         None  0.067952    9.721942     60  \n",
      "29         None -0.002091  308.698599   1940  \n",
      "30         None  0.000000    0.000000   2000  \n",
      "31         None  0.006309  250.588785   1575  \n",
      "32         None -0.023071   67.726212    425  \n",
      "33         None  0.000000    0.000000   2000  \n",
      "34         None  0.004822  272.047953   1699  \n",
      "35         None -0.028114   46.215319    301  \n",
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "15           5           1       5-S0       5-L0        5-L1         None   \n",
      "16           5           2       5-L0       None        None         5-S0   \n",
      "17           5           2       5-L1       None        None         5-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0             f7    14.98890   0.825232            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3             f7    12.10480   0.825232            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6             f7     9.83592   0.825232            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9             f7     8.00268   0.825232            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12            f7     6.52416   0.825232            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "15            f7     5.35219   0.825232            <=              left   \n",
      "16          None         NaN        NaN          None              None   \n",
      "17          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None -0.009982  264.015831   1649  \n",
      "2          None  0.046894   56.197427    351  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None -0.009218  261.440690   1649  \n",
      "5          None  0.041020   58.737541    351  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None -0.008462  258.916480   1649  \n",
      "8          None  0.036180   60.978605    351  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None -0.007778  256.648853   1649  \n",
      "11         None  0.031989   63.033928    351  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None -0.007127  254.476064   1649  \n",
      "14         None  0.028400   64.867252    351  \n",
      "15         None  0.000000    0.000000   2000  \n",
      "16         None -0.006547  252.482024   1649  \n",
      "17         None  0.025350   66.448868    351  \n",
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "15           5           1       5-S0       5-L0        5-L1         None   \n",
      "16           5           2       5-L0       None        None         5-S0   \n",
      "17           5           2       5-L1       None        None         5-S0   \n",
      "18           6           1       6-S0       6-L0        6-L1         None   \n",
      "19           6           2       6-L0       None        None         6-S0   \n",
      "20           6           2       6-L1       None        None         6-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0             f8   26.533300   0.649914            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3             f8   20.207300   0.361349            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6             f8   16.953899   0.649914            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9             f8   16.309000   0.361349            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12            f8   13.759300   0.361349            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "15            f8   13.019100   0.649914            <=              left   \n",
      "16          None         NaN        NaN          None              None   \n",
      "17          None         NaN        NaN          None              None   \n",
      "18            f8   11.202200   0.361349            <=              left   \n",
      "19          None         NaN        NaN          None              None   \n",
      "20          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None  0.015528  414.681746   1294  \n",
      "2          None -0.025619  251.911764    706  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None  0.024675  221.397729    729  \n",
      "5          None -0.012351  441.010667   1271  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None  0.012700  405.345065   1294  \n",
      "8          None -0.020226  254.615267    706  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None  0.022441  217.528637    729  \n",
      "11         None -0.011009  441.827996   1271  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None  0.020780  214.607787    729  \n",
      "14         None -0.010077  442.419579   1271  \n",
      "15         None  0.000000    0.000000   2000  \n",
      "16         None  0.011271  400.751327   1294  \n",
      "17         None -0.017612  255.601940    706  \n",
      "18         None  0.000000    0.000000   2000  \n",
      "19         None  0.018932  211.372729    729  \n",
      "20         None -0.009051  442.614599   1271  \n",
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "15           5           1       5-S0       5-L0        5-L1         None   \n",
      "16           5           2       5-L0       None        None         5-S0   \n",
      "17           5           2       5-L1       None        None         5-S0   \n",
      "18           6           1       6-S0       6-L0        6-L1         None   \n",
      "19           6           2       6-L0       None        None         6-S0   \n",
      "20           6           2       6-L1       None        None         6-S0   \n",
      "21           7           1       7-S0       7-L0        7-L1         None   \n",
      "22           7           2       7-L0       None        None         7-S0   \n",
      "23           7           2       7-L1       None        None         7-S0   \n",
      "24           8           1       8-S0       8-L0        8-L1         None   \n",
      "25           8           2       8-L0       None        None         8-S0   \n",
      "26           8           2       8-L1       None        None         8-S0   \n",
      "27           9           1       9-S0       9-L0        9-L1         None   \n",
      "28           9           2       9-L0       None        None         9-S0   \n",
      "29           9           2       9-L1       None        None         9-S0   \n",
      "30          10           1      10-S0      10-L0       10-L1         None   \n",
      "31          10           2      10-L0       None        None        10-S0   \n",
      "32          10           2      10-L1       None        None        10-S0   \n",
      "33          11           1      11-S0      11-L0       11-L1         None   \n",
      "34          11           2      11-L0       None        None        11-S0   \n",
      "35          11           2      11-L1       None        None        11-S0   \n",
      "36          12           1      12-S0      12-L0       12-L1         None   \n",
      "37          12           2      12-L0       None        None        12-S0   \n",
      "38          12           2      12-L1       None        None        12-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0            f10   86.896103   0.600868            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3            f10   69.306503   0.600868            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6            f10   55.468102   0.600868            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9            f10   44.931801   0.600868            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12           f10   36.785301   0.639497            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "15           f10   30.631100   0.788862            <=              left   \n",
      "16          None         NaN        NaN          None              None   \n",
      "17          None         NaN        NaN          None              None   \n",
      "18           f10   23.232000   0.639497            <=              left   \n",
      "19          None         NaN        NaN          None              None   \n",
      "20          None         NaN        NaN          None              None   \n",
      "21           f10   20.989799   0.858201            <=              left   \n",
      "22          None         NaN        NaN          None              None   \n",
      "23          None         NaN        NaN          None              None   \n",
      "24           f10   17.591600   0.858733            <=              left   \n",
      "25          None         NaN        NaN          None              None   \n",
      "26          None         NaN        NaN          None              None   \n",
      "27           f10   14.925300   0.788862            <=              left   \n",
      "28          None         NaN        NaN          None              None   \n",
      "29          None         NaN        NaN          None              None   \n",
      "30           f10   12.327400   0.858733            <=              left   \n",
      "31          None         NaN        NaN          None              None   \n",
      "32          None         NaN        NaN          None              None   \n",
      "33           f10   10.547600   0.639497            <=              left   \n",
      "34          None         NaN        NaN          None              None   \n",
      "35          None         NaN        NaN          None              None   \n",
      "36           f10    9.565600   0.858733            <=              left   \n",
      "37          None         NaN        NaN          None              None   \n",
      "38          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None  0.027998  420.120950   1242  \n",
      "2          None -0.045876  256.402319    758  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None  0.025498  414.274502   1242  \n",
      "5          None -0.040226  261.861255    758  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None  0.023151  408.516459   1242  \n",
      "8          None -0.035516  266.148131    758  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None  0.021126  403.370250   1242  \n",
      "11         None -0.031613  269.469183    758  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None  0.017720  426.385860   1320  \n",
      "14         None -0.030952  244.222089    680  \n",
      "15         None  0.000000    0.000000   2000  \n",
      "16         None  0.011026  528.575248   1613  \n",
      "17         None -0.041565  140.103791    387  \n",
      "18         None  0.000000    0.000000   2000  \n",
      "19         None  0.014438  416.669375   1320  \n",
      "20         None -0.024206  248.266346    680  \n",
      "21         None  0.000000    0.000000   2000  \n",
      "22         None  0.007124  570.978866   1751  \n",
      "23         None -0.044416   91.707734    249  \n",
      "24         None  0.000000    0.000000   2000  \n",
      "25         None  0.006544  569.260321   1753  \n",
      "26         None -0.040689   91.532205    247  \n",
      "27         None  0.000000    0.000000   2000  \n",
      "28         None  0.007954  515.043092   1613  \n",
      "29         None -0.028522  143.419184    387  \n",
      "30         None  0.000000    0.000000   2000  \n",
      "31         None  0.005550  563.388327   1753  \n",
      "32         None -0.033922   92.047807    247  \n",
      "33         None  0.000000    0.000000   2000  \n",
      "34         None  0.010050  402.696530   1320  \n",
      "35         None -0.016079  250.637314    680  \n",
      "36         None  0.000000    0.000000   2000  \n",
      "37         None  0.004914  559.433781   1753  \n",
      "38         None -0.029851   92.191124    247  \n"
     ]
    }
   ],
   "source": [
    "for booster in LPMC_model_fully_trained.boosters:\n",
    "    try:\n",
    "        print(booster.trees_to_dataframe())\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a775664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000099 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2000\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Info] Start training from score -1.505078\n",
      "[LightGBM] [Info] Start training from score -2.107841\n",
      "[LightGBM] [Info] Start training from score -0.420833\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "Train cross-entropy loss: 0.8378638163070736\n",
      "Cross-entropy loss: 0.7985938414870477\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "train_set = lightgbm.Dataset(\n",
    "    dataset[\"f0\"].values.reshape(-1, 1), label=dataset[label].values, free_raw_data=False\n",
    ")\n",
    "test_set = lightgbm.Dataset(\n",
    "    dataset_test[\"f0\"].values.reshape(-1, 1), label=dataset_test[label].values, free_raw_data=False\n",
    ")\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"num_leaves\": 32,\n",
    "    \"max_depth\": 1,\n",
    "    \"min_data_in_leaf\": 1,\n",
    "    \"min_sum_hessian_in_leaf\": 0,\n",
    "    \"min_data_in_bin\": 1,\n",
    "    \"max_bin\": 2000,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": n_utility,\n",
    "    \"verbosity\": 2,\n",
    "    \"eval_metric\": \"multi_logloss\",\n",
    "    \"monotone_constraints\": [1],\n",
    "    \"monotone_constraints_method\": \"advanced\",\n",
    "}\n",
    "\n",
    "model = lightgbm.train(params, train_set, num_boost_round=20, valid_sets=[test_set])\n",
    "y_preds_train = model.predict(dataset[\"f0\"].values.reshape(-1, 1))\n",
    "cel_train = cross_entropy(y_preds_train, dataset[label].values)\n",
    "print(f\"Train cross-entropy loss: {cel_train}\")\n",
    "y_preds = model.predict(dataset_test[\"f0\"].values.reshape(-1, 1))\n",
    "cel = cross_entropy(y_preds, dataset_test[label].values)\n",
    "print(f\"Cross-entropy loss: {cel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9d8d5999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_index</th>\n",
       "      <th>node_depth</th>\n",
       "      <th>node_index</th>\n",
       "      <th>left_child</th>\n",
       "      <th>right_child</th>\n",
       "      <th>parent_index</th>\n",
       "      <th>split_feature</th>\n",
       "      <th>split_gain</th>\n",
       "      <th>threshold</th>\n",
       "      <th>decision_type</th>\n",
       "      <th>missing_direction</th>\n",
       "      <th>missing_type</th>\n",
       "      <th>value</th>\n",
       "      <th>weight</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0-S0</td>\n",
       "      <td>0-L0</td>\n",
       "      <td>0-L1</td>\n",
       "      <td>None</td>\n",
       "      <td>Column_0</td>\n",
       "      <td>28.259001</td>\n",
       "      <td>0.480596</td>\n",
       "      <td>&lt;=</td>\n",
       "      <td>left</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.262310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0-L0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0-S0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.294601</td>\n",
       "      <td>143.356621</td>\n",
       "      <td>471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0-L1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0-S0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.233557</td>\n",
       "      <td>161.009878</td>\n",
       "      <td>529.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1-S0</td>\n",
       "      <td>1-L0</td>\n",
       "      <td>1-L1</td>\n",
       "      <td>None</td>\n",
       "      <td>Column_0</td>\n",
       "      <td>2.179800</td>\n",
       "      <td>0.997683</td>\n",
       "      <td>&lt;=</td>\n",
       "      <td>left</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.967584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1-L0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1-S0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.967936</td>\n",
       "      <td>352.693192</td>\n",
       "      <td>998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>58-L0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>58-S0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.005331</td>\n",
       "      <td>191.115747</td>\n",
       "      <td>535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>58-L1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>58-S0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.006420</td>\n",
       "      <td>161.236403</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>59-S0</td>\n",
       "      <td>59-L0</td>\n",
       "      <td>59-L1</td>\n",
       "      <td>None</td>\n",
       "      <td>Column_0</td>\n",
       "      <td>0.140207</td>\n",
       "      <td>0.888808</td>\n",
       "      <td>&lt;=</td>\n",
       "      <td>left</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>59-L0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59-S0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.000884</td>\n",
       "      <td>296.981437</td>\n",
       "      <td>881.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>59-L1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>59-S0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.005695</td>\n",
       "      <td>36.351199</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tree_index  node_depth node_index left_child right_child parent_index  \\\n",
       "0             0           1       0-S0       0-L0        0-L1         None   \n",
       "1             0           2       0-L0       None        None         0-S0   \n",
       "2             0           2       0-L1       None        None         0-S0   \n",
       "3             1           1       1-S0       1-L0        1-L1         None   \n",
       "4             1           2       1-L0       None        None         1-S0   \n",
       "..          ...         ...        ...        ...         ...          ...   \n",
       "153          58           2      58-L0       None        None        58-S0   \n",
       "154          58           2      58-L1       None        None        58-S0   \n",
       "155          59           1      59-S0      59-L0       59-L1         None   \n",
       "156          59           2      59-L0       None        None        59-S0   \n",
       "157          59           2      59-L1       None        None        59-S0   \n",
       "\n",
       "    split_feature  split_gain  threshold decision_type missing_direction  \\\n",
       "0        Column_0   28.259001   0.480596            <=              left   \n",
       "1            None         NaN        NaN          None              None   \n",
       "2            None         NaN        NaN          None              None   \n",
       "3        Column_0    2.179800   0.997683            <=              left   \n",
       "4            None         NaN        NaN          None              None   \n",
       "..            ...         ...        ...           ...               ...   \n",
       "153          None         NaN        NaN          None              None   \n",
       "154          None         NaN        NaN          None              None   \n",
       "155      Column_0    0.140207   0.888808            <=              left   \n",
       "156          None         NaN        NaN          None              None   \n",
       "157          None         NaN        NaN          None              None   \n",
       "\n",
       "    missing_type     value      weight   count  \n",
       "0           None -1.262310    0.000000  1000.0  \n",
       "1           None -1.294601  143.356621   471.0  \n",
       "2           None -1.233557  161.009878   529.0  \n",
       "3           None -0.967584    0.000000  1000.0  \n",
       "4           None -0.967936  352.693192   998.0  \n",
       "..           ...       ...         ...     ...  \n",
       "153         None -0.005331  191.115747   535.0  \n",
       "154         None  0.006420  161.236403   465.0  \n",
       "155         None  0.000000    0.000000  1000.0  \n",
       "156         None -0.000884  296.981437   881.0  \n",
       "157         None  0.005695   36.351199   119.0  \n",
       "\n",
       "[158 rows x 15 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trees_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b89857",
   "metadata": {},
   "source": [
    "## Gradient boosting with linear trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3da0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def compute_preds(X):\n",
    "    preds = softmax(X, axis=1)\n",
    "    return preds\n",
    "\n",
    "def compute_grads_and_hess(preds, labels, num_classes=3):\n",
    "    if num_classes > 1:\n",
    "        grads = preds - labels\n",
    "        factor = num_classes / (num_classes - 1)\n",
    "        hess = factor * preds * (1 - preds)\n",
    "        hess = np.maximum(hess, 1e-6)\n",
    "    else:\n",
    "        grads = preds - labels\n",
    "        hess = np.ones_like(preds)\n",
    "    return grads, hess\n",
    "\n",
    "def compute_split_gain(grads, hess, split, feature, one_sided=False):\n",
    "    left_mask = feature <= split\n",
    "    right_mask = feature > split\n",
    "    left_grads = grads[left_mask]\n",
    "    right_grads = grads[right_mask]\n",
    "    left_hess = hess[left_mask]\n",
    "    right_hess = hess[right_mask]\n",
    "\n",
    "    no_split_gain = np.sum(grads) ** 2 / np.sum(hess)\n",
    "    left_gain = np.sum(left_grads) ** 2 / np.sum(left_hess)\n",
    "    right_gain = np.sum(right_grads) ** 2 / np.sum(right_hess)\n",
    "    bigger_gain_left = left_gain > right_gain\n",
    "\n",
    "    if one_sided:\n",
    "        if bigger_gain_left:\n",
    "            right_gain = 0\n",
    "        else:\n",
    "            left_gain = 0\n",
    "    gain = left_gain + right_gain - no_split_gain\n",
    "    return gain, bigger_gain_left\n",
    "\n",
    "def find_best_split(X, grads, hess, linear_trees=False, from_split_point=False):\n",
    "    best_gain = -np.inf\n",
    "    best_split = None\n",
    "    best_feature = None\n",
    "    best_bigger_gain_left = None\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    for feature in range(X.shape[1]):\n",
    "        unique_values = np.unique(X[:, feature])\n",
    "        for split in unique_values:\n",
    "            if linear_trees and from_split_point:\n",
    "                grads_ = grads * (X[:, feature] - split)\n",
    "                hess_ = hess * (X[:, feature] - split)**2\n",
    "                one_sided = True\n",
    "            elif linear_trees:\n",
    "                grads_ = grads * (X[:, feature])\n",
    "                hess_ = hess * (X[:, feature])**2\n",
    "                one_sided = True\n",
    "            else:\n",
    "                grads_ = grads\n",
    "                hess_ = hess\n",
    "                one_sided = False\n",
    "            gain, bigger_gain_left = compute_split_gain(grads_, hess_, split, X[:, feature], one_sided=one_sided)\n",
    "            if gain > best_gain:\n",
    "                best_gain = copy.deepcopy(gain)\n",
    "                best_split = copy.deepcopy(split)\n",
    "                best_feature = copy.deepcopy(feature)\n",
    "                best_bigger_gain_left = copy.deepcopy(bigger_gain_left)\n",
    "\n",
    "    return best_feature, best_split, best_gain, best_bigger_gain_left\n",
    "\n",
    "def compute_leaf_value(grads, hess, feature, split, linear_trees=False, from_split_point=False):\n",
    "    no_split_value = - np.sum(grads) / np.sum(hess)\n",
    "    left_mask = feature <= split\n",
    "    right_mask = feature > split\n",
    "\n",
    "    left_grads = grads[left_mask]\n",
    "    right_grads = grads[right_mask]\n",
    "    left_hess = hess[left_mask]\n",
    "    right_hess = hess[right_mask]\n",
    "\n",
    "    left_intercept = - np.sum(left_grads) / np.sum(left_hess)\n",
    "    right_intercept = - np.sum(right_grads) / np.sum(right_hess)\n",
    "\n",
    "    if linear_trees and from_split_point:\n",
    "        grads = grads * (feature - split)\n",
    "        hess = hess * (feature - split)**2\n",
    "    elif linear_trees:\n",
    "        grads = grads * (feature)\n",
    "        hess = hess * (feature)**2\n",
    "\n",
    "    \n",
    "    left_grads = grads[left_mask]\n",
    "    right_grads = grads[right_mask]\n",
    "    left_hess = hess[left_mask]\n",
    "    right_hess = hess[right_mask]\n",
    "\n",
    "    left_value = - np.sum(left_grads) / np.sum(left_hess)\n",
    "    right_value = - np.sum(right_grads) / np.sum(right_hess)\n",
    "\n",
    "    return left_value, right_value, no_split_value, left_intercept, right_intercept\n",
    "\n",
    "def boost(X, y, preds, num_classes=3, linear_trees=False, from_split_point=False):\n",
    "    grads, hess = compute_grads_and_hess(preds, y, num_classes=num_classes)\n",
    "    feature, split, gain, bigger_left = find_best_split(X, grads, hess, linear_trees=linear_trees, from_split_point=from_split_point)\n",
    "    left_value, right_value, no_split_value, left_intercept, right_intercept = compute_leaf_value(grads, hess, X[:, feature].reshape(-1), split, linear_trees=linear_trees, from_split_point=from_split_point)\n",
    "\n",
    "    return feature, split, left_value, right_value, gain, no_split_value, bigger_left, left_intercept, right_intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b92a84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, linear_trees=False, from_split_point=False):\n",
    "        self.linear_trees = linear_trees\n",
    "        self.from_split_point = from_split_point\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, preds, num_classes=3):\n",
    "        self.tree = self._build_tree(X, y, preds, num_classes=num_classes)\n",
    "\n",
    "    def _build_tree(self, X, y, preds, num_classes=3):\n",
    "\n",
    "        feature, split, left_value, right_value, gain, no_split_value, bigger_left, left_intercept, right_intercept = boost(X, y, preds, num_classes=num_classes, linear_trees=self.linear_trees, from_split_point=self.from_split_point)\n",
    "        if gain <= 0:\n",
    "            return None\n",
    "        \n",
    "        if self.linear_trees:\n",
    "            if bigger_left:\n",
    "                right_value = 0\n",
    "            else:\n",
    "                left_value = 0\n",
    "\n",
    "        return {\n",
    "            \"feature\": feature,\n",
    "            \"split\": split,\n",
    "            \"left_value\": -left_value if (self.linear_trees and not self.from_split_point) else left_value,\n",
    "            # \"left_value\": left_value,\n",
    "            \"right_value\": right_value,\n",
    "            \"gain\": gain,\n",
    "            \"left_constant\": split * right_value  if not self.from_split_point else None,\n",
    "            # \"left_constant\": split * right_value + no_split_value if not self.from_split_point else None,\n",
    "            \"right_constant\": split * -left_value if not self.from_split_point else None,\n",
    "            # \"right_constant\": split * -left_value + no_split_value if not self.from_split_point else None,\n",
    "            \"left_intercept\": left_intercept,\n",
    "            \"right_intercept\": right_intercept,\n",
    "            \"full_intercept\": no_split_value,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a4242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self, linear_trees=False, from_split_point=False):\n",
    "        self.linear_trees = linear_trees\n",
    "        self.from_split_point = from_split_point\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y, preds, num_classes=3):\n",
    "        tree = Tree(linear_trees=self.linear_trees, from_split_point=self.from_split_point)\n",
    "        tree.fit(X, y, preds, num_classes=num_classes)\n",
    "        self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros((X.shape[0]))\n",
    "        for tree in self.trees:\n",
    "            if tree.tree is not None:\n",
    "                split = tree.tree[\"split\"]\n",
    "                feature = tree.tree[\"feature\"]\n",
    "                left_value = tree.tree[\"left_value\"]\n",
    "                right_value = tree.tree[\"right_value\"]\n",
    "                left_constant = tree.tree[\"left_constant\"]\n",
    "                right_constant = tree.tree[\"right_constant\"]\n",
    "                left_intercept = tree.tree[\"left_intercept\"]\n",
    "                right_intercept = tree.tree[\"right_intercept\"]\n",
    "                full_intercept = tree.tree[\"full_intercept\"]\n",
    "\n",
    "                x_f = X[:, feature]\n",
    "\n",
    "                if self.linear_trees and self.from_split_point:\n",
    "                    preds[x_f <= split] += left_value * (x_f[x_f <= split] - split)\n",
    "                    preds[x_f > split] += right_value * (x_f[x_f > split] - split) \n",
    "                    # preds[x_f <= split] += left_value * (x_f[x_f <= split] - split) + full_intercept\n",
    "                    # preds[x_f > split] += right_value * (x_f[x_f > split] - split) + full_intercept\n",
    "                elif self.linear_trees:\n",
    "                    preds[x_f <= split] += left_value * x_f[x_f <= split] - left_constant + full_intercept\n",
    "                    # preds[x_f <= split] += left_value * x_f[x_f <= split] - right_constant + full_intercept\n",
    "                elif self.linear_trees:\n",
    "                    preds[x_f <= split] += left_value * x_f[x_f <= split] - left_constant + full_intercept\n",
    "                    # preds[x_f <= split] += left_value * x_f[x_f <= split] - right_constant + full_intercept\n",
    "                    # preds[x_f <= split] += left_value * x_f[x_f <= split] \n",
    "                    preds[x_f > split] += right_value * x_f[x_f > split] - right_constant + full_intercept\n",
    "                    # preds[x_f > split] += right_value * x_f[x_f > split] - left_constant + full_intercept\n",
    "                    # preds = preds - left_constant - right_constant + full_intercept\n",
    "                    # preds = preds - left_constant - right_constant - full_intercept\n",
    "                    # preds[x_f > split] += right_value * x_f[x_f > split] \n",
    "                else:\n",
    "                    preds[x_f <= split] += left_value\n",
    "                    preds[x_f > split] += right_value\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19139cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingModel:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, num_classes=3, linear_trees=False, from_split_point=False, feature_indices=None):\n",
    "        self.linear_trees = linear_trees\n",
    "        self.from_split_point = from_split_point\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.ensemble = [Ensemble(linear_trees=self.linear_trees, from_split_point=self.from_split_point) for _ in range(num_classes)]\n",
    "        self.feature_indices = feature_indices\n",
    "        if self.feature_indices is None:\n",
    "            self.feature_indices = [(i, i+1) for i in range(num_classes)]\n",
    "\n",
    "    def fit(self, X, y, X_test=None, y_test=None):\n",
    "        self.initial_preds = np.zeros((1, self.num_classes))\n",
    "        if self.num_classes > 1:\n",
    "            for j in range(self.num_classes):\n",
    "                self.initial_preds[:, j] = np.log(np.mean(y==j))\n",
    "            raw_preds = np.zeros((X.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "            preds = softmax(raw_preds, axis=1)\n",
    "            print(\"Initial train cel:\", cross_entropy(preds, y))\n",
    "\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_preds = np.zeros((X_test.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "                test_preds = softmax(test_preds, axis=1)\n",
    "                print(\"Initial test cel:\", cross_entropy(test_preds, y_test))\n",
    "        else:\n",
    "            self.initial_preds[:, 0] = np.mean(y)\n",
    "            raw_preds = np.zeros((X.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "            preds = raw_preds\n",
    "            print(\"Initial train mse:\", np.mean((preds - y.reshape(-1, 1))**2))\n",
    "\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_preds = np.zeros((X_test.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "                print(\"Initial test mse:\", np.mean((test_preds - y_test.reshape(-1, 1))**2))\n",
    "            \n",
    "        for i in range(self.n_estimators):\n",
    "            raw_preds = np.zeros((X.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "            for j in range(self.num_classes):\n",
    "                ensemble = self.ensemble[j]\n",
    "                target = y == j if self.num_classes > 1 else y\n",
    "                ensemble.fit(X[:, self.feature_indices[j][0]:self.feature_indices[j][1]], target, preds[:, j], num_classes=self.num_classes)\n",
    "                raw_preds[:, j] += self.learning_rate * ensemble.predict(X[:, self.feature_indices[j][0]:self.feature_indices[j][1]])\n",
    "            if self.num_classes > 1:\n",
    "                preds = softmax(raw_preds, axis=1)\n",
    "                print(\"Train cel:\", cross_entropy(preds, y))\n",
    "            else:\n",
    "                preds = raw_preds\n",
    "                print(\"Train mse:\", np.mean((preds - y.reshape(-1, 1))**2))\n",
    "\n",
    "            if X_test is not None and y_test is not None:\n",
    "                raw_test_preds = np.zeros((X_test.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "                for j in range(self.num_classes):\n",
    "                    ensemble = self.ensemble[j]\n",
    "                    raw_test_preds[:, j] += self.learning_rate * ensemble.predict(X_test[:, self.feature_indices[j][0]:self.feature_indices[j][1]])\n",
    "                if self.num_classes > 1:\n",
    "                    test_preds = softmax(raw_test_preds, axis=1)\n",
    "                    print(\"Test cel:\", cross_entropy(test_preds, y_test))\n",
    "                else:\n",
    "                    test_preds = raw_test_preds\n",
    "                    print(\"Test mse:\", np.mean((test_preds - y_test.reshape(-1, 1))**2))\n",
    "\n",
    "    def predict(self, X, utilities=False):\n",
    "        preds = np.zeros((X.shape[0], self.num_classes)) + self.initial_preds.copy()\n",
    "\n",
    "        for j in range(self.num_classes):\n",
    "            ensemble = self.ensemble[j]\n",
    "            preds[:, j] += self.learning_rate * ensemble.predict(X[:, self.feature_indices[j][0]:self.feature_indices[j][1]])\n",
    "\n",
    "        return softmax(preds, axis=1) if not utilities else preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4baf816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train mse: 0.12407061124766373\n",
      "Initial test mse: 0.12489784340306385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucesnjs\\AppData\\Local\\Temp\\ipykernel_34200\\602013572.py:27: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  right_gain = np.sum(right_grads) ** 2 / np.sum(right_hess)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mse: 0.11936607150394056\n",
      "Test mse: 0.11996675588547888\n",
      "Train mse: 0.12115074295574628\n",
      "Test mse: 0.12165431482175992\n",
      "Train mse: 0.12692660593916444\n",
      "Test mse: 0.12744565015529855\n",
      "Train mse: 0.1350050264358994\n",
      "Test mse: 0.13562233313858196\n",
      "Train mse: 0.14618381931039856\n",
      "Test mse: 0.14702007203892775\n",
      "Train mse: 0.161541980343507\n",
      "Test mse: 0.16277057465614012\n",
      "Train mse: 0.18253885414359186\n",
      "Test mse: 0.18440616732147647\n",
      "Train mse: 0.21114831025668343\n",
      "Test mse: 0.21400138116293352\n",
      "Train mse: 0.2500402908390945\n",
      "Test mse: 0.254364566230293\n",
      "Train mse: 0.3028264602568423\n",
      "Test mse: 0.30929720818424566\n",
      "Feature: 2, Split: 0.8421143355489855, Gain: 51.628000111346594, Left value: 0, Right value: -0.45063389572853396, Left constant: -0.3794852636772852, Right constant: 0.0, Left intercept: 0.07458316159046104, Right intercept: -0.4097230825034426, Full intercept: -1.2789769243681802e-16\n",
      "Feature: 2, Split: 0.8149803537398126, Gain: 43.4824522466296, Left value: 0, Right value: -0.4183730879569625, Left constant: -0.340965847218383, Right constant: 0.0, Left intercept: 0.00777286810202482, Right intercept: -0.3710785777728265, Full intercept: -0.06420890661419694\n",
      "Feature: 0, Split: 0.2645871348563283, Gain: 47.957294906351095, Left value: 1.9923041670438077, Right value: 0, Left constant: 0.0, Right constant: 0.5271380513204448, Left intercept: -0.39057679751891494, Right intercept: -0.011662576083060212, Full intercept: -0.1090435309920749\n",
      "Feature: 0, Split: 0.2645871348563283, Gain: 62.01229990804694, Left value: 2.266723428692399, Right value: 0, Left constant: 0.0, Right constant: 0.5997458575094347, Left intercept: -0.42246038846165723, Right intercept: -0.011662576083060212, Full intercept: -0.11723761386435964\n",
      "Feature: 0, Split: 0.2645871348563283, Gain: 80.67404112273546, Left value: 2.58670543109844, Right value: 0, Left constant: 0.0, Right constant: 0.6844089787316397, Left intercept: -0.46010072058734863, Right intercept: -0.011662576083060212, Full intercept: -0.12691117922066233\n",
      "Feature: 0, Split: 0.2645871348563283, Gain: 105.51526810837592, Left value: 2.9596797492829685, Right value: 0, Left constant: 0.0, Right constant: 0.7830931849550767, Left intercept: -0.5044298063981133, Right intercept: -0.011662576083060212, Full intercept: -0.13830375427402886\n",
      "Feature: 0, Split: 0.2645871348563283, Gain: 138.65635907244305, Left value: 3.394289401060222, Right value: 0, Left constant: 0.0, Right constant: 0.8980853074997267, Left intercept: -0.556531978880943, Right intercept: -0.011662576083060212, Full intercept: -0.1516940126021161\n",
      "Feature: 0, Split: 0.2645871348563283, Gain: 182.9576083268059, Left value: 3.900589004971207, Right value: 0, Left constant: 0.0, Right constant: 1.0320456690774282, Left intercept: -0.6176687643352343, Right intercept: -0.011662576083060212, Full intercept: -0.16740616646386894\n",
      "Feature: 0, Split: 0.2645871348563283, Gain: 242.27927114682447, Left value: 4.490275298276723, Right value: 0, Left constant: 0.0, Right constant: 1.188069075887183, Left intercept: -0.6893078170634453, Right intercept: -0.011662576083060212, Full intercept: -0.18581740301501917\n",
      "Feature: 0, Split: 0.2645871348563283, Gain: 321.83327760902085, Left value: 5.17695529954322, Right value: 0, Left constant: 0.0, Right constant: 1.3697557699854253, Left intercept: -0.7731565792438352, Right intercept: -0.011662576083060212, Full intercept: -0.2073665348953794\n",
      "Mean squared error: 0.30929720818424566\n"
     ]
    }
   ],
   "source": [
    "features = [f for f in dataset.columns if f not in [\"choice\"]]\n",
    "X, y = dataset[features].values, dataset[\"choice\"].values\n",
    "X_test, y_test = dataset_test[features].values, dataset_test[\"choice\"].values\n",
    "\n",
    "feature_indices = [(i * f_per_utility, (i + 1) * f_per_utility) for i in range(n_utility)]\n",
    "\n",
    "model = BoostingModel(n_estimators=10, learning_rate=0.2, num_classes=n_utility, linear_trees=True, from_split_point=False, feature_indices=feature_indices)\n",
    "\n",
    "model.fit(X, y, X_test=X_test, y_test=y_test)\n",
    "for ensemble in model.ensemble:\n",
    "    for tree in ensemble.trees:\n",
    "        feature, split, left_value, right_value, gain, left_constant, right_constant, left_intercept, right_intercept, full_intercept = tree.tree.values()\n",
    "        print(f\"Feature: {feature}, Split: {split}, Gain: {gain}, Left value: {left_value}, Right value: {right_value}, Left constant: {left_constant}, Right constant: {right_constant}, Left intercept: {left_intercept}, Right intercept: {right_intercept}, Full intercept: {full_intercept}\")\n",
    "\n",
    "if n_utility > 1:\n",
    "    preds = model.predict(X_test)\n",
    "    cel = cross_entropy(preds, y_test)\n",
    "    print(f\"Cross-entropy loss: {cel}\")\n",
    "else:\n",
    "    preds = model.predict(X_test, utilities=True)\n",
    "    mse = np.mean((preds - y_test.reshape(-1, 1))**2)\n",
    "    print(f\"Mean squared error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be58d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train mse: 0.12407061124766373\n",
      "Initial test mse: 0.12489784340306385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ucesnjs\\AppData\\Local\\Temp\\ipykernel_34200\\602013572.py:26: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  left_gain = np.sum(left_grads) ** 2 / np.sum(left_hess)\n",
      "C:\\Users\\ucesnjs\\AppData\\Local\\Temp\\ipykernel_34200\\602013572.py:27: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  right_gain = np.sum(right_grads) ** 2 / np.sum(right_hess)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mse: 0.11861261918271526\n",
      "Test mse: 0.11911512817034577\n",
      "Train mse: 0.11418920610698584\n",
      "Test mse: 0.11442754448066578\n",
      "Train mse: 0.11060534007066143\n",
      "Test mse: 0.11062669723773134\n",
      "Train mse: 0.10770227293701322\n",
      "Test mse: 0.10754293702266687\n",
      "Train mse: 0.1053507890622321\n",
      "Test mse: 0.10504229721229631\n",
      "Train mse: 0.10344636604894086\n",
      "Test mse: 0.10301424045747032\n",
      "Train mse: 0.1006910379995513\n",
      "Test mse: 0.09998008091192682\n",
      "Train mse: 0.09899207152529363\n",
      "Test mse: 0.09818610996279417\n",
      "Train mse: 0.09761688516310091\n",
      "Test mse: 0.09673355214404857\n",
      "Train mse: 0.09504130991879756\n",
      "Test mse: 0.09391537086235417\n",
      "Train mse: 0.09379538432507493\n",
      "Test mse: 0.09261293185662299\n",
      "Train mse: 0.09156085036847827\n",
      "Test mse: 0.09016116926130317\n",
      "Train mse: 0.09043284178119586\n",
      "Test mse: 0.08899531444883767\n",
      "Train mse: 0.08951981440405817\n",
      "Test mse: 0.0880523606709551\n",
      "Train mse: 0.08747496659050663\n",
      "Test mse: 0.08581314767757506\n",
      "Train mse: 0.08663644685532555\n",
      "Test mse: 0.08495765075654121\n",
      "Train mse: 0.08432740143952605\n",
      "Test mse: 0.08254958406676471\n",
      "Train mse: 0.0835759737348857\n",
      "Test mse: 0.08177852833927919\n",
      "Train mse: 0.08185584738795897\n",
      "Test mse: 0.07990107416101838\n",
      "Train mse: 0.08116254740154909\n",
      "Test mse: 0.07919673538323883\n",
      "Train mse: 0.08060119347608126\n",
      "Test mse: 0.0786271779693717\n",
      "Train mse: 0.0790386550665529\n",
      "Test mse: 0.07692430152835499\n",
      "Train mse: 0.0785131807036067\n",
      "Test mse: 0.07639667007131462\n",
      "Train mse: 0.0780876126854175\n",
      "Test mse: 0.07597032252414186\n",
      "Train mse: 0.07721115899114138\n",
      "Test mse: 0.07517145946159141\n",
      "Train mse: 0.07682463779090296\n",
      "Test mse: 0.0747846924149468\n",
      "Train mse: 0.07494538358709792\n",
      "Test mse: 0.07281296088586982\n",
      "Train mse: 0.07458782971584997\n",
      "Test mse: 0.07245241469212985\n",
      "Train mse: 0.0732681103669876\n",
      "Test mse: 0.0710197568894636\n",
      "Train mse: 0.07292564509501555\n",
      "Test mse: 0.07067838892783576\n",
      "Train mse: 0.07264854101404597\n",
      "Test mse: 0.07040291709300239\n",
      "Train mse: 0.07195267675175412\n",
      "Test mse: 0.06976066711919043\n",
      "Train mse: 0.07169670591963043\n",
      "Test mse: 0.0695063517682444\n",
      "Train mse: 0.07148968980586791\n",
      "Test mse: 0.06930128646807703\n",
      "Train mse: 0.07086537788178245\n",
      "Test mse: 0.06872647946447401\n",
      "Train mse: 0.0706719396660347\n",
      "Test mse: 0.06853500858284094\n",
      "Train mse: 0.06951986935955966\n",
      "Test mse: 0.06728082288068477\n",
      "Train mse: 0.06932594835980961\n",
      "Test mse: 0.06709146247024068\n",
      "Train mse: 0.06916887235001207\n",
      "Test mse: 0.06693878768527285\n",
      "Train mse: 0.06865579979294764\n",
      "Test mse: 0.06646645859453743\n",
      "Train mse: 0.06850802792414623\n",
      "Test mse: 0.06632290618016654\n",
      "Train mse: 0.06838833271041707\n",
      "Test mse: 0.06620721656878355\n",
      "Train mse: 0.06829137958729647\n",
      "Test mse: 0.06611403704339501\n",
      "Train mse: 0.06781764760729002\n",
      "Test mse: 0.06567851969686146\n",
      "Train mse: 0.06772301634329349\n",
      "Test mse: 0.06558760691828172\n",
      "Train mse: 0.06764635520447995\n",
      "Test mse: 0.06551442405377841\n",
      "Train mse: 0.06661249239435015\n",
      "Test mse: 0.06438894950334623\n",
      "Train mse: 0.06652711426052796\n",
      "Test mse: 0.06430884624431117\n",
      "Train mse: 0.0664579041395157\n",
      "Test mse: 0.06424448092214093\n",
      "Train mse: 0.06640181420873595\n",
      "Test mse: 0.06419283689317638\n",
      "Train mse: 0.06600318935530325\n",
      "Test mse: 0.0638258233063593\n",
      "Train mse: 0.06594642812807316\n",
      "Test mse: 0.06377352863786201\n",
      "Train mse: 0.0659004286718059\n",
      "Test mse: 0.06373160309852685\n",
      "Train mse: 0.06564542276051184\n",
      "Test mse: 0.0634496497578149\n",
      "Train mse: 0.06559006989753481\n",
      "Test mse: 0.0633989448633661\n",
      "Train mse: 0.06554524116028668\n",
      "Test mse: 0.0633583611309089\n",
      "Train mse: 0.06550895331831558\n",
      "Test mse: 0.06332593299737695\n",
      "Train mse: 0.06525183543976992\n",
      "Test mse: 0.0630418068245225\n",
      "Train mse: 0.06520601096677313\n",
      "Test mse: 0.06300047411419961\n",
      "Train mse: 0.06516891757116627\n",
      "Test mse: 0.06296747268709425\n",
      "Train mse: 0.06492764243087942\n",
      "Test mse: 0.062700036616741\n",
      "Train mse: 0.06488153794065324\n",
      "Test mse: 0.06265863587715072\n",
      "Train mse: 0.06484425036719679\n",
      "Test mse: 0.06262562782936931\n",
      "Train mse: 0.06461722344272172\n",
      "Test mse: 0.06237322990634421\n",
      "Train mse: 0.06457137536932717\n",
      "Test mse: 0.06233227898817327\n",
      "Train mse: 0.06453432450604445\n",
      "Test mse: 0.062299665772413346\n",
      "Train mse: 0.06431977446876376\n",
      "Test mse: 0.062060463024077396\n",
      "Train mse: 0.06427460756915804\n",
      "Test mse: 0.06202035522163568\n",
      "Train mse: 0.06423810170890748\n",
      "Test mse: 0.06198842783823841\n",
      "Train mse: 0.06403485882405098\n",
      "Test mse: 0.06176121142254648\n",
      "Train mse: 0.06399054688363268\n",
      "Test mse: 0.061722170507584356\n",
      "Train mse: 0.06395483321002284\n",
      "Test mse: 0.06169115364970034\n",
      "Train mse: 0.06376149623191761\n",
      "Test mse: 0.06147445207840081\n",
      "Train mse: 0.06371831153718656\n",
      "Test mse: 0.061436673683666884\n",
      "Train mse: 0.06368356867166511\n",
      "Test mse: 0.061406722539310225\n",
      "Train mse: 0.0634989934714243\n",
      "Test mse: 0.061199344215310564\n",
      "Train mse: 0.06040737337998522\n",
      "Test mse: 0.05803675653140407\n",
      "Train mse: 0.06036543346457924\n",
      "Test mse: 0.0580006099138308\n",
      "Train mse: 0.060206282608894865\n",
      "Test mse: 0.05782003183172905\n",
      "Train mse: 0.06015837787848104\n",
      "Test mse: 0.05777843373841054\n",
      "Train mse: 0.06001497327161188\n",
      "Test mse: 0.057614677054327046\n",
      "Train mse: 0.059962229490484215\n",
      "Test mse: 0.0575686882920032\n",
      "Train mse: 0.0599195070277708\n",
      "Test mse: 0.05753208122839838\n",
      "Train mse: 0.059776147203945355\n",
      "Test mse: 0.057368450159611\n",
      "Train mse: 0.059728182739017296\n",
      "Test mse: 0.05732707205052037\n",
      "Train mse: 0.05968932718295508\n",
      "Test mse: 0.05729417585468309\n",
      "Train mse: 0.059547167530730506\n",
      "Test mse: 0.05713189525327044\n",
      "Train mse: 0.05950300353310849\n",
      "Test mse: 0.05709419444344793\n",
      "Train mse: 0.05937492231598571\n",
      "Test mse: 0.05694701365568815\n",
      "Train mse: 0.0593264479768318\n",
      "Test mse: 0.05690544112565798\n",
      "Train mse: 0.05928710312369367\n",
      "Test mse: 0.05687231761400993\n",
      "Train mse: 0.05915845618617646\n",
      "Test mse: 0.056724599025851245\n",
      "Train mse: 0.059114483306554916\n",
      "Test mse: 0.0566873268098347\n",
      "Train mse: 0.05907886286808548\n",
      "Test mse: 0.056657761370867935\n",
      "Train mse: 0.058950932036541386\n",
      "Test mse: 0.05651087762774824\n",
      "Train mse: 0.05891048680804822\n",
      "Test mse: 0.056476988657689874\n",
      "Train mse: 0.05879428777107896\n",
      "Test mse: 0.056342674314422816\n",
      "Train mse: 0.058749856382137554\n",
      "Test mse: 0.056305213664427256\n",
      "Train mse: 0.058713855840055944\n",
      "Test mse: 0.056275505485486474\n",
      "Train mse: 0.05859699547756543\n",
      "Test mse: 0.056140546927140766\n",
      "Train mse: 0.05855673673521257\n",
      "Test mse: 0.056107064728795446\n",
      "Train mse: 0.058524117862462074\n",
      "Test mse: 0.05608053725689921\n",
      "Train mse: 0.05840863074821646\n",
      "Test mse: 0.05594718082007573\n",
      "Train mse: 0.05837162349243239\n",
      "Test mse: 0.055916772658824346\n",
      "Train mse: 0.05826666714062705\n",
      "Test mse: 0.055794733511457154\n",
      "Train mse: 0.058226025581797555\n",
      "Test mse: 0.05576117170150151\n",
      "Train mse: 0.058193124702396036\n",
      "Test mse: 0.05573458628570987\n",
      "Train mse: 0.058087379110443524\n",
      "Test mse: 0.055611760458088244\n",
      "Train mse: 0.05805055483555311\n",
      "Test mse: 0.055581786400214044\n",
      "Train mse: 0.0580207317086833\n",
      "Test mse: 0.055558131631406084\n",
      "Train mse: 0.057915286974729585\n",
      "Test mse: 0.05543569262300683\n",
      "Train mse: 0.057881401949332764\n",
      "Test mse: 0.05540852317661168\n",
      "Train mse: 0.05778622039277197\n",
      "Test mse: 0.05529720281852565\n",
      "Train mse: 0.055207706576301585\n",
      "Test mse: 0.05266430480560065\n",
      "Train mse: 0.05512931065285579\n",
      "Test mse: 0.05257095321837761\n",
      "Train mse: 0.055082941060393784\n",
      "Test mse: 0.052533536527709146\n",
      "Train mse: 0.055045491623392805\n",
      "Test mse: 0.05250422356907835\n",
      "Train mse: 0.05496250507142844\n",
      "Test mse: 0.05240591515586176\n",
      "Train mse: 0.05492243011559567\n",
      "Test mse: 0.052374312677752596\n",
      "Train mse: 0.05484514921612843\n",
      "Test mse: 0.05228227460774189\n",
      "Train mse: 0.05480303128159798\n",
      "Test mse: 0.05224891061739919\n",
      "Train mse: 0.0547305639602642\n",
      "Test mse: 0.052162182328336186\n",
      "Train mse: 0.054686918748487315\n",
      "Test mse: 0.052127536158606186\n",
      "Train mse: 0.05465156612694803\n",
      "Test mse: 0.05210028242410864\n",
      "Train mse: 0.05457481376518375\n",
      "Test mse: 0.052008908610525977\n",
      "Train mse: 0.05453710453782083\n",
      "Test mse: 0.05197964377432676\n",
      "Train mse: 0.054465554510652485\n",
      "Test mse: 0.051894003960111096\n",
      "Train mse: 0.05442605814658695\n",
      "Test mse: 0.051863286723128836\n",
      "Train mse: 0.0543588998410862\n",
      "Test mse: 0.05178250571183108\n",
      "Train mse: 0.054318004886647654\n",
      "Test mse: 0.05175062256375883\n",
      "Train mse: 0.054284879973552434\n",
      "Test mse: 0.05172560678318376\n",
      "Train mse: 0.05421359145422413\n",
      "Test mse: 0.05164033333363844\n",
      "Train mse: 0.05417832493404712\n",
      "Test mse: 0.051613572306458556\n",
      "Train mse: 0.054111831027606576\n",
      "Test mse: 0.051533598003449956\n",
      "Train mse: 0.054074832681884004\n",
      "Test mse: 0.05150536778064817\n",
      "Train mse: 0.054012527030084426\n",
      "Test mse: 0.05143003427106581\n",
      "Train mse: 0.05397422365939835\n",
      "Test mse: 0.051400700695296515\n",
      "Train mse: 0.05394319560492947\n",
      "Test mse: 0.051377739878844295\n",
      "Train mse: 0.05387678172875336\n",
      "Test mse: 0.05129792682628033\n",
      "Train mse: 0.053843699572414715\n",
      "Test mse: 0.051273221094920274\n",
      "Train mse: 0.05378173701330985\n",
      "Test mse: 0.05119834111648783\n",
      "Train mse: 0.053747131879115215\n",
      "Test mse: 0.051172490326758316\n",
      "Train mse: 0.0536890459605447\n",
      "Test mse: 0.051101917290228865\n",
      "Train mse: 0.053653233307507674\n",
      "Test mse: 0.05107508345979275\n",
      "Train mse: 0.053624225058547685\n",
      "Test mse: 0.05105415283777985\n",
      "Train mse: 0.05356225985176939\n",
      "Test mse: 0.05097933633080488\n",
      "Train mse: 0.05353132015844686\n",
      "Test mse: 0.05095677719241321\n",
      "Train mse: 0.05347347319495343\n",
      "Test mse: 0.050886537674664424\n",
      "Train mse: 0.053441028008952916\n",
      "Test mse: 0.05086272176208809\n",
      "Train mse: 0.05338678316592504\n",
      "Test mse: 0.05079649491350718\n",
      "Train mse: 0.053353227401064136\n",
      "Test mse: 0.0507718129254411\n",
      "Train mse: 0.0533260397437701\n",
      "Test mse: 0.05075259253506463\n",
      "Train mse: 0.05326811632163909\n",
      "Test mse: 0.05068232971237918\n",
      "Train mse: 0.05323912961444766\n",
      "Test mse: 0.05066162216766594\n",
      "Train mse: 0.05318524669837535\n",
      "Test mse: 0.05059585234950709\n",
      "Train mse: 0.050991817398110685\n",
      "Test mse: 0.048364039790206204\n",
      "Train mse: 0.05090929458626877\n",
      "Test mse: 0.048309882983326474\n",
      "Train mse: 0.05084245110867683\n",
      "Test mse: 0.04826812504562515\n",
      "Train mse: 0.05081224735913538\n",
      "Test mse: 0.048246731827193884\n",
      "Train mse: 0.05075996248769503\n",
      "Test mse: 0.048182518504323466\n",
      "Train mse: 0.05072852927713259\n",
      "Test mse: 0.04816010451495682\n",
      "Train mse: 0.05067918764663223\n",
      "Test mse: 0.048099192611364076\n",
      "Train mse: 0.0506468513240442\n",
      "Test mse: 0.04807604150892544\n",
      "Train mse: 0.050599982599588325\n",
      "Test mse: 0.04801791439170786\n",
      "Train mse: 0.05056700238091312\n",
      "Test mse: 0.04799424003702134\n",
      "Train mse: 0.05052230328715923\n",
      "Test mse: 0.04793856192561022\n",
      "Train mse: 0.05048892889205712\n",
      "Test mse: 0.04791459689384052\n",
      "Train mse: 0.0504618956320244\n",
      "Test mse: 0.04789602621238595\n",
      "Train mse: 0.05041276080025809\n",
      "Test mse: 0.04783542319439961\n",
      "Train mse: 0.050358458542478945\n",
      "Test mse: 0.047803278158429864\n",
      "Train mse: 0.050330126681784736\n",
      "Test mse: 0.04778360272526619\n",
      "Train mse: 0.05028382769563638\n",
      "Test mse: 0.04772619138311327\n",
      "Train mse: 0.05025455046157389\n",
      "Test mse: 0.04770575404285044\n",
      "Train mse: 0.05023082952919438\n",
      "Test mse: 0.04768997442428352\n",
      "Train mse: 0.05018106911661472\n",
      "Test mse: 0.047628733067437465\n",
      "Train mse: 0.05015579940713063\n",
      "Test mse: 0.04761169862052379\n",
      "Train mse: 0.05010934458390605\n",
      "Test mse: 0.047554167894936446\n",
      "Train mse: 0.05008285948023\n",
      "Test mse: 0.04753614090943598\n",
      "Train mse: 0.05003924703876558\n",
      "Test mse: 0.04748180882413388\n",
      "Train mse: 0.05001184934786589\n",
      "Test mse: 0.04746304619891697\n",
      "Train mse: 0.04997059686718855\n",
      "Test mse: 0.04741138167483125\n",
      "Train mse: 0.04994254512682217\n",
      "Test mse: 0.04739210985775793\n",
      "Train mse: 0.04991981592887385\n",
      "Test mse: 0.04737726401966272\n",
      "Train mse: 0.04987504949258212\n",
      "Test mse: 0.0473216871450924\n",
      "Train mse: 0.04985102735097129\n",
      "Test mse: 0.04730581014640174\n",
      "Train mse: 0.049809129605881096\n",
      "Test mse: 0.04725346765708999\n",
      "Train mse: 0.04978412550656755\n",
      "Test mse: 0.04723684796947527\n",
      "Train mse: 0.049744607729963565\n",
      "Test mse: 0.04718720060874098\n",
      "Train mse: 0.04971885792220451\n",
      "Test mse: 0.04716999602525814\n",
      "Train mse: 0.04969799541757384\n",
      "Test mse: 0.04715680832244837\n",
      "Train mse: 0.04965531984645558\n",
      "Test mse: 0.04710363945160021\n",
      "Train mse: 0.049633156620842105\n",
      "Test mse: 0.04708942200305642\n",
      "Train mse: 0.04959334028691345\n",
      "Test mse: 0.04703947943555832\n",
      "Train mse: 0.049549342289624716\n",
      "Test mse: 0.04701511126725136\n",
      "Train mse: 0.04952616162682945\n",
      "Test mse: 0.047000020284782264\n",
      "Train mse: 0.0494886200620341\n",
      "Test mse: 0.04695266349480025\n",
      "Train mse: 0.04946467906282133\n",
      "Test mse: 0.046937013483464726\n",
      "Train mse: 0.04944529213176762\n",
      "Test mse: 0.04692502766428825\n",
      "Train mse: 0.04941996833957238\n",
      "Test mse: 0.04689880872461059\n",
      "Train mse: 0.049382600901188754\n",
      "Test mse: 0.0468518174599719\n",
      "Feature: 2, Split: 0.7881898524928181, Gain: 54.546885639899344, Left value: 0, Right value: -3.0529254554388103, Full intercept: -1.2789769243681802e-16, Left intercept: 0.08766554733803121, Right intercept: -0.3172561493734299\n",
      "Feature: 2, Split: 0.7848254211209238, Gain: 43.9725888416315, Left value: 0, Right value: -2.68301820043395, Full intercept: 0.006785341204752548, Left intercept: 0.08832893876091903, Right intercept: -0.28232377740347403\n",
      "Feature: 2, Split: 0.7826211226795207, Gain: 35.423286529383645, Left value: 0, Right value: -2.3777067485800205, Full intercept: 0.012945994677963703, Left intercept: 0.08869014474274055, Right intercept: -0.25326990069643257\n",
      "Feature: 2, Split: 0.7820347602006341, Gain: 28.51252594638272, Left value: 0, Right value: -2.1311968773294536, Full intercept: 0.018521546394704026, Left intercept: 0.08876978059434641, Right intercept: -0.22695262030292274\n",
      "Feature: 2, Split: 0.7796158068945256, Gain: 22.926984737323576, Left value: 0, Right value: -1.8860053206729197, Full intercept: 0.023546847082232425, Left intercept: 0.0888133689498192, Right intercept: -0.20255503224476454\n",
      "Feature: 2, Split: 0.7773217922926088, Gain: 18.416537961623742, Left value: 0, Right value: -1.6706410525192898, Full intercept: 0.028095977243118547, Left intercept: 0.08891865639758456, Right intercept: -0.18200641778711937\n",
      "Feature: 1, Split: 0.7523426867070209, Gain: 15.337995047381714, Left value: 0, Right value: 1.7433122814215292, Full intercept: 0.032211678108674896, Left intercept: -0.039393626388741394, Right intercept: 0.2571086696668126\n",
      "Feature: 2, Split: 0.7730048924533168, Gain: 16.457405914280837, Left value: 0, Right value: -1.5323525284187278, Full intercept: 0.027104249527085535, Left intercept: 0.08555486455494563, Right intercept: -0.17024651412277908\n",
      "Feature: 2, Split: 0.7697250569057278, Gain: 13.211636995783827, Left value: 0, Right value: -1.3487669058093703, Full intercept: 0.031029269190880407, Left intercept: 0.08556831002201198, Right intercept: -0.15207456941646982\n",
      "Feature: 1, Split: 0.744571475587276, Gain: 13.606374832545454, Left value: 0, Right value: 1.6081238991367341, Full intercept: 0.03458549576710465, Left intercept: -0.037289028641531975, Right intercept: 0.24736255317485067\n",
      "Feature: 2, Split: 0.7641433317322536, Gain: 12.017021493473116, Left value: 0, Right value: -1.2377624153087257, Full intercept: 0.02956545650432809, Left intercept: 0.08277249651257375, Right intercept: -0.14268106284439933\n",
      "Feature: 1, Split: 0.7414338644207579, Gain: 10.507365096383683, Left value: 0, Right value: 1.4703113466986741, Full intercept: 0.032989750942756066, Left intercept: -0.03552997745234218, Right intercept: 0.23370274708635425\n",
      "Feature: 2, Split: 0.7568446082630744, Gain: 10.928723162287431, Left value: 0, Right value: -1.1243005586520436, Full intercept: 0.028282987082495104, Left intercept: 0.08003141672638112, Right intercept: -0.13380506940537928\n",
      "Feature: 2, Split: 0.7545929843608853, Gain: 8.788204011426481, Left value: 0, Right value: -0.9974118560533762, Full intercept: 0.031589432126712974, Left intercept: 0.0801346542549355, Right intercept: -0.11841431150466601\n",
      "Feature: 1, Split: 0.7393388918041073, Gain: 9.013019688588829, Left value: 0, Right value: 1.3893568164260213, Full intercept: 0.034577279402649604, Left intercept: -0.03201839135409441, Right intercept: 0.22660557274976584\n",
      "Feature: 2, Split: 0.7476122788509603, Gain: 8.124448664799464, Left value: 0, Right value: -0.9159256432659896, Full intercept: 0.030055132643148262, Left intercept: 0.07744192223900663, Right intercept: -0.11248509016924339\n",
      "Feature: 0, Split: 0.8092296324920794, Gain: 7.122116554849551, Left value: 0, Right value: 2.4471774330307863, Full intercept: 0.032957321334377135, Left intercept: -0.01952889398336395, Right intercept: 0.27045019616990174\n",
      "Feature: 2, Split: 0.7462136113786653, Gain: 7.3076789382919625, Left value: 0, Right value: -0.859803260127532, Full intercept: 0.028979746080998688, Left intercept: 0.07441722780185249, Right intercept: -0.10553319485499425\n",
      "Feature: 1, Split: 0.7393388918041073, Gain: 6.209160561078539, Left value: 0, Right value: 1.274275914277242, Full intercept: 0.031734277870204886, Left intercept: -0.030931220657031168, Right intercept: 0.21242993867204088\n",
      "Feature: 2, Split: 0.7438261629687111, Gain: 6.776468345142378, Left value: 0, Right value: -0.814211156710628, Full intercept: 0.027586702079499956, Left intercept: 0.07162950123389866, Right intercept: -0.10108735819511588\n",
      "Feature: 2, Split: 0.7429859737264326, Gain: 5.463313075038095, Left value: 0, Right value: -0.7290117402711328, Full intercept: 0.030244500094664395, Left intercept: 0.0714352266837768, Right intercept: -0.08915240134395194\n",
      "Feature: 1, Split: 0.7370818774440678, Gain: 4.95349134640372, Left value: 0, Right value: 1.1986723434613744, Full intercept: 0.032639844943518334, Left intercept: -0.028387478445817547, Right intercept: 0.20543368396543102\n",
      "Feature: 2, Split: 0.7419481984148196, Gain: 5.142523231000203, Left value: 0, Right value: -0.7010251398360416, Full intercept: 0.02866825006342726, Left intercept: 0.06858952011535505, Right intercept: -0.08584479343175434\n",
      "Feature: 2, Split: 0.7417130483265368, Gain: 4.147250163817241, Left value: 0, Right value: -0.630001367673125, Full intercept: 0.03099036742746977, Left intercept: 0.06857904716264163, Right intercept: -0.07571530882650182\n",
      "Feature: 3, Split: 0.7257241746386247, Gain: 4.397570204499077, Left value: 0, Right value: 0.8256744213412803, Full intercept: 0.03308107377887091, Left intercept: -0.011019771609859496, Right intercept: 0.15081819311942637\n",
      "Feature: 2, Split: 0.7379380313325139, Gain: 3.791879938639744, Left value: 0, Right value: -0.5873173228223502, Full intercept: 0.030009330711693885, Left intercept: 0.06664043714813526, Right intercept: -0.0710704893046519\n",
      "Feature: 0, Split: 0.8120401508206501, Gain: 3.549818076394125, Left value: 0, Right value: 2.2583267416134785, Full intercept: 0.03201674162505787, Left intercept: -0.015684854180370852, Right intercept: 0.2561191105229267\n",
      "Feature: 2, Split: 0.733483794849054, Gain: 3.538100593225397, Left value: 0, Right value: -0.5506046188673082, Full intercept: 0.028459271989990997, Left intercept: 0.06435863155127547, Right intercept: -0.06835619122611629\n",
      "Feature: 1, Split: 0.7380961130275108, Gain: 2.975692321819219, Left value: 0, Right value: 1.1081041142421575, Full intercept: 0.030406984811801784, Left intercept: -0.0269338889261715, Right intercept: 0.19445944596947823\n",
      "Feature: 2, Split: 0.7311333828027428, Gain: 3.4129636186155277, Left value: 0, Right value: -0.5317234663612603, Full intercept: 0.026764691377122205, Left intercept: 0.0618299346989755, Right intercept: -0.06661418003308786\n",
      "Feature: 2, Split: 0.7296469300430493, Gain: 2.7621012035058534, Left value: 0, Right value: -0.4743101556826477, Full intercept: 0.028679537236443245, Left intercept: 0.061768576524204276, Right intercept: -0.05899434058441265\n",
      "Feature: 3, Split: 0.7257241746386247, Gain: 2.8468992310406147, Left value: 0, Right value: 0.7357099154183759, Full intercept: 0.030406911626026066, Left intercept: -0.010201240700645996, Right intercept: 0.1388195017825542\n",
      "Feature: 2, Split: 0.7291375421051084, Gain: 2.5646175400807123, Left value: 0, Right value: -0.45456352910317727, Full intercept: 0.027669862041192567, Left intercept: 0.05976810792647756, Right intercept: -0.056319580627898574\n",
      "Feature: 2, Split: 0.7273115953143292, Gain: 2.0771885661278415, Left value: 0, Right value: -0.4046407375927234, Full intercept: 0.029331713192437766, Left intercept: 0.05978423169468734, Right intercept: -0.04975720176664202\n",
      "Feature: 3, Split: 0.7245501012858759, Gain: 2.280722663811024, Left value: 0, Right value: 0.6923850713955526, Full intercept: 0.030831564672469235, Left intercept: -0.007988171028287794, Right intercept: 0.13420866304041928\n",
      "Feature: 2, Split: 0.7241013618707731, Gain: 1.9560705968380443, Left value: 0, Right value: -0.38425366530303234, Full intercept: 0.02823350333086001, Left intercept: 0.05802976753186944, Right intercept: -0.0481957732916936\n",
      "Feature: 1, Split: 0.7380961130275108, Gain: 1.6773770062557833, Left value: 0, Right value: 1.035331116890325, Full intercept: 0.029692195818921, Left intercept: -0.025153620454354046, Right intercept: 0.18660628793666928\n",
      "Feature: 2, Split: 0.7208017465937171, Gain: 1.9763213905804018, Left value: 0, Right value: -0.3778474215407335, Full intercept: 0.02628910428510054, Left intercept: 0.05600141439329628, Right intercept: -0.04861939584682955\n",
      "Feature: 2, Split: 0.7208017465937171, Gain: 1.6041567359275288, Left value: 0, Right value: -0.34006267938666, Full intercept: 0.027758742294739982, Left intercept: 0.05600141439329628, Right intercept: -0.04344461412274701\n",
      "Feature: 3, Split: 0.7257241746386247, Gain: 1.3792943520164744, Left value: 0, Right value: 0.6317332500933801, Full intercept: 0.029081416503415475, Left intercept: -0.00745593051082107, Right intercept: 0.12662607688087268\n",
      "Feature: 2, Split: 0.7167157755859229, Gain: 1.5203899591541392, Left value: 0, Right value: -0.3226379406726058, Full intercept: 0.02673118960874283, Left intercept: 0.05432160975093664, Right intercept: -0.04231733995902418\n",
      "Feature: 2, Split: 0.7167157755859229, Gain: 1.2350426808725452, Left value: 0, Right value: -0.2903741466053452, Full intercept: 0.028023689306357347, Left intercept: 0.05432160975093664, Right intercept: -0.03779019565914844\n",
      "Feature: 2, Split: 0.7167157755859229, Gain: 1.0033369326791388, Left value: 0, Right value: -0.2613367319448108, Full intercept: 0.02918693903421043, Left intercept: 0.05432160975093664, Right intercept: -0.03371576578926028\n",
      "Feature: 3, Split: 0.7245501012858759, Gain: 1.1065662846567208, Left value: 0, Right value: 0.6031331957875135, Full intercept: 0.03023386378927818, Left intercept: -0.005045697076535663, Right intercept: 0.12418346360410117\n",
      "Feature: 2, Split: 0.7164494948281488, Gain: 0.9830397260984053, Left value: 0, Right value: -0.25782030208082407, Full intercept: 0.0279707054622819, Left intercept: 0.05272806732814281, Right intercept: -0.033685063093710284\n",
      "Feature: 2, Split: 0.716609300071746, Gain: 0.7986375543654959, Left value: 0, Right value: -0.2322519117230942, Full intercept: 0.029005508791351806, Left intercept: 0.05286625052926906, Right intercept: -0.030562916386525567\n",
      "Feature: 1, Split: 0.7370818774440678, Gain: 0.7964009414425472, Left value: 0, Right value: 0.9750268848965279, Full intercept: 0.029936625918630406, Left intercept: -0.02323114371104565, Right intercept: 0.1804767859045714\n",
      "Feature: 2, Split: 0.7132307458942986, Gain: 0.8931350878605459, Left value: 0, Right value: -0.24072869504965833, Full intercept: 0.026706041843566203, Left intercept: 0.050774378833272814, Right intercept: -0.03250706403768436\n",
      "Feature: 2, Split: 0.714550330132963, Gain: 0.7253815199285994, Left value: 0, Right value: -0.2182636190523789, Full intercept: 0.02769456479158881, Left intercept: 0.05078311441852125, Right intercept: -0.029385460674994146\n",
      "Feature: 2, Split: 0.7155173364033109, Gain: 0.5889069057458954, Left value: 0, Right value: -0.19750463917175798, Full intercept: 0.02858252254371192, Left intercept: 0.05083110161613646, Right intercept: -0.026555260374905472\n",
      "Feature: 3, Split: 0.7257241746386247, Gain: 0.5372117955364133, Left value: 0, Right value: 0.5568350050253492, Full intercept: 0.02938052646985431, Left intercept: -0.004127738380547823, Right intercept: 0.11883837116221224\n",
      "Feature: 2, Split: 0.7132307458942986, Gain: 0.5970651309649101, Left value: 0, Right value: -0.19628197582249524, Full intercept: 0.027308942293285617, Left intercept: 0.049499426707708974, Right intercept: -0.027284256387181547\n",
      "Feature: 2, Split: 0.7155173364033109, Gain: 0.48411152001140917, Left value: 0, Right value: -0.17885916472257335, Full intercept: 0.028114950224096397, Left intercept: 0.04955121537352413, Right intercept: -0.02500970688535495\n",
      "Feature: 2, Split: 0.3138778580814897, Gain: 0.39605023793815786, Left value: 0, Right value: 0.11183000598951093, Full intercept: 0.028837618391657962, Left intercept: -0.02182175895889887, Right intercept: 0.05262258689056012\n",
      "Feature: 2, Split: 0.712314620748857, Gain: 0.5826612784550947, Left value: 0, Right value: -0.19289501706321877, Full intercept: 0.026212999880395438, Left intercept: 0.04834284858501416, Right intercept: -0.02770474146522197\n",
      "Feature: 2, Split: 0.7132307458942986, Gain: 0.4716593098039962, Left value: 0, Right value: -0.17443466098312593, Full intercept: 0.027010224810221303, Left intercept: 0.04826915842054186, Right intercept: -0.025291165490601935\n",
      "Feature: 2, Split: 0.7155173364033109, Gain: 0.38141693893906964, Left value: 0, Right value: -0.15886031521755595, Full intercept: 0.02772651941430576, Left intercept: 0.04831424650544168, Right intercept: -0.023295239028944145\n",
      "Feature: 2, Split: 0.3158847002253059, Gain: 0.3997479685014498, Left value: 0, Right value: 0.11278678065064428, Full intercept: 0.028368383604349325, Left intercept: -0.022207129171092105, Right intercept: 0.05233305178018487\n",
      "Feature: 2, Split: 0.712314620748857, Gain: 0.4814309495254101, Left value: 0, Right value: -0.17550912627727455, Full intercept: 0.025736688327048386, Left intercept: 0.0471236205497322, Right intercept: -0.026370991899353102\n",
      "Feature: 2, Split: 0.7132307458942986, Gain: 0.3884827649488941, Left value: 0, Right value: -0.15867292525711346, Full intercept: 0.026462058287405325, Left intercept: 0.04704079101941184, Right intercept: -0.024165896634589918\n",
      "Feature: 2, Split: 0.3138778580814897, Gain: 0.3785081201810856, Left value: 0, Right value: 0.10877761002122466, Full intercept: 0.027113629251199967, Left intercept: -0.02182175895889887, Right intercept: 0.050089171548226535\n",
      "Feature: 2, Split: 0.7106139554973521, Gain: 0.48285978265513474, Left value: 0, Right value: -0.17447591914945257, Full intercept: 0.02456064960726271, Left intercept: 0.04571448588712885, Right intercept: -0.02660632190728526\n",
      "Feature: 2, Split: 0.712314620748857, Gain: 0.38875241709955344, Left value: 0, Right value: -0.15831899547888056, Full intercept: 0.025290409073704105, Left intercept: 0.045935807489051815, Right intercept: -0.025010578817984956\n",
      "Feature: 2, Split: 0.31335323797955245, Gain: 0.35999085797041497, Left value: 0, Right value: 0.10539592947369977, Full intercept: 0.02594473313029385, Left intercept: -0.021425326103648282, Right intercept: 0.04808319808408778\n",
      "Feature: 2, Split: 0.7089566066861899, Gain: 0.47789300474725377, Left value: 0, Right value: -0.17248784273892795, Full intercept: 0.023467353931200682, Left intercept: 0.044473506695876734, Right intercept: -0.026854686053447686\n",
      "Feature: 2, Split: 0.7110032322455508, Gain: 0.3836666433034199, Left value: 0, Right value: -0.15672949864323193, Full intercept: 0.02419720433257005, Left intercept: 0.044591203389474435, Right intercept: -0.025251259134170714\n",
      "Feature: 2, Split: 0.3127088051755843, Gain: 0.3436555103311998, Left value: 0, Right value: 0.10231472944709798, Full intercept: 0.024850953401793035, Left intercept: -0.020833667793426675, Right intercept: 0.04615258029340574\n",
      "Feature: 2, Split: 0.7080889161130581, Gain: 0.4680938559295818, Left value: 0, Right value: -0.1704296206010617, Full intercept: 0.02244150244416454, Left intercept: 0.04324432555177842, Right intercept: -0.027154399008674654\n",
      "Feature: 2, Split: 0.7096754958673868, Gain: 0.3746233320716502, Left value: 0, Right value: -0.15449345487305483, Full intercept: 0.023167011124875535, Left intercept: 0.04350412565193973, Right intercept: -0.025669733283652737\n",
      "Feature: 2, Split: 0.3127088051755843, Gain: 0.3289994779815859, Left value: 0, Right value: 0.09958215912928949, Full intercept: 0.023817450777245328, Left intercept: -0.020833667793426675, Right intercept: 0.04463718055066716\n",
      "Feature: 2, Split: 0.7055043918574319, Gain: 0.4548381270973364, Left value: 0, Right value: -0.16656470800011644, Full intercept: 0.0214723502235578, Left intercept: 0.04215305136221187, Right intercept: -0.027361910448390023\n",
      "Feature: 2, Split: 0.708652250075503, Gain: 0.36250536539197836, Left value: 0, Right value: -0.15199383737553787, Full intercept: 0.022194178124519324, Left intercept: 0.042248429292530504, Right intercept: -0.025732083141405664\n",
      "Feature: 2, Split: 0.3116473365100653, Gain: 0.3157911363635162, Left value: 0, Right value: 0.09690043024031651, Full intercept: 0.02283867661269312, Left intercept: -0.02013592690932998, Right intercept: 0.0428305251375903\n",
      "Feature: 2, Split: 0.7046483253953602, Gain: 0.43902395980059805, Left value: 0, Right value: -0.1637094311681997, Full intercept: 0.020549709202991664, Left intercept: 0.04101040745867505, Right intercept: -0.027420021490098815\n",
      "Feature: 2, Split: 0.7080889161130581, Gain: 0.34824447339980863, Left value: 0, Right value: -0.14947472211995358, Full intercept: 0.02126334617548282, Left intercept: 0.04111675894502603, Right intercept: -0.026069071070348625\n",
      "Feature: 2, Split: 0.3089383459587619, Gain: 0.30369373459980853, Left value: 0, Right value: 0.09412295128148002, Full intercept: 0.021899651109228677, Left intercept: -0.019733848407635354, Right intercept: 0.04091218147854678\n",
      "Feature: 0, Split: 0.22101781886496263, Gain: 0.4407204201165129, Left value: 2.216356843096382, Right value: 0, Full intercept: 0.019658826761550636, Left intercept: -0.2813571285445289, Right intercept: 0.10283681568786363\n",
      "Feature: 2, Split: 0.692036864655883, Gain: 0.4278271723863584, Left value: 0, Right value: -0.15140858162182874, Full intercept: 0.024758517391254235, Left intercept: 0.04509359391517675, Right intercept: -0.019257914071919815\n",
      "Feature: 2, Split: 0.34097656573366125, Gain: 0.45173918634004506, Left value: 0, Right value: 0.09385312586475793, Full intercept: 0.02547748022383957, Left intercept: -0.017244378100382343, Right intercept: 0.04748571026965082\n",
      "Feature: 2, Split: 0.6912393383363863, Gain: 0.4901390710804875, Left value: 0, Right value: -0.16118328350089772, Full intercept: 0.023445459300880992, Left intercept: 0.04407009649196341, Right intercept: -0.021094633969592397\n",
      "Feature: 2, Split: 0.33969638674928365, Gain: 0.41692174930402315, Left value: 0, Right value: 0.0888303122193428, Full intercept: 0.024214905793957506, Left intercept: -0.016388378408007685, Right intercept: 0.0450386778733315\n",
      "Feature: 2, Split: 0.6890115359511171, Gain: 0.5420309112520179, Left value: 0, Right value: -0.16729161736261366, Full intercept: 0.022284120935366934, Left intercept: 0.04331746405238981, Right intercept: -0.022929285922485373\n",
      "Feature: 2, Split: 0.6890115359511171, Gain: 0.4409714008198401, Left value: 0, Right value: -0.1505624556263522, Full intercept: 0.02309454242902276, Left intercept: 0.04331746405238981, Right intercept: -0.02037677728104971\n",
      "Feature: 2, Split: 0.3401743330075008, Gain: 0.41718405738362496, Left value: 0, Right value: 0.08891299045871373, Full intercept: 0.023823921773313002, Left intercept: -0.016901107534357838, Right intercept: 0.04475677181109385\n",
      "Feature: 2, Split: 0.6888341435942873, Gain: 0.49560499948208103, Left value: 0, Right value: -0.15939408449482118, Full intercept: 0.02189414881040112, Left intercept: 0.042352565065255296, Right intercept: -0.021881080946845734\n",
      "Feature: 2, Split: 0.6889387074869371, Gain: 0.40317760297704774, Left value: 0, Right value: -0.14353594033404476, Full intercept: 0.022667211767546092, Left intercept: 0.042377732440641844, Right intercept: -0.019605036971608935\n",
      "Feature: 2, Split: 0.3401743330075008, Gain: 0.41458281270681363, Left value: 0, Right value: 0.08854002988308585, Full intercept: 0.023362884535265138, Left intercept: -0.016901107534357838, Right intercept: 0.044058759338651984\n",
      "Feature: 2, Split: 0.6875507064991165, Gain: 0.45897912921117767, Left value: 0, Right value: -0.15199575289862188, Full intercept: 0.021441206331413398, Left intercept: 0.04146103551838392, Right intercept: -0.021297049707229013\n",
      "Feature: 2, Split: 0.33969638674928365, Gain: 0.38345598356815835, Left value: 0, Right value: 0.08395032077007598, Full intercept: 0.022184610259758458, Left intercept: -0.016388378408007685, Right intercept: 0.041967126384376816\n",
      "Feature: 2, Split: 0.6849792019175278, Gain: 0.5050174275798831, Left value: 0, Right value: -0.15727023609463966, Full intercept: 0.020359895202848095, Left intercept: 0.040574004570811076, Right intercept: -0.02259508720407323\n",
      "Feature: 2, Split: 0.6875507064991165, Gain: 0.41076513194741504, Left value: 0, Right value: -0.14346343606454914, Full intercept: 0.02114203094835741, Left intercept: 0.04075248138704656, Right intercept: -0.020722284878436685\n",
      "Feature: 2, Split: 0.33969638674928365, Gain: 0.38470481471811235, Left value: 0, Right value: 0.08413551608200101, Full intercept: 0.021843703724802693, Left intercept: -0.016388378408007685, Right intercept: 0.04145138276114569\n",
      "Feature: 2, Split: 0.6848609538826154, Gain: 0.45988380030034337, Left value: 0, Right value: -0.1497047194166567, Full intercept: 0.020014963326821088, Left intercept: 0.03983506936959586, Right intercept: -0.022006135132041502\n",
      "Feature: 2, Split: 0.6849792019175278, Gain: 0.3737243954529128, Left value: 0, Right value: -0.13481562230896904, Full intercept: 0.020760041631655228, Left intercept: 0.03986709346759966, Right intercept: -0.0198424435197267\n",
      "Feature: 2, Split: 0.33969638674928365, Gain: 0.3831238244953382, Left value: 0, Right value: 0.08390102148212158, Full intercept: 0.021430506176500684, Left intercept: -0.016388378408007685, Right intercept: 0.040826273005772\n",
      "Feature: 2, Split: 0.6836593077875284, Gain: 0.4244338149967544, Left value: 0, Right value: -0.1427467830608296, Full intercept: 0.019606862671815484, Left intercept: 0.03907521566267516, Right intercept: -0.02138550791142322\n",
      "Feature: 2, Split: 0.3360239036769572, Gain: 0.35503752269054323, Left value: 0, Right value: 0.0792987855034091, Full intercept: 0.020322825862354087, Left intercept: -0.015924859378970653, Right intercept: 0.03866502803266301\n",
      "Feature: 2, Split: 0.6826367574019714, Gain: 0.4664152370750626, Left value: 0, Right value: -0.14888431666352867, Full intercept: 0.01857992527512574, Left intercept: 0.03819008369909799, Right intercept: -0.022428644041156267\n",
      "Feature: 2, Split: 0.6835921999664685, Gain: 0.37863407181547903, Left value: 0, Right value: -0.13463168812988716, Full intercept: 0.01933159172311908, Left intercept: 0.03834521538001257, Right intercept: -0.020611757199502145\n",
      "Feature: 2, Split: 0.3360239036769572, Gain: 0.3565031166253534, Left value: 0, Right value: 0.07952412259377371, Full intercept: 0.020007144079203378, Left intercept: -0.015924859378970653, Right intercept: 0.038189603660448074\n",
      "Feature: 2, Split: 0.6811887879627946, Gain: 0.4235359420155828, Left value: 0, Right value: -0.1407445413869363, Full intercept: 0.018259290829148858, Left intercept: 0.03744579072830523, Right intercept: -0.021680557188971685\n",
      "Feature: 2, Split: 0.6830171589570735, Gain: 0.3433563339433106, Left value: 0, Right value: -0.1277996225003444, Full intercept: 0.018976472081089546, Left intercept: 0.03755773477078016, Right intercept: -0.019969394299469385\n",
      "Feature: 2, Split: 0.33969638674928365, Gain: 0.35535979268499374, Left value: 0, Right value: 0.07971616578199411, Full intercept: 0.019620116354348547, Left intercept: -0.016388613654879515, Right intercept: 0.03808752856785584\n",
      "Feature: 2, Split: 0.6811887879627946, Gain: 0.3895497610636086, Left value: 0, Right value: -0.13494130295811788, Full intercept: 0.01788743342178474, Left intercept: 0.03678981686260871, Right intercept: -0.02146097956520013\n",
      "Feature: 2, Split: 0.33597857029791117, Gain: 0.3298200881117659, Left value: 0, Right value: 0.07535726767327092, Full intercept: 0.018575043553164177, Left intercept: -0.015264943532141494, Right intercept: 0.03566054493332979\n",
      "Feature: 2, Split: 0.6788099290209411, Gain: 0.42779965668020936, Left value: 0, Right value: -0.13982456268234564, Full intercept: 0.016918546211086186, Left intercept: 0.03600399349951718, Right intercept: -0.022720459695655098\n",
      "Feature: 2, Split: 0.6811887879627946, Gain: 0.3460398520180692, Left value: 0, Right value: -0.12723459492540728, Full intercept: 0.017641849842147197, Left intercept: 0.03615572225148056, Right intercept: -0.020897813678668484\n",
      "Feature: 2, Split: 0.3360239036769572, Gain: 0.3316277908165294, Left value: 0, Right value: 0.07564781839353063, Full intercept: 0.01829018948771757, Left intercept: -0.015924859887333685, Right intercept: 0.03560382893051458\n",
      "Feature: 2, Split: 0.6783101438550929, Gain: 0.3872300387476949, Left value: 0, Right value: -0.13278222752404462, Full intercept: 0.016627533165005516, Left intercept: 0.035159155239498086, Right intercept: -0.021773631471692607\n",
      "Feature: 2, Split: 0.6788099290209411, Gain: 0.31250556630093784, Left value: 0, Right value: -0.11977739930146283, Full intercept: 0.01731656734333197, Left intercept: 0.03536906960192056, Right intercept: -0.020177091193736625\n",
      "Feature: 2, Split: 0.3360239036769572, Gain: 0.33094690634863033, Left value: 0, Right value: 0.07554012850283956, Full intercept: 0.017936168264456327, Left intercept: -0.015924859887333685, Right intercept: 0.035070664437651285\n",
      "Feature: 2, Split: 0.6765604341202477, Gain: 0.3552315746386576, Left value: 0, Right value: -0.12632796815149805, Full intercept: 0.016275878847776114, Left intercept: 0.03451290528208897, Right intercept: -0.021428893596171315\n",
      "Feature: 2, Split: 0.3358449288054268, Gain: 0.30738908800037135, Left value: 0, Right value: 0.07174077088246633, Full intercept: 0.016938626367285572, Left intercept: -0.015198056910882012, Right intercept: 0.033127782605159466\n",
      "Feature: 0, Split: 0.22461714240539365, Gain: 0.7879971713568423, Left value: 1.9745154040991728, Right value: 0, Full intercept: 0.015360988992496431, Left intercept: -0.25860104280223045, Right intercept: 0.09218260866348899\n",
      "Feature: 2, Split: 0.34339168382164453, Gain: 0.37753388322779236, Left value: 0, Right value: 0.06623436890942655, Full intercept: 0.02005862741897385, Left intercept: -0.009464018262937027, Right intercept: 0.03547151549948441\n",
      "Feature: 2, Split: 0.6537971920588502, Gain: 0.44809430341837997, Left value: 0, Right value: -0.13328950045619306, Full intercept: 0.018635114044030145, Left intercept: 0.038093560660572554, Right intercept: -0.018471691131701887\n",
      "Feature: 2, Split: 0.6523570874102573, Gain: 0.3646753924524678, Left value: 0, Right value: -0.11903370712660483, Full intercept: 0.01943666526171205, Left intercept: 0.03809488246024034, Right intercept: -0.016065399683528298\n",
      "Feature: 2, Split: 0.3453923880782648, Gain: 0.3941391828708508, Left value: 0, Right value: 0.0684587621744453, Full intercept: 0.02015839313585446, Left intercept: -0.009987246316186702, Right intercept: 0.03596647236070532\n",
      "Feature: 2, Split: 0.6523570874102573, Gain: 0.3912066730163324, Left value: 0, Right value: -0.1231356589759891, Full intercept: 0.018696065962889204, Left intercept: 0.037601671829618405, Right intercept: -0.017276719655807408\n",
      "Feature: 2, Split: 0.34374413819390764, Gain: 0.3740918176605898, Left value: 0, Right value: 0.06581465631871479, Full intercept: 0.019442664884565593, Left intercept: -0.009768912267973436, Right intercept: 0.03472701637260392\n",
      "Feature: 2, Split: 0.6521579531857877, Gain: 0.41235811363354935, Left value: 0, Right value: -0.12612582222631047, Full intercept: 0.018029696020769807, Left intercept: 0.03695107166011664, Right intercept: -0.017893495410453875\n",
      "Feature: 2, Split: 0.3424672193417797, Gain: 0.3570854559620909, Left value: 0, Right value: 0.06354640165497068, Full intercept: 0.01879529146900673, Left intercept: -0.009181113540100925, Right intercept: 0.03336855187299058\n",
      "Feature: 2, Split: 0.6516780522067431, Gain: 0.4287766064511807, Left value: 0, Right value: -0.1281243170658257, Full intercept: 0.017425685486327308, Left intercept: 0.0364747974159312, Right intercept: -0.018580439374831493\n",
      "Feature: 2, Split: 0.6516780522067431, Gain: 0.34940511836149446, Left value: 0, Right value: -0.11531188535924307, Full intercept: 0.01820553744425396, Left intercept: 0.0364747974159312, Right intercept: -0.016326531981979895\n",
      "Feature: 2, Split: 0.3453923880782648, Gain: 0.3734807905513513, Left value: 0, Right value: 0.06583715508722543, Full intercept: 0.018907404206387948, Left intercept: -0.00998730943434608, Right intercept: 0.03405951013994358\n",
      "Feature: 2, Split: 0.6512282989901553, Gain: 0.37362219782379874, Left value: 0, Right value: -0.11886063413536566, Full intercept: 0.017501076399445564, Left intercept: 0.03585894433097032, Right intercept: -0.01712191550026994\n",
      "Feature: 2, Split: 0.34374413819390764, Gain: 0.35482637515965537, Left value: 0, Right value: 0.0633273675412501, Full intercept: 0.01822639558447522, Left intercept: -0.00976893263041731, Right intercept: 0.03287437005791861\n",
      "Feature: 2, Split: 0.6498425110815371, Gain: 0.392910527885789, Left value: 0, Right value: -0.12091604284026718, Full intercept: 0.016866826096964375, Left intercept: 0.035327862501263, Right intercept: -0.017873971516888666\n",
      "Feature: 2, Split: 0.3424672193417797, Gain: 0.3389826595079792, Left value: 0, Right value: 0.06117440079263261, Full intercept: 0.017610502402737482, Left intercept: -0.009181113540100925, Right intercept: 0.031566591315927074\n",
      "Feature: 2, Split: 0.6493984071227249, Gain: 0.4078755098848432, Left value: 0, Right value: -0.12280228982815755, Full intercept: 0.01629201979584915, Left intercept: 0.0347484524455192, Right intercept: -0.018287273559279765\n",
      "Feature: 2, Split: 0.6493984071227249, Gain: 0.33268398678961186, Left value: 0, Right value: -0.11052206084534179, Full intercept: 0.017049194788489673, Left intercept: 0.0347484524455192, Right intercept: -0.016111483350542643\n",
      "Feature: 2, Split: 0.3453923880782648, Gain: 0.35497662167769134, Left value: 0, Right value: 0.06345048985559504, Full intercept: 0.01773065228186614, Left intercept: -0.009987370189153803, Right intercept: 0.03226571284593756\n",
      "Feature: 2, Split: 0.6479765302023832, Gain: 0.3552228039110775, Left value: 0, Right value: -0.11334200585385652, Full intercept: 0.01637530531813696, Left intercept: 0.03425696698503561, Right intercept: -0.017053396478088224\n",
      "Feature: 2, Split: 0.34374413819390764, Gain: 0.33745006542699585, Left value: 0, Right value: 0.06104884582525302, Full intercept: 0.017079766412626738, Left intercept: -0.009768952232790853, Right intercept: 0.03112780122557562\n",
      "Feature: 2, Split: 0.6478257730262027, Gain: 0.3731556017217702, Left value: 0, Right value: -0.11601634034567618, Full intercept: 0.015769114300367305, Left intercept: 0.033740589429438764, Right intercept: -0.017753608648129873\n",
      "Feature: 2, Split: 0.34339168382164453, Gain: 0.3225669438194573, Left value: 0, Right value: 0.05904734630606016, Full intercept: 0.016490807751320696, Left intercept: -0.00946403507051017, Right intercept: 0.030041052938364822\n",
      "Feature: 2, Split: 0.6477684964157739, Gain: 0.38687966754165015, Left value: 0, Right value: -0.1180156380243442, Full intercept: 0.015221758334747378, Left intercept: 0.03314381948120096, Right intercept: -0.018135325429968086\n",
      "Feature: 2, Split: 0.6478257730262027, Gain: 0.31567911131054377, Left value: 0, Right value: -0.10624421237581018, Full intercept: 0.015956124900598966, Left intercept: 0.03331912282638298, Right intercept: -0.016431587562682908\n",
      "Feature: 2, Split: 0.3453923880782648, Gain: 0.3381166082357406, Left value: 0, Right value: 0.06124273765303924, Full intercept: 0.016617029666641955, Left intercept: -0.009987405010596471, Right intercept: 0.030568135655925532\n",
      "Feature: 2, Split: 0.6477684964157739, Gain: 0.33670634558027684, Left value: 0, Right value: -0.10967760713418855, Full intercept: 0.015308841838345934, Left intercept: 0.032713563543453304, Right intercept: -0.017085354067726554\n",
      "Feature: 2, Split: 0.34374413819390764, Gain: 0.32153990094853135, Left value: 0, Right value: 0.058932003609602, Full intercept: 0.015991323999611205, Left intercept: -0.009768955262120602, Right intercept: 0.029469855494515803\n",
      "Feature: 2, Split: 0.6454897188052808, Gain: 0.3533484352888211, Left value: 0, Right value: -0.11108390697315185, Full intercept: 0.01472611818013191, Left intercept: 0.03212759000465842, Right intercept: -0.017449252800260412\n",
      "Feature: 2, Split: 0.34339168382164453, Gain: 0.30747532703690095, Left value: 0, Right value: 0.057012763624924676, Full intercept: 0.015426224252760787, Left intercept: -0.00946403507051017, Right intercept: 0.028420682316508024\n",
      "Feature: 2, Split: 0.6449951735503929, Gain: 0.36618469208972143, Left value: 0, Right value: -0.11276717297075893, Full intercept: 0.014200902219820968, Left intercept: 0.03169560961283089, Right intercept: -0.018075961917780537\n",
      "Feature: 2, Split: 0.6449951735503929, Gain: 0.29878813144725175, Left value: 0, Right value: -0.10149045567368303, Full intercept: 0.01491357733432632, Left intercept: 0.03169560961283089, Right intercept: -0.01604843669870414\n",
      "Feature: 2, Split: 0.34556444304106626, Gain: 0.32249633448360415, Left value: 0, Right value: 0.05917938150540189, Full intercept: 0.015554984937381135, Left intercept: -0.010350989268096602, Right intercept: 0.029169947734920543\n",
      "Feature: 2, Split: 0.6449951735503929, Gain: 0.3186144063066165, Left value: 0, Right value: -0.10481476083343055, Full intercept: 0.01429153978968658, Left intercept: 0.03128454393889048, Right intercept: -0.017059706841205956\n",
      "Feature: 2, Split: 0.34374413819390764, Gain: 0.30678215600891, Left value: 0, Right value: 0.05694122645000951, Full intercept: 0.01495395660577708, Left intercept: -0.009768958187069339, Right intercept: 0.02788970867179802\n",
      "Feature: 2, Split: 0.6448408338333962, Gain: 0.3342514373249512, Left value: 0, Right value: -0.10726396597794115, Full intercept: 0.013731490599868149, Left intercept: 0.030665469715226466, Right intercept: -0.017442425498859656\n",
      "Feature: 2, Split: 0.34339168382164453, Gain: 0.2934431233549484, Left value: 0, Right value: 0.05509546984018468, Full intercept: 0.014409968840203471, Left intercept: -0.00946403507051017, Right intercept: 0.026873870425248796\n",
      "Feature: 2, Split: 0.6438241876914454, Gain: 0.3461859201604536, Left value: 0, Right value: -0.10861340328866995, Full intercept: 0.013225853410288828, Left intercept: 0.030113068156472356, Right intercept: -0.01772606710183792\n",
      "Feature: 2, Split: 0.6441336282808384, Gain: 0.28232475390201234, Left value: 0, Right value: -0.09789421433796264, Full intercept: 0.013916761299747527, Left intercept: 0.0301974908227488, Right intercept: -0.015988975909169693\n",
      "Feature: 2, Split: 0.34556444304106626, Gain: 0.3079212626877654, Left value: 0, Right value: 0.057216801491356675, Full intercept: 0.01453841332552426, Left intercept: -0.010351054124603238, Right intercept: 0.02761914793508784\n",
      "Feature: 2, Split: 0.6438241876914454, Gain: 0.3009493979598982, Left value: 0, Right value: -0.10094836576273811, Full intercept: 0.013316868112108018, Left intercept: 0.029718680832238803, Right intercept: -0.016745377865015537\n",
      "Feature: 2, Split: 0.3453923880782648, Gain: 0.2929665983972823, Left value: 0, Right value: 0.055163362092374296, Full intercept: 0.013959017425375258, Left intercept: -0.009987471114738182, Right intercept: 0.026516322391532315\n",
      "Feature: 0, Split: 0.2346921775725765, Gain: 0.378965514730794, Left value: 1.7025071013712936, Right value: 0, Full intercept: 0.012780689322768822, Left intercept: -0.2364410722284098, Right intercept: 0.08680375468621876\n",
      "Feature: 2, Split: 0.2101167682363268, Gain: 0.5433701362414365, Left value: -0.3556592082870712, Right value: 0, Full intercept: 0.017214966756867848, Left intercept: 0.03493976134954309, Right intercept: 0.012230223114212851\n",
      "Feature: 2, Split: 0.2101167682363268, Gain: 0.36290906067433165, Left value: -0.3200932874583641, Right value: 0, Full intercept: 0.016367793745198655, Left intercept: 0.03108020320982012, Right intercept: 0.012230223114212851\n",
      "Feature: 2, Split: 0.6382289482005228, Gain: 0.2888595724914248, Left value: 0, Right value: -0.10064435250954186, Full intercept: 0.015605338034696388, Left intercept: 0.032234416040825775, Right intercept: -0.014086135508281174\n",
      "Feature: 2, Split: 0.3467544859637179, Gain: 0.3004301429116426, Left value: 0, Right value: 0.05450928129254454, Full intercept: 0.016265592003105043, Left intercept: -0.006948789395023369, Right intercept: 0.028547206626579696\n",
      "Feature: 2, Split: 0.6382289482005228, Gain: 0.30137933814958007, Left value: 0, Right value: -0.10267231418518967, Full intercept: 0.015106098856216221, Left intercept: 0.03187323639686596, Right intercept: -0.014831882100765607\n",
      "Feature: 2, Split: 0.34627731343156354, Gain: 0.2884709958933124, Left value: 0, Right value: 0.0528948497928915, Full intercept: 0.015779656797582534, Left intercept: -0.006529406522880875, Right intercept: 0.02755625172076069\n",
      "Feature: 2, Split: 0.6380885031388304, Gain: 0.310931619045238, Left value: 0, Right value: -0.10407570920358365, Full intercept: 0.01465285304383068, Left intercept: 0.03126454695074824, Right intercept: -0.014943224695754024\n",
      "Feature: 2, Split: 0.34575405391576763, Gain: 0.27821486495227044, Left value: 0, Right value: 0.0514904698791931, Full intercept: 0.015336143111483153, Left intercept: -0.006392578301917859, Right intercept: 0.026781042176556958\n",
      "Feature: 2, Split: 0.6380885031388304, Gain: 0.3179756049255653, Left value: 0, Right value: -0.10510680386677825, Full intercept: 0.014237491700102193, Left intercept: 0.030921900611268028, Right intercept: -0.015488138084603555\n",
      "Feature: 2, Split: 0.34575405391576763, Gain: 0.26932345716874306, Left value: 0, Right value: 0.0502845591512718, Full intercept: 0.014927551231518702, Left intercept: -0.006392578301917859, Right intercept: 0.026157237779664687\n",
      "Feature: 2, Split: 0.6376817251899117, Gain: 0.32287099910857037, Left value: 0, Right value: -0.10555359338636339, Full intercept: 0.013854630319904774, Left intercept: 0.030522606634773523, Right intercept: -0.015713111298288195\n",
      "Feature: 2, Split: 0.6376817251899117, Gain: 0.26335066133713186, Left value: 0, Right value: -0.09499823404772706, Full intercept: 0.014549169540830019, Left intercept: 0.030522606634773523, Right intercept: -0.013786511517635644\n",
      "Feature: 2, Split: 0.3468735116196069, Gain: 0.2879568016703298, Left value: 0, Right value: 0.05285619618450287, Full intercept: 0.01517425483966274, Left intercept: -0.007161374412358893, Right intercept: 0.027017094221186064\n",
      "Feature: 2, Split: 0.20797665731488973, Gain: 0.2780949483248377, Left value: -0.29284378904555536, Right value: 0, Full intercept: 0.014050336708728935, Left intercept: 0.02870982545140474, Right intercept: 0.009951682466534871\n",
      "Feature: 2, Split: 0.6384728057198756, Gain: 0.2892729376552754, Left value: 0, Right value: -0.09757518626599081, Full intercept: 0.013366508966745318, Left intercept: 0.029583298892966098, Right intercept: -0.015651819450745983\n",
      "Feature: 2, Split: 0.34575405391576763, Gain: 0.2511759457846571, Left value: 0, Right value: 0.05117655273889159, Full intercept: 0.014005774160241376, Left intercept: -0.008374687698971817, Right intercept: 0.025793956360895653\n",
      "Feature: 2, Split: 0.6382289482005228, Gain: 0.2992039996439498, Left value: 0, Right value: -0.09908867073545675, Full intercept: 0.012913820794344524, Left intercept: 0.029084338172052342, Right intercept: -0.015958885721284198\n",
      "Feature: 2, Split: 0.6384728057198756, Gain: 0.24426170066854247, Left value: 0, Right value: -0.08928278276206565, Full intercept: 0.01356386907272379, Left intercept: 0.02924094006166461, Right intercept: -0.01448868612784952\n",
      "Feature: 2, Split: 0.3467544859637179, Gain: 0.26433996159250606, Left value: 0, Right value: 0.05317707188790111, Full intercept: 0.0141488064690273, Left intercept: -0.008925207477491077, Right intercept: 0.02635615941320979\n",
      "Feature: 2, Split: 0.6382289482005228, Gain: 0.26015638125085405, Left value: 0, Right value: -0.0920574847274217, Full intercept: 0.013017651389313038, Left intercept: 0.0287319857756188, Right intercept: -0.015040533406291402\n",
      "Feature: 2, Split: 0.34575405391576763, Gain: 0.2517548831572862, Left value: 0, Right value: 0.051262607589956564, Full intercept: 0.013621573200099562, Left intercept: -0.008374687698971817, Right intercept: 0.025207390009534113\n",
      "Feature: 2, Split: 0.6382289482005228, Gain: 0.27268153470749007, Left value: 0, Right value: -0.09424532549609257, Full intercept: 0.012527783683089691, Left intercept: 0.028389952348321354, Right intercept: -0.01579436148240752\n",
      "Feature: 2, Split: 0.34575405391576763, Gain: 0.2410987032913043, Left value: 0, Right value: 0.04966958000114498, Full intercept: 0.013146058316600413, Left intercept: -0.008374687698971817, Right intercept: 0.024481413088161368\n",
      "Feature: 2, Split: 0.6380885031388304, Gain: 0.28222991552797816, Left value: 0, Right value: -0.09579901538390187, Full intercept: 0.012086259206221583, Left intercept: 0.027806998907885627, Right intercept: -0.015922457842223\n",
      "Feature: 2, Split: 0.34556444304106626, Gain: 0.2320179833002464, Left value: 0, Right value: 0.04828603058005911, Full intercept: 0.012715210151678619, Left intercept: -0.007906670566078567, Right intercept: 0.02355310169594613\n",
      "Feature: 2, Split: 0.6376817251899117, Gain: 0.2892628772026873, Left value: 0, Right value: -0.09677122227857647, Full intercept: 0.011684331668566422, Left intercept: 0.02742970692492825, Right intercept: -0.016246784770943667\n",
      "Feature: 2, Split: 0.6380885031388304, Gain: 0.23598810610750132, Left value: 0, Right value: -0.08725616474072598, Full intercept: 0.012321083172452504, Left intercept: 0.027485257582037143, Right intercept: -0.01469603423878245\n",
      "Feature: 2, Split: 0.3467544859637179, Gain: 0.24583117458354645, Left value: 0, Right value: 0.050438097301299334, Full intercept: 0.01289394759168178, Left intercept: -0.008925244302521375, Right intercept: 0.02443743443479231\n",
      "Feature: 2, Split: 0.6376817251899117, Gain: 0.2493913593724534, Left value: 0, Right value: -0.08955135933912785, Full intercept: 0.011821054558124818, Left intercept: 0.027098165239189325, Right intercept: -0.015279395595941053\n",
      "Feature: 2, Split: 0.34627731343156354, Gain: 0.2347492431730393, Left value: 0, Right value: 0.04874184506996609, Full intercept: 0.012410299598040535, Left intercept: -0.00850867298655794, Right intercept: 0.02345308802887135\n",
      "Feature: 2, Split: 0.6362918982421957, Gain: 0.2598307490404119, Left value: 0, Right value: -0.09083595351898412, Full intercept: 0.011371966108018789, Left intercept: 0.026590331668906824, Right intercept: -0.01556580561887171\n",
      "Feature: 2, Split: 0.34575405391576763, Gain: 0.22535878972695855, Left value: 0, Right value: 0.04728044318919085, Full intercept: 0.011974221225815434, Left intercept: -0.008374689025864062, Right intercept: 0.02269234952631837\n",
      "Feature: 2, Split: 0.6358909957416347, Gain: 0.2677163522004837, Left value: 0, Right value: -0.09202711496794878, Full intercept: 0.01096539909305195, Left intercept: 0.02610895719613124, Right intercept: -0.015781936588320465\n",
      "Feature: 2, Split: 0.6362918982421957, Gain: 0.21816415407072445, Left value: 0, Right value: -0.08297273290376402, Full intercept: 0.011576885492900679, Left intercept: 0.02627820749042337, Right intercept: -0.014445676159223984\n",
      "Feature: 2, Split: 0.3467544859637179, Gain: 0.23789618728114623, Left value: 0, Right value: 0.0492461297013021, Full intercept: 0.012127006354663017, Left intercept: -0.008925258074065025, Right intercept: 0.02326474869769039\n",
      "Feature: 2, Split: 0.6358909957416347, Gain: 0.23168094385756458, Left value: 0, Right value: -0.08537786597334514, Full intercept: 0.011079468237383405, Left intercept: 0.025786982118341788, Right intercept: -0.01489770358278792\n",
      "Feature: 2, Split: 0.3467544859637179, Gain: 0.22684664025955187, Left value: 0, Right value: 0.04756780030552777, Full intercept: 0.011646772818310343, Left intercept: -0.008925258074065025, Right intercept: 0.022530446654337677\n",
      "Feature: 2, Split: 0.20768365009563622, Gain: 0.24858013530236053, Left value: -0.26414096632792305, Right value: 0, Full intercept: 0.010634935253055587, Left intercept: 0.026459421216752127, Right intercept: 0.0062364359596319505\n",
      "Feature: 2, Split: 0.6376817251899117, Gain: 0.24398791284377064, Left value: 0, Right value: -0.08796891711084108, Full intercept: 0.0100198180306597, Left intercept: 0.02486553800639742, Right intercept: -0.01631537732158515\n",
      "Feature: 2, Split: 0.3453923880782648, Gain: 0.19712115885608045, Left value: 0, Right value: 0.046044882334038285, Full intercept: 0.0105986506520387, Left intercept: -0.009379876214962276, Right intercept: 0.021075195228636757\n",
      "Feature: 2, Split: 0.6362918982421957, Gain: 0.25197668510541915, Left value: 0, Right value: -0.08888394213931869, Full intercept: 0.009615099672231672, Left intercept: 0.024376171735294477, Right intercept: -0.016513224561278385\n",
      "Feature: 2, Split: 0.6380885031388304, Gain: 0.20355069635091466, Left value: 0, Right value: -0.0805858140804395, Full intercept: 0.010204412680677805, Left intercept: 0.02460698419655006, Right intercept: -0.015455801661230908\n",
      "Feature: 2, Split: 0.9191027847703966, Gain: 0.21015290957204755, Left value: 0, Right value: 0.8920598565084201, Full intercept: 0.010733484130454272, Left intercept: 0.00867016176011338, Right intercept: 0.034461691389374535\n",
      "Feature: 2, Split: 0.3453923880782648, Gain: 0.19649398251179445, Left value: 0, Right value: 0.04593797484750414, Full intercept: 0.010453597532603333, Left intercept: -0.009379876214962276, Right intercept: 0.020854077668521893\n",
      "Mean squared error: 0.0468518174599719\n"
     ]
    }
   ],
   "source": [
    "features = [f for f in dataset.columns if f not in [\"choice\"]]\n",
    "X, y = dataset[features].values, dataset[\"choice\"].values\n",
    "X_test, y_test = dataset_test[features].values, dataset_test[\"choice\"].values\n",
    "\n",
    "model = BoostingModel(n_estimators=200, learning_rate=0.1, num_classes=n_utility, linear_trees=True, from_split_point=True, feature_indices=feature_indices)\n",
    "\n",
    "model.fit(X, y, X_test=X_test, y_test=y_test)\n",
    "for ensemble in model.ensemble:\n",
    "    for tree in ensemble.trees:\n",
    "        feature, split, left_value, right_value, gain, left_constant, right_constant, left_intercept, right_intercept, full_intercept = tree.tree.values()\n",
    "        print(f\"Feature: {feature}, Split: {split}, Gain: {gain}, Left value: {left_value}, Right value: {right_value}, Full intercept: {full_intercept}, Left intercept: {left_intercept}, Right intercept: {right_intercept}\")\n",
    "\n",
    "if n_utility > 1:\n",
    "    preds = model.predict(X_test)\n",
    "    cel = cross_entropy(preds, y_test)\n",
    "    print(f\"Cross-entropy loss: {cel}\")\n",
    "else:\n",
    "    preds = model.predict(X_test, utilities=True)\n",
    "    mse = np.mean((preds - y_test.reshape(-1, 1))**2)\n",
    "    print(f\"Mean squared error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06e1d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_plot = np.linspace(1e-4, 1, 1000)\n",
    "\n",
    "y_plot = []\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    plot_dataset = pd.DataFrame({feature_names[i]: np.zeros_like(x_plot) for i in range(f_per_utility*n_utility)})\n",
    "    plot_dataset[feature_names[i]] = x_plot\n",
    "\n",
    "    if n_utility == 2:\n",
    "        y_plot.append(model.predict(plot_dataset.values, utilities=True))\n",
    "    else:\n",
    "        y_plot.append(model.predict(plot_dataset.values, utilities=True)[:, i // f_per_utility])\n",
    "\n",
    "colours = [\"r\", \"g\", \"b\", \"y\", \"k\", \"magenta\", \"cyan\", \"orange\", \"purple\", \"brown\", \"pink\", \"grey\", \"olive\", \"lime\", \"teal\", \"coral\"]\n",
    "\n",
    "\n",
    "if n_utility == 2:\n",
    "    \n",
    "    y_plot_true = []\n",
    "    for i, (sp_i, beta_i, inter_i) in enumerate(zip(sp, betas, intercept)):\n",
    "        if i % f_per_utility == 0:\n",
    "            plt.figure()\n",
    "        y_plot_true.append(apply_linear_feature(x_plot.reshape(-1,1), sp_i.reshape(1,-1), beta_i.reshape(1,-1), feature_names[i], [0]).values)\n",
    "    \n",
    "    y_plot_ttrue = [y_1 - y_0 for y_0, y_1 in zip(y_plot_true[0], y_plot_true[1])]\n",
    "    y_plot_ttrue = np.array(y_plot_ttrue).reshape(-1)\n",
    "\n",
    "    plt.plot(x_plot, y_plot_ttrue, label=f\"{feature_names[0]}_true\", color=colours[0], linewidth=2)\n",
    "        # ascc = ascs[i//f_per_utility].cpu().numpy() if LPMC_model_fully_trained.device is not None else ascs[i//f_per_utility]\n",
    "        # plt.plot(x_plot, y_plot[i]+ascc, label=feature_names[i], color=colours[i], linestyle=\"--\", linewidth=2)\n",
    "    plt.plot(x_plot, y_plot[0], label=feature_names[0], color=colours[0], linestyle=\"--\", linewidth=2)\n",
    "\n",
    "    plt.title(f\"Utility 0\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.grid()\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "else:\n",
    "    for i, (sp_i, beta_i, inter_i) in enumerate(zip(sp, betas, intercept)):\n",
    "        if i % f_per_utility == 0:\n",
    "            plt.figure()\n",
    "        y_plot_true = apply_linear_feature(x_plot.reshape(-1,1), sp_i.reshape(1,-1), beta_i.reshape(1,-1), feature_names[i], [0]).values\n",
    "        # y_plot_true = apply_linear_feature(x_plot.reshape(-1,1), sp_i.reshape(1,-1), beta_i.reshape(1,-1), feature_names[i], [inter_i]).values\n",
    "        plt.plot(x_plot, y_plot_true, label=f\"{feature_names[i]}_true\", color=colours[i], linewidth=2)\n",
    "        # ascc = ascs[i//f_per_utility].cpu().numpy() if LPMC_model_fully_trained.device is not None else ascs[i//f_per_utility]\n",
    "        # plt.plot(x_plot, y_plot[i]+ascc, label=feature_names[i], color=colours[i], linestyle=\"--\", linewidth=2)\n",
    "        plt.plot(x_plot, y_plot[i], label=feature_names[i], color=colours[i], linestyle=\"--\", linewidth=2)\n",
    "\n",
    "\n",
    "\n",
    "        if i % f_per_utility == f_per_utility - 1:\n",
    "            plt.title(f\"Utility {i//f_per_utility}\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.grid()\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.show()\n",
    "\n",
    "    # for i in range(len(feature_names)):\n",
    "    #     plt.figure()\n",
    "    #     plt.hist(dataset[feature_names[i]], bins=150, alpha=0.5, label=feature_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276b6902",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m min_data_in_bin \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m     13\u001b[0m mb \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m---> 14\u001b[0m variables_0 \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_names\u001b[49m[:f_per_utility]\n\u001b[0;32m     15\u001b[0m dico1 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(variables_0):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "md = 1\n",
    "verbose = 2\n",
    "mono = False \n",
    "# monotonicity_0 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1] if mono else [0]*17\n",
    "monotonicity_0 = [1, 1, -1, 0] if mono else [0] * 17\n",
    "# monotonicity_0 = [1] if mono else [0] * 17\n",
    "l1 = 0\n",
    "l2 = 0\n",
    "min_data_in_leaf = 1 \n",
    "min_sum_hessian_in_leaf = 1e-3\n",
    "min_data_in_bin = 20\n",
    "mb = 2000\n",
    "variables_0 = feature_names[:f_per_utility]\n",
    "dico1 = []\n",
    "for i, v in enumerate(variables_0):\n",
    "    dico1.append({\n",
    "        \"utility\": [0],\n",
    "        \"variables\": [v],\n",
    "        \"boosting_params\": {\n",
    "            'boosting': 'gbdt',\n",
    "            'monotone_constraints_method': 'advanced',\n",
    "            \"max_depth\": md,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"verbose\": verbose,\n",
    "            \"monotone_constraints\": [monotonicity_0[i]],\n",
    "            \"lambda_l2\": l2,\n",
    "            \"lambda_l1\": l1,\n",
    "            \"min_data_in_leaf\": min_data_in_leaf,\n",
    "            \"min_sum_hessian_in_leaf\": min_sum_hessian_in_leaf,\n",
    "            \"max_bin\": mb,\n",
    "            \"min_data_in_bin\": min_data_in_bin,\n",
    "            \"num_classes\": 1,\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    })\n",
    "\n",
    "# monotonicity_1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1] if mono else [0]*17\n",
    "monotonicity_1 = [0, 1, -1, 1] if mono else [0]*17\n",
    "# monotonicity_1 = [1] if mono else [0]*17\n",
    "variables_1 = feature_names[f_per_utility:2*f_per_utility]\n",
    "dico2 = []\n",
    "for i, v in enumerate(variables_1):\n",
    "    dico2.append({\n",
    "        \"utility\": [1],\n",
    "        \"variables\": [v],\n",
    "        \"boosting_params\": {\n",
    "            'boosting': 'gbdt',\n",
    "            'monotone_constraints_method': 'advanced',\n",
    "            \"max_depth\": md,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"verbose\": verbose,\n",
    "            \"monotone_constraints\": [monotonicity_1[i]],\n",
    "            \"lambda_l2\": l2,\n",
    "            \"lambda_l1\": l1,\n",
    "            \"min_data_in_leaf\": min_data_in_leaf,\n",
    "            \"min_sum_hessian_in_leaf\": min_sum_hessian_in_leaf,\n",
    "            \"max_bin\": mb,\n",
    "            \"min_data_in_bin\": min_data_in_bin,\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    })\n",
    "    \n",
    "monotonicity_3 = [0 ,0, -1, 1] if mono else [0]*23\n",
    "# monotonicity_3 = [1] if mono else [0]*23\n",
    "# monotonicity_3 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1] if mono else [0]*23\n",
    "variables_3 = feature_names[2*f_per_utility:3*f_per_utility]\n",
    "dico3 = []\n",
    "for i, v in enumerate(variables_3):\n",
    "\n",
    "    dico3.append({\n",
    "        \"utility\": [2],\n",
    "        \"variables\": [v],\n",
    "        \"boosting_params\": {\n",
    "            'boosting': 'gbdt',\n",
    "            'monotone_constraints_method': 'advanced',\n",
    "            \"max_depth\": md,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"verbose\": verbose,\n",
    "            \"monotone_constraints\": [monotonicity_3[i]],\n",
    "            \"lambda_l2\": l2,\n",
    "            \"lambda_l1\": l1,\n",
    "            \"min_data_in_leaf\": min_data_in_leaf,\n",
    "            \"min_sum_hessian_in_leaf\": min_sum_hessian_in_leaf,\n",
    "            \"max_bin\": mb,\n",
    "            \"min_data_in_bin\": min_data_in_bin,\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    })\n",
    "\n",
    "monotonicity_4 = [0, 0, 0, -1] if mono else [0]*20\n",
    "# monotonicity_4 = [1] if mono else [0]*20\n",
    "variables_4 = feature_names[3*f_per_utility:]\n",
    "dico4 = []\n",
    "for i, v in enumerate(variables_4):\n",
    "    dico4.append({\n",
    "        \"utility\": [3],\n",
    "        \"variables\": [v],\n",
    "        \"boosting_params\": {\n",
    "            'boosting': 'gbdt',\n",
    "            'monotone_constraints_method': 'advanced',\n",
    "            \"max_depth\": md,\n",
    "            \"n_jobs\": -1,\n",
    "            \"learning_rate\": lr,\n",
    "            \"verbose\": verbose,\n",
    "            \"monotone_constraints\": [monotonicity_4[i]],\n",
    "            \"lambda_l2\": l2,\n",
    "            \"lambda_l1\": l1,\n",
    "            \"min_data_in_leaf\": min_data_in_leaf,\n",
    "            \"min_sum_hessian_in_leaf\": min_sum_hessian_in_leaf,\n",
    "            \"max_bin\": mb,\n",
    "            \"min_data_in_bin\": min_data_in_bin,\n",
    "        },\n",
    "        \"shared\": False,\n",
    "    })\n",
    "\n",
    "# rum_structure = dico1 + dico2 + dico3 #+ dico4\n",
    "if n_utility == 4:\n",
    "    rum_structure = dico1 + dico2 + dico3 + dico4\n",
    "elif n_utility == 3:\n",
    "    rum_structure = dico1 + dico2 + dico3\n",
    "else:\n",
    "    rum_structure = dico1\n",
    "# boost_from_param_space = [False] * len(rum_structure)\n",
    "boost_from_param_space = [True] * len(rum_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c08d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_utility' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# parameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m general_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mn_utility\u001b[49m,  \u001b[38;5;66;03m# important\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# specific RUMBoost parameter\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_iterations\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# \"early_stopping_round\": 10,\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# \"max_booster_to_update\": 23 * 4,\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# \"max_booster_to_update\": 17 * 4,\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# \"max_booster_to_update\": 8,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_booster_to_update\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_utility \u001b[38;5;241m*\u001b[39m f_per_utility,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# \"max_booster_to_update\": 1,\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# \"max_booster_to_update\": n_utility,\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# \"boost_from_parameter_space\": boost_from_param_space + boost_from_param_space2,\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboost_from_parameter_space\u001b[39m\u001b[38;5;124m\"\u001b[39m: boost_from_param_space, \u001b[38;5;66;03m#+ [False]*len(rum_structure2),\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose_interval\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim_interval\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     19\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_utility' is not defined"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "general_params = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_classes\": n_utility,  # important\n",
    "    \"verbosity\": 2,  # specific RUMBoost parameter\n",
    "    \"num_iterations\": 100,\n",
    "    \"objective\": \"regression\",\n",
    "    # \"early_stopping_round\": 10,\n",
    "    # \"max_booster_to_update\": 23 * 4,\n",
    "    # \"max_booster_to_update\": 17 * 4,\n",
    "    # \"max_booster_to_update\": 8,\n",
    "    \"max_booster_to_update\": n_utility * f_per_utility,\n",
    "    # \"max_booster_to_update\": 1,\n",
    "    # \"max_booster_to_update\": n_utility,\n",
    "    # \"boost_from_parameter_space\": boost_from_param_space + boost_from_param_space2,\n",
    "    \"boost_from_parameter_space\": boost_from_param_space, #+ [False]*len(rum_structure2),\n",
    "    \"verbose_interval\": 1,\n",
    "    \"optim_interval\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f46eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensors = {\"device\":\"cuda\"}\n",
    "# torch_tensors = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bea3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_specification = {\n",
    "    \"general_params\": general_params,\n",
    "    \"rum_structure\": rum_structure,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bbc23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm\n",
    "from rumboost.rumboost import rum_train\n",
    "# features and label column names\n",
    "features = [f for f in dataset.columns if f != \"choice\"]\n",
    "label = \"choice\"\n",
    "\n",
    "# create lightgbm dataset\n",
    "lgb_train_set = lightgbm.Dataset(\n",
    "    dataset, label=dataset[label], free_raw_data=False\n",
    ")\n",
    "lgb_test_set = lightgbm.Dataset(\n",
    "    dataset_test, label=dataset_test[label], free_raw_data=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a15ab5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ucesnjs\\OneDrive - University College London\\Documents\\PhD - UCL\\pwl-experiment\\src\\rumboost\\rumboost.py:2723: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch tensors on cuda\n",
      "[LightGBM] [Warning] Unknown parameter: boost_from_parameter_space\n",
      "[LightGBM] [Warning] Unknown parameter: max_booster_to_update\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_interval\n",
      "[LightGBM] [Warning] Unknown parameter: optim_interval\n",
      "[LightGBM] [Warning] Unknown parameter: boost_from_parameter_space\n",
      "[LightGBM] [Warning] Unknown parameter: max_booster_to_update\n",
      "[LightGBM] [Warning] Unknown parameter: verbose_interval\n",
      "[LightGBM] [Warning] Unknown parameter: optim_interval\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000127 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000073 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000084 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.000000\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000001 seconds, init for row-wise cost 0.000099 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000180 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 101\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[1]------NCE value on train set : 0.2330\n",
      "---------NCE value on test set 1: 0.2346\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[2]------NCE value on train set : 0.2144\n",
      "---------NCE value on test set 1: 0.2138\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[3]------NCE value on train set : 0.1933\n",
      "---------NCE value on test set 1: 0.1916\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[4]------NCE value on train set : 0.1784\n",
      "---------NCE value on test set 1: 0.1763\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[5]------NCE value on train set : 0.1702\n",
      "---------NCE value on test set 1: 0.1683\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[6]------NCE value on train set : 0.1659\n",
      "---------NCE value on test set 1: 0.1655\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[7]------NCE value on train set : 0.1670\n",
      "---------NCE value on test set 1: 0.1683\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[8]------NCE value on train set : 0.1602\n",
      "---------NCE value on test set 1: 0.1626\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[9]------NCE value on train set : 0.1513\n",
      "---------NCE value on test set 1: 0.1545\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[10]-----NCE value on train set : 0.1443\n",
      "---------NCE value on test set 1: 0.1481\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[11]-----NCE value on train set : 0.1393\n",
      "---------NCE value on test set 1: 0.1437\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[12]-----NCE value on train set : 0.1340\n",
      "---------NCE value on test set 1: 0.1383\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[13]-----NCE value on train set : 0.1320\n",
      "---------NCE value on test set 1: 0.1360\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[14]-----NCE value on train set : 0.1308\n",
      "---------NCE value on test set 1: 0.1345\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[15]-----NCE value on train set : 0.1303\n",
      "---------NCE value on test set 1: 0.1339\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[16]-----NCE value on train set : 0.1306\n",
      "---------NCE value on test set 1: 0.1342\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[17]-----NCE value on train set : 0.1317\n",
      "---------NCE value on test set 1: 0.1354\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[18]-----NCE value on train set : 0.1337\n",
      "---------NCE value on test set 1: 0.1377\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[19]-----NCE value on train set : 0.1361\n",
      "---------NCE value on test set 1: 0.1402\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[20]-----NCE value on train set : 0.1393\n",
      "---------NCE value on test set 1: 0.1433\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[21]-----NCE value on train set : 0.1432\n",
      "---------NCE value on test set 1: 0.1471\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[22]-----NCE value on train set : 0.1484\n",
      "---------NCE value on test set 1: 0.1518\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[23]-----NCE value on train set : 0.1538\n",
      "---------NCE value on test set 1: 0.1572\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[24]-----NCE value on train set : 0.1599\n",
      "---------NCE value on test set 1: 0.1634\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[25]-----NCE value on train set : 0.1667\n",
      "---------NCE value on test set 1: 0.1703\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[26]-----NCE value on train set : 0.2060\n",
      "---------NCE value on test set 1: 0.2142\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[27]-----NCE value on train set : 0.2245\n",
      "---------NCE value on test set 1: 0.2330\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[28]-----NCE value on train set : 0.2011\n",
      "---------NCE value on test set 1: 0.2079\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[29]-----NCE value on train set : 0.1884\n",
      "---------NCE value on test set 1: 0.1944\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[30]-----NCE value on train set : 0.1635\n",
      "---------NCE value on test set 1: 0.1653\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[31]-----NCE value on train set : 0.2458\n",
      "---------NCE value on test set 1: 0.2423\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[32]-----NCE value on train set : 0.2505\n",
      "---------NCE value on test set 1: 0.2481\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[33]-----NCE value on train set : 0.2234\n",
      "---------NCE value on test set 1: 0.2227\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[34]-----NCE value on train set : 0.2034\n",
      "---------NCE value on test set 1: 0.2041\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[35]-----NCE value on train set : 0.1965\n",
      "---------NCE value on test set 1: 0.1976\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[36]-----NCE value on train set : 0.2115\n",
      "---------NCE value on test set 1: 0.2117\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[37]-----NCE value on train set : 0.2960\n",
      "---------NCE value on test set 1: 0.2960\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[38]-----NCE value on train set : 0.2796\n",
      "---------NCE value on test set 1: 0.2791\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[39]-----NCE value on train set : 0.2271\n",
      "---------NCE value on test set 1: 0.2267\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[40]-----NCE value on train set : 0.1906\n",
      "---------NCE value on test set 1: 0.1914\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[41]-----NCE value on train set : 0.1778\n",
      "---------NCE value on test set 1: 0.1795\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[42]-----NCE value on train set : 0.2123\n",
      "---------NCE value on test set 1: 0.2131\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[43]-----NCE value on train set : 0.2317\n",
      "---------NCE value on test set 1: 0.2317\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[44]-----NCE value on train set : 0.1936\n",
      "---------NCE value on test set 1: 0.1922\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[45]-----NCE value on train set : 0.1688\n",
      "---------NCE value on test set 1: 0.1658\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[46]-----NCE value on train set : 0.1397\n",
      "---------NCE value on test set 1: 0.1356\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[47]-----NCE value on train set : 0.1504\n",
      "---------NCE value on test set 1: 0.1451\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[48]-----NCE value on train set : 0.1790\n",
      "---------NCE value on test set 1: 0.1731\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[49]-----NCE value on train set : 0.1744\n",
      "---------NCE value on test set 1: 0.1695\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[50]-----NCE value on train set : 0.1663\n",
      "---------NCE value on test set 1: 0.1624\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[51]-----NCE value on train set : 0.1425\n",
      "---------NCE value on test set 1: 0.1394\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[52]-----NCE value on train set : 0.1498\n",
      "---------NCE value on test set 1: 0.1475\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[53]-----NCE value on train set : 0.2991\n",
      "---------NCE value on test set 1: 0.3004\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[54]-----NCE value on train set : 0.2055\n",
      "---------NCE value on test set 1: 0.2015\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[55]-----NCE value on train set : 0.2089\n",
      "---------NCE value on test set 1: 0.2046\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[56]-----NCE value on train set : 0.1936\n",
      "---------NCE value on test set 1: 0.1870\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[57]-----NCE value on train set : 0.1833\n",
      "---------NCE value on test set 1: 0.1742\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[58]-----NCE value on train set : 0.1712\n",
      "---------NCE value on test set 1: 0.1558\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[59]-----NCE value on train set : 0.3154\n",
      "---------NCE value on test set 1: 0.2979\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[60]-----NCE value on train set : 0.3623\n",
      "---------NCE value on test set 1: 0.3477\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[61]-----NCE value on train set : 0.3416\n",
      "---------NCE value on test set 1: 0.3321\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[62]-----NCE value on train set : 0.3287\n",
      "---------NCE value on test set 1: 0.3242\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[63]-----NCE value on train set : 0.3153\n",
      "---------NCE value on test set 1: 0.3148\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[64]-----NCE value on train set : 0.3018\n",
      "---------NCE value on test set 1: 0.3045\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[65]-----NCE value on train set : 0.3005\n",
      "---------NCE value on test set 1: 0.3053\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[66]-----NCE value on train set : 0.2969\n",
      "---------NCE value on test set 1: 0.3051\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[67]-----NCE value on train set : 0.3989\n",
      "---------NCE value on test set 1: 0.4057\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[68]-----NCE value on train set : 0.6640\n",
      "---------NCE value on test set 1: 0.6650\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[69]-----NCE value on train set : 0.5993\n",
      "---------NCE value on test set 1: 0.5990\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[70]-----NCE value on train set : 0.5553\n",
      "---------NCE value on test set 1: 0.5544\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[71]-----NCE value on train set : 0.4920\n",
      "---------NCE value on test set 1: 0.4906\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[72]-----NCE value on train set : 0.4115\n",
      "---------NCE value on test set 1: 0.4059\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[73]-----NCE value on train set : 0.3262\n",
      "---------NCE value on test set 1: 0.3173\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[74]-----NCE value on train set : 0.2929\n",
      "---------NCE value on test set 1: 0.2815\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[75]-----NCE value on train set : 0.2945\n",
      "---------NCE value on test set 1: 0.2812\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[76]-----NCE value on train set : 0.6922\n",
      "---------NCE value on test set 1: 0.6827\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[77]-----NCE value on train set : 0.5683\n",
      "---------NCE value on test set 1: 0.5580\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[78]-----NCE value on train set : 0.5046\n",
      "---------NCE value on test set 1: 0.4964\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[79]-----NCE value on train set : 0.3826\n",
      "---------NCE value on test set 1: 0.3773\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[80]-----NCE value on train set : 0.2997\n",
      "---------NCE value on test set 1: 0.2953\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[81]-----NCE value on train set : 0.2556\n",
      "---------NCE value on test set 1: 0.2495\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[82]-----NCE value on train set : 0.2216\n",
      "---------NCE value on test set 1: 0.2094\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[83]-----NCE value on train set : 0.6006\n",
      "---------NCE value on test set 1: 0.5783\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[84]-----NCE value on train set : 0.5582\n",
      "---------NCE value on test set 1: 0.5402\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[85]-----NCE value on train set : 0.4000\n",
      "---------NCE value on test set 1: 0.3816\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[86]-----NCE value on train set : 0.2768\n",
      "---------NCE value on test set 1: 0.2587\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[87]-----NCE value on train set : 0.2318\n",
      "---------NCE value on test set 1: 0.2147\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[88]-----NCE value on train set : 0.5651\n",
      "---------NCE value on test set 1: 0.5621\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[89]-----NCE value on train set : 0.5942\n",
      "---------NCE value on test set 1: 0.5978\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[90]-----NCE value on train set : 0.4215\n",
      "---------NCE value on test set 1: 0.4252\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[91]-----NCE value on train set : 0.3230\n",
      "---------NCE value on test set 1: 0.3257\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[92]-----NCE value on train set : 0.2883\n",
      "---------NCE value on test set 1: 0.2899\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[93]-----NCE value on train set : 0.3374\n",
      "---------NCE value on test set 1: 0.3423\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[94]-----NCE value on train set : 0.3166\n",
      "---------NCE value on test set 1: 0.3194\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[95]-----NCE value on train set : 0.4965\n",
      "---------NCE value on test set 1: 0.4860\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[96]-----NCE value on train set : 0.6196\n",
      "---------NCE value on test set 1: 0.6097\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[97]-----NCE value on train set : 0.5224\n",
      "---------NCE value on test set 1: 0.5170\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[98]-----NCE value on train set : 0.4217\n",
      "---------NCE value on test set 1: 0.4188\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[99]-----NCE value on train set : 0.3382\n",
      "---------NCE value on test set 1: 0.3351\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 2 and depth = 1\n",
      "[100]----NCE value on train set : 0.2991\n",
      "---------NCE value on test set 1: 0.2946\n"
     ]
    }
   ],
   "source": [
    "LPMC_model_fully_trained = rum_train(lgb_train_set, model_specification, valid_sets=[lgb_test_set], torch_tensors=torch_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45cc4fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_plot = np.linspace(1e-4, 1, 1000)\n",
    "\n",
    "y_plot = []\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    plot_dataset = pd.DataFrame({feature_names[i]: np.zeros_like(x_plot) for i in range(f_per_utility*n_utility)})\n",
    "    plot_dataset[feature_names[i]] = x_plot\n",
    "    plot_dataset_lgb = lightgbm.Dataset(plot_dataset, free_raw_data=False)\n",
    "\n",
    "    if n_utility == 2:\n",
    "        y_plot.append(LPMC_model_fully_trained.predict(plot_dataset_lgb, utilities=True))\n",
    "    else:\n",
    "        y_plot.append(LPMC_model_fully_trained.predict(plot_dataset_lgb, utilities=True)[:, i // f_per_utility])\n",
    "    if LPMC_model_fully_trained.device is not None:\n",
    "        y_plot[-1] = y_plot[-1].cpu().numpy()\n",
    "\n",
    "colours = [\"r\", \"g\", \"b\", \"y\", \"k\", \"magenta\", \"cyan\", \"orange\", \"purple\", \"brown\", \"pink\", \"grey\", \"olive\", \"lime\", \"teal\", \"coral\"]\n",
    "\n",
    "\n",
    "if n_utility == 2:\n",
    "    \n",
    "    y_plot_true = []\n",
    "    for i, (sp_i, beta_i, inter_i) in enumerate(zip(sp, betas, intercept)):\n",
    "        if i % f_per_utility == 0:\n",
    "            plt.figure()\n",
    "        y_plot_true.append(apply_linear_feature(x_plot.reshape(-1,1), sp_i.reshape(1,-1), beta_i.reshape(1,-1), feature_names[i], [0]).values)\n",
    "    \n",
    "    y_plot_ttrue = [y_1 - y_0 for y_0, y_1 in zip(y_plot_true[0], y_plot_true[1])]\n",
    "    y_plot_ttrue = np.array(y_plot_ttrue).reshape(-1)\n",
    "\n",
    "    plt.plot(x_plot, y_plot_ttrue, label=f\"{feature_names[0]}_true\", color=colours[0], linewidth=2)\n",
    "        # ascc = ascs[i//f_per_utility].cpu().numpy() if LPMC_model_fully_trained.device is not None else ascs[i//f_per_utility]\n",
    "        # plt.plot(x_plot, y_plot[i]+ascc, label=feature_names[i], color=colours[i], linestyle=\"--\", linewidth=2)\n",
    "    plt.plot(x_plot, y_plot[0], label=feature_names[0], color=colours[0], linestyle=\"--\", linewidth=2)\n",
    "\n",
    "    plt.title(f\"Utility 0\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.grid()\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "else:\n",
    "    for i, (sp_i, beta_i, inter_i) in enumerate(zip(sp, betas, intercept)):\n",
    "        if i % f_per_utility == 0:\n",
    "            plt.figure()\n",
    "        y_plot_true = apply_linear_feature(x_plot.reshape(-1,1), sp_i.reshape(1,-1), beta_i.reshape(1,-1), feature_names[i], [0]).values\n",
    "        # y_plot_true = apply_linear_feature(x_plot.reshape(-1,1), sp_i.reshape(1,-1), beta_i.reshape(1,-1), feature_names[i], [inter_i]).values\n",
    "        plt.plot(x_plot, y_plot_true, label=f\"{feature_names[i]}_true\", color=colours[i], linewidth=2)\n",
    "        # ascc = ascs[i//f_per_utility].cpu().numpy() if LPMC_model_fully_trained.device is not None else ascs[i//f_per_utility]\n",
    "        # plt.plot(x_plot, y_plot[i]+ascc, label=feature_names[i], color=colours[i], linestyle=\"--\", linewidth=2)\n",
    "        plt.plot(x_plot, y_plot[i], label=feature_names[i], color=colours[i], linestyle=\"--\", linewidth=2)\n",
    "\n",
    "\n",
    "\n",
    "        if i % f_per_utility == f_per_utility - 1:\n",
    "            plt.title(f\"Utility {i//f_per_utility}\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.grid()\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.show()\n",
    "\n",
    "    # for i in range(len(feature_names)):\n",
    "    #     plt.figure()\n",
    "    #     plt.hist(dataset[feature_names[i]], bins=150, alpha=0.5, label=feature_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b6efdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "15           5           1       5-S0       5-L0        5-L1         None   \n",
      "16           5           2       5-L0       None        None         5-S0   \n",
      "17           5           2       5-L1       None        None         5-S0   \n",
      "18           6           1       6-S0       6-L0        6-L1         None   \n",
      "19           6           2       6-L0       None        None         6-S0   \n",
      "20           6           2       6-L1       None        None         6-S0   \n",
      "21           7           1       7-S0       7-L0        7-L1         None   \n",
      "22           7           2       7-L0       None        None         7-S0   \n",
      "23           7           2       7-L1       None        None         7-S0   \n",
      "24           8           1       8-S0       8-L0        8-L1         None   \n",
      "25           8           2       8-L0       None        None         8-S0   \n",
      "26           8           2       8-L1       None        None         8-S0   \n",
      "27           9           1       9-S0       9-L0        9-L1         None   \n",
      "28           9           2       9-L0       None        None         9-S0   \n",
      "29           9           2       9-L1       None        None         9-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0             f0   45.012699   0.332292            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3             f0   49.229900   0.332292            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6             f0   45.011002   0.332292            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9             f0   41.677399   0.332292            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12            f0   38.933998   0.332292            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "15            f0   36.237598   0.332292            <=              left   \n",
      "16          None         NaN        NaN          None              None   \n",
      "17          None         NaN        NaN          None              None   \n",
      "18            f0   33.790798   0.347724            <=              left   \n",
      "19          None         NaN        NaN          None              None   \n",
      "20          None         NaN        NaN          None              None   \n",
      "21            f0   31.440300   0.347724            <=              left   \n",
      "22          None         NaN        NaN          None              None   \n",
      "23          None         NaN        NaN          None              None   \n",
      "24            f0   28.652399   0.347724            <=              left   \n",
      "25          None         NaN        NaN          None              None   \n",
      "26          None         NaN        NaN          None              None   \n",
      "27            f0   27.388000   0.347724            <=              left   \n",
      "28          None         NaN        NaN          None              None   \n",
      "29          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None  0.213277    7.191735    648  \n",
      "2          None  0.041346  200.657889   1352  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None  0.235511    7.499696    648  \n",
      "5          None  0.025182  212.385598   1352  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None  0.226100    7.421243    648  \n",
      "8          None  0.024428  213.004878   1352  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None  0.219045    7.366171    648  \n",
      "11         None  0.022875  214.161018   1352  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None  0.213458    7.324920    648  \n",
      "14         None  0.020976  215.527910   1352  \n",
      "15         None  0.000000    0.000000   2000  \n",
      "16         None  0.207353    7.274101    648  \n",
      "17         None  0.019562  216.542621   1352  \n",
      "18         None  0.000000    0.000000   2000  \n",
      "19         None  0.185717    8.382094    678  \n",
      "20         None  0.018918  216.247488   1322  \n",
      "21         None  0.000000    0.000000   2000  \n",
      "22         None  0.180111    8.320399    678  \n",
      "23         None  0.017971  216.918239   1322  \n",
      "24         None  0.000000    0.000000   2000  \n",
      "25         None  0.172632    8.228111    678  \n",
      "26         None  0.017476  217.291163   1322  \n",
      "27         None  0.000000    0.000000   2000  \n",
      "28         None  0.172227    8.242402    678  \n",
      "29         None  0.013444  219.903729   1322  \n",
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "15           5           1       5-S0       5-L0        5-L1         None   \n",
      "16           5           2       5-L0       None        None         5-S0   \n",
      "17           5           2       5-L1       None        None         5-S0   \n",
      "18           6           1       6-S0       6-L0        6-L1         None   \n",
      "19           6           2       6-L0       None        None         6-S0   \n",
      "20           6           2       6-L1       None        None         6-S0   \n",
      "21           7           1       7-S0       7-L0        7-L1         None   \n",
      "22           7           2       7-L0       None        None         7-S0   \n",
      "23           7           2       7-L1       None        None         7-S0   \n",
      "24           8           1       8-S0       8-L0        8-L1         None   \n",
      "25           8           2       8-L0       None        None         8-S0   \n",
      "26           8           2       8-L1       None        None         8-S0   \n",
      "27           9           1       9-S0       9-L0        9-L1         None   \n",
      "28           9           2       9-L0       None        None         9-S0   \n",
      "29           9           2       9-L1       None        None         9-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0             f1     6.67474   0.103098            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3             f1     8.01977   0.103098            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6             f1     6.90074   0.103098            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9             f1     6.42010   0.145076            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12            f1     5.87108   0.174438            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "15            f1     5.32345   0.174438            <=              left   \n",
      "16          None         NaN        NaN          None              None   \n",
      "17          None         NaN        NaN          None              None   \n",
      "18            f1     4.95984   0.174438            <=              left   \n",
      "19          None         NaN        NaN          None              None   \n",
      "20          None         NaN        NaN          None              None   \n",
      "21            f1     4.63552   0.174438            <=              left   \n",
      "22          None         NaN        NaN          None              None   \n",
      "23          None         NaN        NaN          None              None   \n",
      "24            f1     4.03253   0.945707            <=              left   \n",
      "25          None         NaN        NaN          None              None   \n",
      "26          None         NaN        NaN          None              None   \n",
      "27            f1     4.68200   0.945707            <=              left   \n",
      "28          None         NaN        NaN          None              None   \n",
      "29          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None  0.529369    0.235197    211  \n",
      "2          None  0.003642  217.375311   1789  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None  0.582702    0.241075    211  \n",
      "5          None -0.005619  224.016383   1789  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None  0.542929    0.237980    211  \n",
      "8          None -0.004151  222.895058   1789  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None  0.310711    0.681843    295  \n",
      "11         None -0.003388  222.111322   1705  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None  0.219178    1.252338    361  \n",
      "14         None -0.002044  220.742832   1639  \n",
      "15         None  0.000000    0.000000   2000  \n",
      "16         None  0.208623    1.246581    361  \n",
      "17         None -0.001388  220.211592   1639  \n",
      "18         None  0.000000    0.000000   2000  \n",
      "19         None  0.201692    1.243377    361  \n",
      "20         None -0.001403  220.171225   1639  \n",
      "21         None  0.000000    0.000000   2000  \n",
      "22         None  0.195420    1.240499    361  \n",
      "23         None -0.001568  220.236134   1639  \n",
      "24         None  0.000000    0.000000   2000  \n",
      "25         None -0.003274  191.704536   1907  \n",
      "26         None -0.036539   29.332951     93  \n",
      "27         None  0.000000    0.000000   2000  \n",
      "28         None -0.011020  187.106592   1907  \n",
      "29         None -0.032423   28.600740     93  \n",
      "    tree_index  node_depth node_index left_child right_child parent_index  \\\n",
      "0            0           1       0-S0       0-L0        0-L1         None   \n",
      "1            0           2       0-L0       None        None         0-S0   \n",
      "2            0           2       0-L1       None        None         0-S0   \n",
      "3            1           1       1-S0       1-L0        1-L1         None   \n",
      "4            1           2       1-L0       None        None         1-S0   \n",
      "5            1           2       1-L1       None        None         1-S0   \n",
      "6            2           1       2-S0       2-L0        2-L1         None   \n",
      "7            2           2       2-L0       None        None         2-S0   \n",
      "8            2           2       2-L1       None        None         2-S0   \n",
      "9            3           1       3-S0       3-L0        3-L1         None   \n",
      "10           3           2       3-L0       None        None         3-S0   \n",
      "11           3           2       3-L1       None        None         3-S0   \n",
      "12           4           1       4-S0       4-L0        4-L1         None   \n",
      "13           4           2       4-L0       None        None         4-S0   \n",
      "14           4           2       4-L1       None        None         4-S0   \n",
      "15           5           1       5-S0       5-L0        5-L1         None   \n",
      "16           5           2       5-L0       None        None         5-S0   \n",
      "17           5           2       5-L1       None        None         5-S0   \n",
      "18           6           1       6-S0       6-L0        6-L1         None   \n",
      "19           6           2       6-L0       None        None         6-S0   \n",
      "20           6           2       6-L1       None        None         6-S0   \n",
      "21           7           1       7-S0       7-L0        7-L1         None   \n",
      "22           7           2       7-L0       None        None         7-S0   \n",
      "23           7           2       7-L1       None        None         7-S0   \n",
      "24           8           1       8-S0       8-L0        8-L1         None   \n",
      "25           8           2       8-L0       None        None         8-S0   \n",
      "26           8           2       8-L1       None        None         8-S0   \n",
      "27           9           1       9-S0       9-L0        9-L1         None   \n",
      "28           9           2       9-L0       None        None         9-S0   \n",
      "29           9           2       9-L1       None        None         9-S0   \n",
      "\n",
      "   split_feature  split_gain  threshold decision_type missing_direction  \\\n",
      "0             f2   49.281502   0.115291            <=              left   \n",
      "1           None         NaN        NaN          None              None   \n",
      "2           None         NaN        NaN          None              None   \n",
      "3             f2   63.249401   0.175118            <=              left   \n",
      "4           None         NaN        NaN          None              None   \n",
      "5           None         NaN        NaN          None              None   \n",
      "6             f2   53.452900   0.175118            <=              left   \n",
      "7           None         NaN        NaN          None              None   \n",
      "8           None         NaN        NaN          None              None   \n",
      "9             f2   46.762299   0.175118            <=              left   \n",
      "10          None         NaN        NaN          None              None   \n",
      "11          None         NaN        NaN          None              None   \n",
      "12            f2   40.686100   0.186976            <=              left   \n",
      "13          None         NaN        NaN          None              None   \n",
      "14          None         NaN        NaN          None              None   \n",
      "15            f2   35.106602   0.186976            <=              left   \n",
      "16          None         NaN        NaN          None              None   \n",
      "17          None         NaN        NaN          None              None   \n",
      "18            f2   30.679300   0.186976            <=              left   \n",
      "19          None         NaN        NaN          None              None   \n",
      "20          None         NaN        NaN          None              None   \n",
      "21            f2   27.039200   0.332248            <=              left   \n",
      "22          None         NaN        NaN          None              None   \n",
      "23          None         NaN        NaN          None              None   \n",
      "24            f2   24.594101   0.510441            <=              left   \n",
      "25          None         NaN        NaN          None              None   \n",
      "26          None         NaN        NaN          None              None   \n",
      "27            f2   20.293699   0.510441            <=              left   \n",
      "28          None         NaN        NaN          None              None   \n",
      "29          None         NaN        NaN          None              None   \n",
      "\n",
      "   missing_type     value      weight  count  \n",
      "0          None  0.000000    0.000000   2000  \n",
      "1          None -1.174657    0.337891    232  \n",
      "2          None -0.033912  230.792500   1768  \n",
      "3          None  0.000000    0.000000   2000  \n",
      "4          None -0.696453    1.259289    364  \n",
      "5          None -0.014340  213.376575   1636  \n",
      "6          None  0.000000    0.000000   2000  \n",
      "7          None -0.630655    1.288601    364  \n",
      "8          None -0.015341  214.043562   1636  \n",
      "9          None  0.000000    0.000000   2000  \n",
      "10         None -0.585633    1.306430    364  \n",
      "11         None -0.014478  213.150261   1636  \n",
      "12         None  0.000000    0.000000   2000  \n",
      "13         None -0.490079    1.611840    390  \n",
      "14         None -0.014240  212.180974   1610  \n",
      "15         None  0.000000    0.000000   2000  \n",
      "16         None -0.452230    1.629576    390  \n",
      "17         None -0.013703  211.562282   1610  \n",
      "18         None  0.000000    0.000000   2000  \n",
      "19         None -0.421249    1.642152    390  \n",
      "20         None -0.012665  210.521320   1610  \n",
      "21         None  0.000000    0.000000   2000  \n",
      "22         None -0.175291    7.706107    647  \n",
      "23         None -0.015540  203.523108   1353  \n",
      "24         None  0.000000    0.000000   2000  \n",
      "25         None -0.073894   30.512923   1023  \n",
      "26         None -0.023167  180.755228    977  \n",
      "27         None  0.000000    0.000000   2000  \n",
      "28         None -0.062328   30.974972   1023  \n",
      "29         None -0.025213  182.521492    977  \n"
     ]
    }
   ],
   "source": [
    "for booster in LPMC_model_fully_trained.boosters:\n",
    "    try:\n",
    "        print(booster.trees_to_dataframe())\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3103c7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0. Boosting step: 1. Model terms: 1. Terms eligible: 4. Validation error: 3.15812.\n",
      "Fold: 0. Boosting step: 2. Model terms: 1. Terms eligible: 4. Validation error: 2.21356.\n",
      "Fold: 0. Boosting step: 3. Model terms: 2. Terms eligible: 4. Validation error: 1.52958.\n",
      "Fold: 0. Boosting step: 4. Model terms: 2. Terms eligible: 4. Validation error: 1.0673.\n",
      "Fold: 0. Boosting step: 5. Model terms: 2. Terms eligible: 4. Validation error: 0.75469.\n",
      "Fold: 0. Boosting step: 6. Model terms: 2. Terms eligible: 4. Validation error: 0.534991.\n",
      "Fold: 0. Boosting step: 7. Model terms: 3. Terms eligible: 4. Validation error: 0.385859.\n",
      "Fold: 0. Boosting step: 8. Model terms: 3. Terms eligible: 4. Validation error: 0.28136.\n",
      "Fold: 0. Boosting step: 9. Model terms: 3. Terms eligible: 4. Validation error: 0.210393.\n",
      "Fold: 0. Boosting step: 10. Model terms: 3. Terms eligible: 4. Validation error: 0.160711.\n",
      "Fold: 0. Boosting step: 11. Model terms: 3. Terms eligible: 4. Validation error: 0.127016.\n",
      "Fold: 0. Boosting step: 12. Model terms: 4. Terms eligible: 4. Validation error: 0.101269.\n",
      "Fold: 0. Boosting step: 13. Model terms: 5. Terms eligible: 4. Validation error: 0.0824559.\n",
      "Fold: 0. Boosting step: 14. Model terms: 5. Terms eligible: 4. Validation error: 0.0692895.\n",
      "Fold: 0. Boosting step: 15. Model terms: 6. Terms eligible: 4. Validation error: 0.0586282.\n",
      "Fold: 0. Boosting step: 16. Model terms: 7. Terms eligible: 4. Validation error: 0.0507776.\n",
      "Fold: 0. Boosting step: 17. Model terms: 8. Terms eligible: 4. Validation error: 0.0452987.\n",
      "Fold: 0. Boosting step: 18. Model terms: 9. Terms eligible: 4. Validation error: 0.040465.\n",
      "Fold: 0. Boosting step: 19. Model terms: 10. Terms eligible: 4. Validation error: 0.0372275.\n",
      "Fold: 0. Boosting step: 20. Model terms: 11. Terms eligible: 4. Validation error: 0.0341395.\n",
      "Fold: 0. Boosting step: 21. Model terms: 12. Terms eligible: 4. Validation error: 0.0318185.\n",
      "Fold: 0. Boosting step: 22. Model terms: 13. Terms eligible: 4. Validation error: 0.0300973.\n",
      "Fold: 0. Boosting step: 23. Model terms: 14. Terms eligible: 4. Validation error: 0.028532.\n",
      "Fold: 0. Boosting step: 24. Model terms: 15. Terms eligible: 4. Validation error: 0.0273925.\n",
      "Fold: 0. Boosting step: 25. Model terms: 16. Terms eligible: 4. Validation error: 0.0262779.\n",
      "Fold: 0. Boosting step: 26. Model terms: 17. Terms eligible: 4. Validation error: 0.0254784.\n",
      "Fold: 0. Boosting step: 27. Model terms: 18. Terms eligible: 4. Validation error: 0.0246795.\n",
      "Fold: 0. Boosting step: 28. Model terms: 19. Terms eligible: 4. Validation error: 0.0237749.\n",
      "Fold: 0. Boosting step: 29. Model terms: 20. Terms eligible: 4. Validation error: 0.0230464.\n",
      "Fold: 0. Boosting step: 30. Model terms: 20. Terms eligible: 4. Validation error: 0.0222652.\n",
      "Fold: 0. Boosting step: 31. Model terms: 21. Terms eligible: 4. Validation error: 0.0216012.\n",
      "Fold: 0. Boosting step: 32. Model terms: 22. Terms eligible: 4. Validation error: 0.0210456.\n",
      "Fold: 0. Boosting step: 33. Model terms: 22. Terms eligible: 4. Validation error: 0.0204953.\n",
      "Fold: 0. Boosting step: 34. Model terms: 22. Terms eligible: 4. Validation error: 0.0197754.\n",
      "Fold: 0. Boosting step: 35. Model terms: 23. Terms eligible: 4. Validation error: 0.0192702.\n",
      "Fold: 0. Boosting step: 36. Model terms: 24. Terms eligible: 4. Validation error: 0.0186413.\n",
      "Fold: 0. Boosting step: 37. Model terms: 25. Terms eligible: 4. Validation error: 0.0181786.\n",
      "Fold: 0. Boosting step: 38. Model terms: 26. Terms eligible: 4. Validation error: 0.0176264.\n",
      "Fold: 0. Boosting step: 39. Model terms: 27. Terms eligible: 4. Validation error: 0.0172013.\n",
      "Fold: 0. Boosting step: 40. Model terms: 28. Terms eligible: 4. Validation error: 0.0168007.\n",
      "Fold: 0. Boosting step: 41. Model terms: 29. Terms eligible: 4. Validation error: 0.0163162.\n",
      "Fold: 0. Boosting step: 42. Model terms: 30. Terms eligible: 4. Validation error: 0.0159278.\n",
      "Fold: 0. Boosting step: 43. Model terms: 30. Terms eligible: 4. Validation error: 0.0154933.\n",
      "Fold: 0. Boosting step: 44. Model terms: 31. Terms eligible: 4. Validation error: 0.015142.\n",
      "Fold: 0. Boosting step: 45. Model terms: 31. Terms eligible: 4. Validation error: 0.0148254.\n",
      "Fold: 0. Boosting step: 46. Model terms: 31. Terms eligible: 4. Validation error: 0.0144332.\n",
      "Fold: 0. Boosting step: 47. Model terms: 32. Terms eligible: 4. Validation error: 0.0141124.\n",
      "Fold: 0. Boosting step: 48. Model terms: 33. Terms eligible: 4. Validation error: 0.0137567.\n",
      "Fold: 0. Boosting step: 49. Model terms: 34. Terms eligible: 4. Validation error: 0.0134684.\n",
      "Fold: 0. Boosting step: 50. Model terms: 34. Terms eligible: 4. Validation error: 0.0132097.\n",
      "Fold: 0. Boosting step: 51. Model terms: 35. Terms eligible: 4. Validation error: 0.0128832.\n",
      "Fold: 0. Boosting step: 52. Model terms: 36. Terms eligible: 4. Validation error: 0.0126226.\n",
      "Fold: 0. Boosting step: 53. Model terms: 37. Terms eligible: 4. Validation error: 0.0123251.\n",
      "Fold: 0. Boosting step: 54. Model terms: 38. Terms eligible: 4. Validation error: 0.0121064.\n",
      "Fold: 0. Boosting step: 55. Model terms: 39. Terms eligible: 4. Validation error: 0.0118714.\n",
      "Fold: 0. Boosting step: 56. Model terms: 39. Terms eligible: 4. Validation error: 0.0115941.\n",
      "Fold: 0. Boosting step: 57. Model terms: 40. Terms eligible: 4. Validation error: 0.0113829.\n",
      "Fold: 0. Boosting step: 58. Model terms: 41. Terms eligible: 4. Validation error: 0.011129.\n",
      "Fold: 0. Boosting step: 59. Model terms: 42. Terms eligible: 4. Validation error: 0.0109427.\n",
      "Fold: 0. Boosting step: 60. Model terms: 43. Terms eligible: 4. Validation error: 0.0107239.\n",
      "Fold: 0. Boosting step: 61. Model terms: 44. Terms eligible: 4. Validation error: 0.0105131.\n",
      "Fold: 0. Boosting step: 62. Model terms: 45. Terms eligible: 4. Validation error: 0.0103546.\n",
      "Fold: 0. Boosting step: 63. Model terms: 46. Terms eligible: 4. Validation error: 0.010144.\n",
      "Fold: 0. Boosting step: 64. Model terms: 47. Terms eligible: 4. Validation error: 0.00995978.\n",
      "Fold: 0. Boosting step: 65. Model terms: 48. Terms eligible: 4. Validation error: 0.0097638.\n",
      "Fold: 0. Boosting step: 66. Model terms: 49. Terms eligible: 4. Validation error: 0.00962256.\n",
      "Fold: 0. Boosting step: 67. Model terms: 50. Terms eligible: 4. Validation error: 0.00946118.\n",
      "Fold: 0. Boosting step: 68. Model terms: 51. Terms eligible: 4. Validation error: 0.00927333.\n",
      "Fold: 0. Boosting step: 69. Model terms: 52. Terms eligible: 4. Validation error: 0.00916281.\n",
      "Fold: 0. Boosting step: 70. Model terms: 53. Terms eligible: 4. Validation error: 0.00899785.\n",
      "Fold: 0. Boosting step: 71. Model terms: 54. Terms eligible: 4. Validation error: 0.00884207.\n",
      "Fold: 0. Boosting step: 72. Model terms: 55. Terms eligible: 4. Validation error: 0.00868721.\n",
      "Fold: 0. Boosting step: 73. Model terms: 56. Terms eligible: 4. Validation error: 0.00856563.\n",
      "Fold: 0. Boosting step: 74. Model terms: 57. Terms eligible: 4. Validation error: 0.00843105.\n",
      "Fold: 0. Boosting step: 75. Model terms: 58. Terms eligible: 4. Validation error: 0.00828064.\n",
      "Fold: 0. Boosting step: 76. Model terms: 59. Terms eligible: 4. Validation error: 0.0081878.\n",
      "Fold: 0. Boosting step: 77. Model terms: 60. Terms eligible: 4. Validation error: 0.0080543.\n",
      "Fold: 0. Boosting step: 78. Model terms: 61. Terms eligible: 4. Validation error: 0.00792612.\n",
      "Fold: 0. Boosting step: 79. Model terms: 62. Terms eligible: 4. Validation error: 0.00782506.\n",
      "Fold: 0. Boosting step: 80. Model terms: 63. Terms eligible: 4. Validation error: 0.00769404.\n",
      "Fold: 0. Boosting step: 81. Model terms: 64. Terms eligible: 4. Validation error: 0.00758396.\n",
      "Fold: 0. Boosting step: 82. Model terms: 65. Terms eligible: 4. Validation error: 0.00746114.\n",
      "Fold: 0. Boosting step: 83. Model terms: 66. Terms eligible: 4. Validation error: 0.00736888.\n",
      "Fold: 0. Boosting step: 84. Model terms: 66. Terms eligible: 4. Validation error: 0.00729455.\n",
      "Fold: 0. Boosting step: 85. Model terms: 67. Terms eligible: 4. Validation error: 0.00717975.\n",
      "Fold: 0. Boosting step: 86. Model terms: 68. Terms eligible: 4. Validation error: 0.00707652.\n",
      "Fold: 0. Boosting step: 87. Model terms: 69. Terms eligible: 4. Validation error: 0.0069684.\n",
      "Fold: 0. Boosting step: 88. Model terms: 70. Terms eligible: 4. Validation error: 0.0068879.\n",
      "Fold: 0. Boosting step: 89. Model terms: 70. Terms eligible: 4. Validation error: 0.0067894.\n",
      "Fold: 0. Boosting step: 90. Model terms: 71. Terms eligible: 4. Validation error: 0.00669363.\n",
      "Fold: 0. Boosting step: 91. Model terms: 71. Terms eligible: 4. Validation error: 0.00663062.\n",
      "Fold: 0. Boosting step: 92. Model terms: 72. Terms eligible: 4. Validation error: 0.00653398.\n",
      "Fold: 0. Boosting step: 93. Model terms: 72. Terms eligible: 4. Validation error: 0.00646336.\n",
      "Fold: 0. Boosting step: 94. Model terms: 72. Terms eligible: 4. Validation error: 0.00637468.\n",
      "Fold: 0. Boosting step: 95. Model terms: 73. Terms eligible: 4. Validation error: 0.00628844.\n",
      "Fold: 0. Boosting step: 96. Model terms: 74. Terms eligible: 4. Validation error: 0.00620455.\n",
      "Fold: 0. Boosting step: 97. Model terms: 75. Terms eligible: 4. Validation error: 0.00614798.\n",
      "Fold: 0. Boosting step: 98. Model terms: 76. Terms eligible: 4. Validation error: 0.00608575.\n",
      "Fold: 0. Boosting step: 99. Model terms: 77. Terms eligible: 4. Validation error: 0.00600526.\n",
      "Fold: 0. Boosting step: 100. Model terms: 78. Terms eligible: 4. Validation error: 0.0059286.\n",
      "Fold: 0. Boosting step: 101. Model terms: 79. Terms eligible: 4. Validation error: 0.00585245.\n",
      "Fold: 0. Boosting step: 102. Model terms: 80. Terms eligible: 4. Validation error: 0.00579461.\n",
      "Fold: 0. Boosting step: 103. Model terms: 81. Terms eligible: 4. Validation error: 0.00574827.\n",
      "Fold: 0. Boosting step: 104. Model terms: 82. Terms eligible: 4. Validation error: 0.00567509.\n",
      "Fold: 0. Boosting step: 105. Model terms: 83. Terms eligible: 4. Validation error: 0.00560747.\n",
      "Fold: 0. Boosting step: 106. Model terms: 84. Terms eligible: 4. Validation error: 0.00553825.\n",
      "Fold: 0. Boosting step: 107. Model terms: 84. Terms eligible: 4. Validation error: 0.00548691.\n",
      "Fold: 0. Boosting step: 108. Model terms: 85. Terms eligible: 4. Validation error: 0.00542282.\n",
      "Fold: 0. Boosting step: 109. Model terms: 86. Terms eligible: 4. Validation error: 0.00536163.\n",
      "Fold: 0. Boosting step: 110. Model terms: 87. Terms eligible: 4. Validation error: 0.0053226.\n",
      "Fold: 0. Boosting step: 111. Model terms: 88. Terms eligible: 4. Validation error: 0.00525945.\n",
      "Fold: 0. Boosting step: 112. Model terms: 89. Terms eligible: 4. Validation error: 0.00521392.\n",
      "Fold: 0. Boosting step: 113. Model terms: 89. Terms eligible: 4. Validation error: 0.00515531.\n",
      "Fold: 0. Boosting step: 114. Model terms: 90. Terms eligible: 4. Validation error: 0.00509723.\n",
      "Fold: 0. Boosting step: 115. Model terms: 91. Terms eligible: 4. Validation error: 0.00504166.\n",
      "Fold: 0. Boosting step: 116. Model terms: 92. Terms eligible: 4. Validation error: 0.00499906.\n",
      "Fold: 0. Boosting step: 117. Model terms: 92. Terms eligible: 4. Validation error: 0.00496589.\n",
      "Fold: 0. Boosting step: 118. Model terms: 93. Terms eligible: 4. Validation error: 0.00491226.\n",
      "Fold: 0. Boosting step: 119. Model terms: 93. Terms eligible: 4. Validation error: 0.00486395.\n",
      "Fold: 0. Boosting step: 120. Model terms: 94. Terms eligible: 4. Validation error: 0.00481318.\n",
      "Fold: 0. Boosting step: 121. Model terms: 95. Terms eligible: 4. Validation error: 0.00477538.\n",
      "Fold: 0. Boosting step: 122. Model terms: 96. Terms eligible: 4. Validation error: 0.00472822.\n",
      "Fold: 0. Boosting step: 123. Model terms: 97. Terms eligible: 4. Validation error: 0.00468066.\n",
      "Fold: 0. Boosting step: 124. Model terms: 98. Terms eligible: 4. Validation error: 0.0046473.\n",
      "Fold: 0. Boosting step: 125. Model terms: 99. Terms eligible: 4. Validation error: 0.00460023.\n",
      "Fold: 0. Boosting step: 126. Model terms: 100. Terms eligible: 4. Validation error: 0.00457251.\n",
      "Fold: 0. Boosting step: 127. Model terms: 101. Terms eligible: 4. Validation error: 0.00452926.\n",
      "Fold: 0. Boosting step: 128. Model terms: 102. Terms eligible: 4. Validation error: 0.0044896.\n",
      "Fold: 0. Boosting step: 129. Model terms: 103. Terms eligible: 4. Validation error: 0.00446002.\n",
      "Fold: 0. Boosting step: 130. Model terms: 104. Terms eligible: 4. Validation error: 0.00441684.\n",
      "Fold: 0. Boosting step: 131. Model terms: 104. Terms eligible: 4. Validation error: 0.00439278.\n",
      "Fold: 0. Boosting step: 132. Model terms: 105. Terms eligible: 4. Validation error: 0.00435294.\n",
      "Fold: 0. Boosting step: 133. Model terms: 106. Terms eligible: 4. Validation error: 0.00431261.\n",
      "Fold: 0. Boosting step: 134. Model terms: 107. Terms eligible: 4. Validation error: 0.00427465.\n",
      "Fold: 0. Boosting step: 135. Model terms: 108. Terms eligible: 4. Validation error: 0.00424699.\n",
      "Fold: 0. Boosting step: 136. Model terms: 108. Terms eligible: 4. Validation error: 0.00421165.\n",
      "Fold: 0. Boosting step: 137. Model terms: 108. Terms eligible: 4. Validation error: 0.00417823.\n",
      "Fold: 0. Boosting step: 138. Model terms: 108. Terms eligible: 4. Validation error: 0.00415396.\n",
      "Fold: 0. Boosting step: 139. Model terms: 109. Terms eligible: 4. Validation error: 0.00411857.\n",
      "Fold: 0. Boosting step: 140. Model terms: 110. Terms eligible: 4. Validation error: 0.00409806.\n",
      "Fold: 0. Boosting step: 141. Model terms: 111. Terms eligible: 4. Validation error: 0.00406534.\n",
      "Fold: 0. Boosting step: 142. Model terms: 112. Terms eligible: 4. Validation error: 0.00403137.\n",
      "Fold: 0. Boosting step: 143. Model terms: 113. Terms eligible: 4. Validation error: 0.0040002.\n",
      "Fold: 0. Boosting step: 144. Model terms: 114. Terms eligible: 4. Validation error: 0.00397756.\n",
      "Fold: 0. Boosting step: 145. Model terms: 115. Terms eligible: 4. Validation error: 0.0039551.\n",
      "Fold: 0. Boosting step: 146. Model terms: 116. Terms eligible: 4. Validation error: 0.00392478.\n",
      "Fold: 0. Boosting step: 147. Model terms: 117. Terms eligible: 4. Validation error: 0.00389474.\n",
      "Fold: 0. Boosting step: 148. Model terms: 118. Terms eligible: 4. Validation error: 0.0038659.\n",
      "Fold: 0. Boosting step: 149. Model terms: 119. Terms eligible: 4. Validation error: 0.0038456.\n",
      "Fold: 0. Boosting step: 150. Model terms: 119. Terms eligible: 4. Validation error: 0.00382916.\n",
      "Fold: 0. Boosting step: 151. Model terms: 119. Terms eligible: 4. Validation error: 0.00380109.\n",
      "Fold: 0. Boosting step: 152. Model terms: 119. Terms eligible: 4. Validation error: 0.00377598.\n",
      "Fold: 0. Boosting step: 153. Model terms: 120. Terms eligible: 4. Validation error: 0.00374928.\n",
      "Fold: 0. Boosting step: 154. Model terms: 121. Terms eligible: 4. Validation error: 0.00372989.\n",
      "Fold: 0. Boosting step: 155. Model terms: 121. Terms eligible: 4. Validation error: 0.00371258.\n",
      "Fold: 0. Boosting step: 156. Model terms: 121. Terms eligible: 4. Validation error: 0.00368656.\n",
      "Fold: 0. Boosting step: 157. Model terms: 122. Terms eligible: 4. Validation error: 0.00366145.\n",
      "Fold: 0. Boosting step: 158. Model terms: 123. Terms eligible: 4. Validation error: 0.0036367.\n",
      "Fold: 0. Boosting step: 159. Model terms: 124. Terms eligible: 4. Validation error: 0.00362037.\n",
      "Fold: 0. Boosting step: 160. Model terms: 125. Terms eligible: 4. Validation error: 0.00359728.\n",
      "Fold: 0. Boosting step: 161. Model terms: 126. Terms eligible: 4. Validation error: 0.00358306.\n",
      "Fold: 0. Boosting step: 162. Model terms: 127. Terms eligible: 4. Validation error: 0.00355985.\n",
      "Fold: 0. Boosting step: 163. Model terms: 128. Terms eligible: 4. Validation error: 0.00353684.\n",
      "Fold: 0. Boosting step: 164. Model terms: 128. Terms eligible: 4. Validation error: 0.00352066.\n",
      "Fold: 0. Boosting step: 165. Model terms: 129. Terms eligible: 4. Validation error: 0.00349944.\n",
      "Fold: 0. Boosting step: 166. Model terms: 130. Terms eligible: 4. Validation error: 0.00348477.\n",
      "Fold: 0. Boosting step: 167. Model terms: 131. Terms eligible: 4. Validation error: 0.00346577.\n",
      "Fold: 0. Boosting step: 168. Model terms: 132. Terms eligible: 4. Validation error: 0.00344448.\n",
      "Fold: 0. Boosting step: 169. Model terms: 132. Terms eligible: 4. Validation error: 0.00343297.\n",
      "Fold: 0. Boosting step: 170. Model terms: 132. Terms eligible: 4. Validation error: 0.00341325.\n",
      "Fold: 0. Boosting step: 171. Model terms: 133. Terms eligible: 4. Validation error: 0.00339888.\n",
      "Fold: 0. Boosting step: 172. Model terms: 134. Terms eligible: 4. Validation error: 0.00337929.\n",
      "Fold: 0. Boosting step: 173. Model terms: 135. Terms eligible: 4. Validation error: 0.00335967.\n",
      "Fold: 0. Boosting step: 174. Model terms: 136. Terms eligible: 4. Validation error: 0.00334736.\n",
      "Fold: 0. Boosting step: 175. Model terms: 137. Terms eligible: 4. Validation error: 0.00332905.\n",
      "Fold: 0. Boosting step: 176. Model terms: 137. Terms eligible: 4. Validation error: 0.00331282.\n",
      "Fold: 0. Boosting step: 177. Model terms: 138. Terms eligible: 4. Validation error: 0.0032954.\n",
      "Fold: 0. Boosting step: 178. Model terms: 139. Terms eligible: 4. Validation error: 0.00328548.\n",
      "Fold: 0. Boosting step: 179. Model terms: 140. Terms eligible: 4. Validation error: 0.00327447.\n",
      "Fold: 0. Boosting step: 180. Model terms: 141. Terms eligible: 4. Validation error: 0.00325742.\n",
      "Fold: 0. Boosting step: 181. Model terms: 141. Terms eligible: 4. Validation error: 0.00324531.\n",
      "Fold: 0. Boosting step: 182. Model terms: 142. Terms eligible: 4. Validation error: 0.00322927.\n",
      "Fold: 0. Boosting step: 183. Model terms: 143. Terms eligible: 4. Validation error: 0.00321238.\n",
      "Fold: 0. Boosting step: 184. Model terms: 144. Terms eligible: 4. Validation error: 0.00320244.\n",
      "Fold: 0. Boosting step: 185. Model terms: 145. Terms eligible: 4. Validation error: 0.00318666.\n",
      "Fold: 0. Boosting step: 186. Model terms: 146. Terms eligible: 4. Validation error: 0.00317175.\n",
      "Fold: 0. Boosting step: 187. Model terms: 147. Terms eligible: 4. Validation error: 0.00315673.\n",
      "Fold: 0. Boosting step: 188. Model terms: 148. Terms eligible: 4. Validation error: 0.00314735.\n",
      "Fold: 0. Boosting step: 189. Model terms: 149. Terms eligible: 4. Validation error: 0.0031333.\n",
      "Fold: 0. Boosting step: 190. Model terms: 150. Terms eligible: 4. Validation error: 0.00312293.\n",
      "Fold: 0. Boosting step: 191. Model terms: 151. Terms eligible: 4. Validation error: 0.0031109.\n",
      "Fold: 0. Boosting step: 192. Model terms: 151. Terms eligible: 4. Validation error: 0.00309657.\n",
      "Fold: 0. Boosting step: 193. Model terms: 151. Terms eligible: 4. Validation error: 0.00308877.\n",
      "Fold: 0. Boosting step: 194. Model terms: 151. Terms eligible: 4. Validation error: 0.0030754.\n",
      "Fold: 0. Boosting step: 195. Model terms: 151. Terms eligible: 4. Validation error: 0.00306277.\n",
      "Fold: 0. Boosting step: 196. Model terms: 151. Terms eligible: 4. Validation error: 0.00305459.\n",
      "Fold: 0. Boosting step: 197. Model terms: 152. Terms eligible: 4. Validation error: 0.00304115.\n",
      "Fold: 0. Boosting step: 198. Model terms: 153. Terms eligible: 4. Validation error: 0.00302959.\n",
      "Fold: 0. Boosting step: 199. Model terms: 153. Terms eligible: 4. Validation error: 0.00301647.\n",
      "Fold: 0. Boosting step: 200. Model terms: 154. Terms eligible: 4. Validation error: 0.00300795.\n",
      "Model terms: 154. Terms available in final boosting step: 4.\n",
      "Fold: 1. Boosting step: 1. Model terms: 1. Terms eligible: 4. Validation error: 3.16581.\n",
      "Fold: 1. Boosting step: 2. Model terms: 2. Terms eligible: 4. Validation error: 2.21026.\n",
      "Fold: 1. Boosting step: 3. Model terms: 2. Terms eligible: 4. Validation error: 1.53331.\n",
      "Fold: 1. Boosting step: 4. Model terms: 2. Terms eligible: 4. Validation error: 1.07929.\n",
      "Fold: 1. Boosting step: 5. Model terms: 3. Terms eligible: 4. Validation error: 0.755676.\n",
      "Fold: 1. Boosting step: 6. Model terms: 3. Terms eligible: 4. Validation error: 0.540233.\n",
      "Fold: 1. Boosting step: 7. Model terms: 3. Terms eligible: 4. Validation error: 0.385805.\n",
      "Fold: 1. Boosting step: 8. Model terms: 3. Terms eligible: 4. Validation error: 0.28361.\n",
      "Fold: 1. Boosting step: 9. Model terms: 3. Terms eligible: 4. Validation error: 0.209602.\n",
      "Fold: 1. Boosting step: 10. Model terms: 3. Terms eligible: 4. Validation error: 0.159535.\n",
      "Fold: 1. Boosting step: 11. Model terms: 4. Terms eligible: 4. Validation error: 0.125005.\n",
      "Fold: 1. Boosting step: 12. Model terms: 5. Terms eligible: 4. Validation error: 0.0999399.\n",
      "Fold: 1. Boosting step: 13. Model terms: 5. Terms eligible: 4. Validation error: 0.0804607.\n",
      "Fold: 1. Boosting step: 14. Model terms: 6. Terms eligible: 4. Validation error: 0.0666493.\n",
      "Fold: 1. Boosting step: 15. Model terms: 7. Terms eligible: 4. Validation error: 0.0565587.\n",
      "Fold: 1. Boosting step: 16. Model terms: 8. Terms eligible: 4. Validation error: 0.0491776.\n",
      "Fold: 1. Boosting step: 17. Model terms: 9. Terms eligible: 4. Validation error: 0.0426807.\n",
      "Fold: 1. Boosting step: 18. Model terms: 10. Terms eligible: 4. Validation error: 0.0382927.\n",
      "Fold: 1. Boosting step: 19. Model terms: 11. Terms eligible: 4. Validation error: 0.0343536.\n",
      "Fold: 1. Boosting step: 20. Model terms: 12. Terms eligible: 4. Validation error: 0.0316693.\n",
      "Fold: 1. Boosting step: 21. Model terms: 13. Terms eligible: 4. Validation error: 0.029195.\n",
      "Fold: 1. Boosting step: 22. Model terms: 14. Terms eligible: 4. Validation error: 0.027491.\n",
      "Fold: 1. Boosting step: 23. Model terms: 15. Terms eligible: 4. Validation error: 0.0258674.\n",
      "Fold: 1. Boosting step: 24. Model terms: 16. Terms eligible: 4. Validation error: 0.0247349.\n",
      "Fold: 1. Boosting step: 25. Model terms: 17. Terms eligible: 4. Validation error: 0.0236134.\n",
      "Fold: 1. Boosting step: 26. Model terms: 18. Terms eligible: 4. Validation error: 0.0228206.\n",
      "Fold: 1. Boosting step: 27. Model terms: 19. Terms eligible: 4. Validation error: 0.0220002.\n",
      "Fold: 1. Boosting step: 28. Model terms: 20. Terms eligible: 4. Validation error: 0.0212083.\n",
      "Fold: 1. Boosting step: 29. Model terms: 21. Terms eligible: 4. Validation error: 0.0205647.\n",
      "Fold: 1. Boosting step: 30. Model terms: 22. Terms eligible: 4. Validation error: 0.0198495.\n",
      "Fold: 1. Boosting step: 31. Model terms: 23. Terms eligible: 4. Validation error: 0.0192928.\n",
      "Fold: 1. Boosting step: 32. Model terms: 24. Terms eligible: 4. Validation error: 0.0186497.\n",
      "Fold: 1. Boosting step: 33. Model terms: 25. Terms eligible: 4. Validation error: 0.0180437.\n",
      "Fold: 1. Boosting step: 34. Model terms: 26. Terms eligible: 4. Validation error: 0.0175698.\n",
      "Fold: 1. Boosting step: 35. Model terms: 27. Terms eligible: 4. Validation error: 0.016993.\n",
      "Fold: 1. Boosting step: 36. Model terms: 27. Terms eligible: 4. Validation error: 0.0165714.\n",
      "Fold: 1. Boosting step: 37. Model terms: 28. Terms eligible: 4. Validation error: 0.0160577.\n",
      "Fold: 1. Boosting step: 38. Model terms: 29. Terms eligible: 4. Validation error: 0.0155823.\n",
      "Fold: 1. Boosting step: 39. Model terms: 30. Terms eligible: 4. Validation error: 0.0152099.\n",
      "Fold: 1. Boosting step: 40. Model terms: 31. Terms eligible: 4. Validation error: 0.0147522.\n",
      "Fold: 1. Boosting step: 41. Model terms: 32. Terms eligible: 4. Validation error: 0.0144158.\n",
      "Fold: 1. Boosting step: 42. Model terms: 33. Terms eligible: 4. Validation error: 0.014009.\n",
      "Fold: 1. Boosting step: 43. Model terms: 33. Terms eligible: 4. Validation error: 0.013622.\n",
      "Fold: 1. Boosting step: 44. Model terms: 34. Terms eligible: 4. Validation error: 0.0133186.\n",
      "Fold: 1. Boosting step: 45. Model terms: 35. Terms eligible: 4. Validation error: 0.0129585.\n",
      "Fold: 1. Boosting step: 46. Model terms: 36. Terms eligible: 4. Validation error: 0.012682.\n",
      "Fold: 1. Boosting step: 47. Model terms: 37. Terms eligible: 4. Validation error: 0.0123636.\n",
      "Fold: 1. Boosting step: 48. Model terms: 37. Terms eligible: 4. Validation error: 0.0120417.\n",
      "Fold: 1. Boosting step: 49. Model terms: 38. Terms eligible: 4. Validation error: 0.011789.\n",
      "Fold: 1. Boosting step: 50. Model terms: 39. Terms eligible: 4. Validation error: 0.0115075.\n",
      "Fold: 1. Boosting step: 51. Model terms: 40. Terms eligible: 4. Validation error: 0.0112762.\n",
      "Fold: 1. Boosting step: 52. Model terms: 40. Terms eligible: 4. Validation error: 0.0110048.\n",
      "Fold: 1. Boosting step: 53. Model terms: 41. Terms eligible: 4. Validation error: 0.0107572.\n",
      "Fold: 1. Boosting step: 54. Model terms: 41. Terms eligible: 4. Validation error: 0.0105425.\n",
      "Fold: 1. Boosting step: 55. Model terms: 42. Terms eligible: 4. Validation error: 0.0103236.\n",
      "Fold: 1. Boosting step: 56. Model terms: 43. Terms eligible: 4. Validation error: 0.0101269.\n",
      "Fold: 1. Boosting step: 57. Model terms: 44. Terms eligible: 4. Validation error: 0.00989325.\n",
      "Fold: 1. Boosting step: 58. Model terms: 45. Terms eligible: 4. Validation error: 0.0097001.\n",
      "Fold: 1. Boosting step: 59. Model terms: 46. Terms eligible: 4. Validation error: 0.00951684.\n",
      "Fold: 1. Boosting step: 60. Model terms: 46. Terms eligible: 4. Validation error: 0.00931763.\n",
      "Fold: 1. Boosting step: 61. Model terms: 47. Terms eligible: 4. Validation error: 0.00916368.\n",
      "Fold: 1. Boosting step: 62. Model terms: 48. Terms eligible: 4. Validation error: 0.00897789.\n",
      "Fold: 1. Boosting step: 63. Model terms: 49. Terms eligible: 4. Validation error: 0.0088331.\n",
      "Fold: 1. Boosting step: 64. Model terms: 50. Terms eligible: 4. Validation error: 0.00867177.\n",
      "Fold: 1. Boosting step: 65. Model terms: 50. Terms eligible: 4. Validation error: 0.00849435.\n",
      "Fold: 1. Boosting step: 66. Model terms: 51. Terms eligible: 4. Validation error: 0.00835594.\n",
      "Fold: 1. Boosting step: 67. Model terms: 52. Terms eligible: 4. Validation error: 0.00821586.\n",
      "Fold: 1. Boosting step: 68. Model terms: 53. Terms eligible: 4. Validation error: 0.00808641.\n",
      "Fold: 1. Boosting step: 69. Model terms: 54. Terms eligible: 4. Validation error: 0.00792974.\n",
      "Fold: 1. Boosting step: 70. Model terms: 55. Terms eligible: 4. Validation error: 0.00781925.\n",
      "Fold: 1. Boosting step: 71. Model terms: 56. Terms eligible: 4. Validation error: 0.00768559.\n",
      "Fold: 1. Boosting step: 72. Model terms: 56. Terms eligible: 4. Validation error: 0.00754819.\n",
      "Fold: 1. Boosting step: 73. Model terms: 57. Terms eligible: 4. Validation error: 0.00743965.\n",
      "Fold: 1. Boosting step: 74. Model terms: 58. Terms eligible: 4. Validation error: 0.00732585.\n",
      "Fold: 1. Boosting step: 75. Model terms: 59. Terms eligible: 4. Validation error: 0.00722322.\n",
      "Fold: 1. Boosting step: 76. Model terms: 60. Terms eligible: 4. Validation error: 0.00709896.\n",
      "Fold: 1. Boosting step: 77. Model terms: 61. Terms eligible: 4. Validation error: 0.00701006.\n",
      "Fold: 1. Boosting step: 78. Model terms: 62. Terms eligible: 4. Validation error: 0.00690332.\n",
      "Fold: 1. Boosting step: 79. Model terms: 63. Terms eligible: 4. Validation error: 0.00681819.\n",
      "Fold: 1. Boosting step: 80. Model terms: 64. Terms eligible: 4. Validation error: 0.00670443.\n",
      "Fold: 1. Boosting step: 81. Model terms: 65. Terms eligible: 4. Validation error: 0.00661459.\n",
      "Fold: 1. Boosting step: 82. Model terms: 66. Terms eligible: 4. Validation error: 0.00653058.\n",
      "Fold: 1. Boosting step: 83. Model terms: 67. Terms eligible: 4. Validation error: 0.00642976.\n",
      "Fold: 1. Boosting step: 84. Model terms: 68. Terms eligible: 4. Validation error: 0.00635653.\n",
      "Fold: 1. Boosting step: 85. Model terms: 69. Terms eligible: 4. Validation error: 0.00627281.\n",
      "Fold: 1. Boosting step: 86. Model terms: 70. Terms eligible: 4. Validation error: 0.0062024.\n",
      "Fold: 1. Boosting step: 87. Model terms: 71. Terms eligible: 4. Validation error: 0.00610927.\n",
      "Fold: 1. Boosting step: 88. Model terms: 72. Terms eligible: 4. Validation error: 0.00603908.\n",
      "Fold: 1. Boosting step: 89. Model terms: 73. Terms eligible: 4. Validation error: 0.00596924.\n",
      "Fold: 1. Boosting step: 90. Model terms: 74. Terms eligible: 4. Validation error: 0.00588629.\n",
      "Fold: 1. Boosting step: 91. Model terms: 75. Terms eligible: 4. Validation error: 0.0058252.\n",
      "Fold: 1. Boosting step: 92. Model terms: 76. Terms eligible: 4. Validation error: 0.00573236.\n",
      "Fold: 1. Boosting step: 93. Model terms: 77. Terms eligible: 4. Validation error: 0.00566878.\n",
      "Fold: 1. Boosting step: 94. Model terms: 78. Terms eligible: 4. Validation error: 0.00560752.\n",
      "Fold: 1. Boosting step: 95. Model terms: 79. Terms eligible: 4. Validation error: 0.00553488.\n",
      "Fold: 1. Boosting step: 96. Model terms: 80. Terms eligible: 4. Validation error: 0.00548073.\n",
      "Fold: 1. Boosting step: 97. Model terms: 81. Terms eligible: 4. Validation error: 0.00542647.\n",
      "Fold: 1. Boosting step: 98. Model terms: 82. Terms eligible: 4. Validation error: 0.00537373.\n",
      "Fold: 1. Boosting step: 99. Model terms: 83. Terms eligible: 4. Validation error: 0.00529199.\n",
      "Fold: 1. Boosting step: 100. Model terms: 83. Terms eligible: 4. Validation error: 0.00522842.\n",
      "Fold: 1. Boosting step: 101. Model terms: 84. Terms eligible: 4. Validation error: 0.00517985.\n",
      "Fold: 1. Boosting step: 102. Model terms: 85. Terms eligible: 4. Validation error: 0.00512499.\n",
      "Fold: 1. Boosting step: 103. Model terms: 86. Terms eligible: 4. Validation error: 0.0050779.\n",
      "Fold: 1. Boosting step: 104. Model terms: 87. Terms eligible: 4. Validation error: 0.00501803.\n",
      "Fold: 1. Boosting step: 105. Model terms: 88. Terms eligible: 4. Validation error: 0.00497619.\n",
      "Fold: 1. Boosting step: 106. Model terms: 89. Terms eligible: 4. Validation error: 0.00493016.\n",
      "Fold: 1. Boosting step: 107. Model terms: 89. Terms eligible: 4. Validation error: 0.00486218.\n",
      "Fold: 1. Boosting step: 108. Model terms: 90. Terms eligible: 4. Validation error: 0.0048195.\n",
      "Fold: 1. Boosting step: 109. Model terms: 91. Terms eligible: 4. Validation error: 0.0047758.\n",
      "Fold: 1. Boosting step: 110. Model terms: 92. Terms eligible: 4. Validation error: 0.00473465.\n",
      "Fold: 1. Boosting step: 111. Model terms: 93. Terms eligible: 4. Validation error: 0.00468028.\n",
      "Fold: 1. Boosting step: 112. Model terms: 94. Terms eligible: 4. Validation error: 0.00464421.\n",
      "Fold: 1. Boosting step: 113. Model terms: 95. Terms eligible: 4. Validation error: 0.00460616.\n",
      "Fold: 1. Boosting step: 114. Model terms: 96. Terms eligible: 4. Validation error: 0.00454678.\n",
      "Fold: 1. Boosting step: 115. Model terms: 97. Terms eligible: 4. Validation error: 0.00451002.\n",
      "Fold: 1. Boosting step: 116. Model terms: 98. Terms eligible: 4. Validation error: 0.00446299.\n",
      "Fold: 1. Boosting step: 117. Model terms: 99. Terms eligible: 4. Validation error: 0.0044305.\n",
      "Fold: 1. Boosting step: 118. Model terms: 100. Terms eligible: 4. Validation error: 0.00439154.\n",
      "Fold: 1. Boosting step: 119. Model terms: 101. Terms eligible: 4. Validation error: 0.0043597.\n",
      "Fold: 1. Boosting step: 120. Model terms: 102. Terms eligible: 4. Validation error: 0.00431556.\n",
      "Fold: 1. Boosting step: 121. Model terms: 103. Terms eligible: 4. Validation error: 0.00428537.\n",
      "Fold: 1. Boosting step: 122. Model terms: 104. Terms eligible: 4. Validation error: 0.00425283.\n",
      "Fold: 1. Boosting step: 123. Model terms: 105. Terms eligible: 4. Validation error: 0.0042029.\n",
      "Fold: 1. Boosting step: 124. Model terms: 106. Terms eligible: 4. Validation error: 0.00417455.\n",
      "Fold: 1. Boosting step: 125. Model terms: 106. Terms eligible: 4. Validation error: 0.00414552.\n",
      "Fold: 1. Boosting step: 126. Model terms: 107. Terms eligible: 4. Validation error: 0.00411756.\n",
      "Fold: 1. Boosting step: 127. Model terms: 108. Terms eligible: 4. Validation error: 0.0040778.\n",
      "Fold: 1. Boosting step: 128. Model terms: 108. Terms eligible: 4. Validation error: 0.00403549.\n",
      "Fold: 1. Boosting step: 129. Model terms: 109. Terms eligible: 4. Validation error: 0.00400991.\n",
      "Fold: 1. Boosting step: 130. Model terms: 109. Terms eligible: 4. Validation error: 0.00397872.\n",
      "Fold: 1. Boosting step: 131. Model terms: 110. Terms eligible: 4. Validation error: 0.0039536.\n",
      "Fold: 1. Boosting step: 132. Model terms: 110. Terms eligible: 4. Validation error: 0.0039189.\n",
      "Fold: 1. Boosting step: 133. Model terms: 110. Terms eligible: 4. Validation error: 0.00389555.\n",
      "Fold: 1. Boosting step: 134. Model terms: 111. Terms eligible: 4. Validation error: 0.00386984.\n",
      "Fold: 1. Boosting step: 135. Model terms: 112. Terms eligible: 4. Validation error: 0.0038325.\n",
      "Fold: 1. Boosting step: 136. Model terms: 113. Terms eligible: 4. Validation error: 0.00381014.\n",
      "Fold: 1. Boosting step: 137. Model terms: 113. Terms eligible: 4. Validation error: 0.00378398.\n",
      "Fold: 1. Boosting step: 138. Model terms: 114. Terms eligible: 4. Validation error: 0.00376207.\n",
      "Fold: 1. Boosting step: 139. Model terms: 114. Terms eligible: 4. Validation error: 0.00373075.\n",
      "Fold: 1. Boosting step: 140. Model terms: 115. Terms eligible: 4. Validation error: 0.00371073.\n",
      "Fold: 1. Boosting step: 141. Model terms: 116. Terms eligible: 4. Validation error: 0.00368839.\n",
      "Fold: 1. Boosting step: 142. Model terms: 117. Terms eligible: 4. Validation error: 0.00366031.\n",
      "Fold: 1. Boosting step: 143. Model terms: 118. Terms eligible: 4. Validation error: 0.0036409.\n",
      "Fold: 1. Boosting step: 144. Model terms: 118. Terms eligible: 4. Validation error: 0.00361865.\n",
      "Fold: 1. Boosting step: 145. Model terms: 119. Terms eligible: 4. Validation error: 0.00359963.\n",
      "Fold: 1. Boosting step: 146. Model terms: 120. Terms eligible: 4. Validation error: 0.00356729.\n",
      "Fold: 1. Boosting step: 147. Model terms: 120. Terms eligible: 4. Validation error: 0.00354998.\n",
      "Fold: 1. Boosting step: 148. Model terms: 121. Terms eligible: 4. Validation error: 0.00353058.\n",
      "Fold: 1. Boosting step: 149. Model terms: 122. Terms eligible: 4. Validation error: 0.00350538.\n",
      "Fold: 1. Boosting step: 150. Model terms: 123. Terms eligible: 4. Validation error: 0.0034886.\n",
      "Fold: 1. Boosting step: 151. Model terms: 124. Terms eligible: 4. Validation error: 0.00347172.\n",
      "Fold: 1. Boosting step: 152. Model terms: 125. Terms eligible: 4. Validation error: 0.00345505.\n",
      "Fold: 1. Boosting step: 153. Model terms: 126. Terms eligible: 4. Validation error: 0.00343672.\n",
      "Fold: 1. Boosting step: 154. Model terms: 126. Terms eligible: 4. Validation error: 0.00340907.\n",
      "Fold: 1. Boosting step: 155. Model terms: 127. Terms eligible: 4. Validation error: 0.00339413.\n",
      "Fold: 1. Boosting step: 156. Model terms: 127. Terms eligible: 4. Validation error: 0.00337541.\n",
      "Fold: 1. Boosting step: 157. Model terms: 128. Terms eligible: 4. Validation error: 0.00336063.\n",
      "Fold: 1. Boosting step: 158. Model terms: 129. Terms eligible: 4. Validation error: 0.00333812.\n",
      "Fold: 1. Boosting step: 159. Model terms: 130. Terms eligible: 4. Validation error: 0.0033211.\n",
      "Fold: 1. Boosting step: 160. Model terms: 131. Terms eligible: 4. Validation error: 0.00330681.\n",
      "Fold: 1. Boosting step: 161. Model terms: 131. Terms eligible: 4. Validation error: 0.00328268.\n",
      "Fold: 1. Boosting step: 162. Model terms: 132. Terms eligible: 4. Validation error: 0.00326701.\n",
      "Fold: 1. Boosting step: 163. Model terms: 133. Terms eligible: 4. Validation error: 0.00325399.\n",
      "Fold: 1. Boosting step: 164. Model terms: 133. Terms eligible: 4. Validation error: 0.00324002.\n",
      "Fold: 1. Boosting step: 165. Model terms: 134. Terms eligible: 4. Validation error: 0.00322699.\n",
      "Fold: 1. Boosting step: 166. Model terms: 135. Terms eligible: 4. Validation error: 0.00320734.\n",
      "Fold: 1. Boosting step: 167. Model terms: 135. Terms eligible: 4. Validation error: 0.00319271.\n",
      "Fold: 1. Boosting step: 168. Model terms: 136. Terms eligible: 4. Validation error: 0.00317947.\n",
      "Fold: 1. Boosting step: 169. Model terms: 137. Terms eligible: 4. Validation error: 0.0031618.\n",
      "Fold: 1. Boosting step: 170. Model terms: 138. Terms eligible: 4. Validation error: 0.00315025.\n",
      "Fold: 1. Boosting step: 171. Model terms: 138. Terms eligible: 4. Validation error: 0.00313848.\n",
      "Fold: 1. Boosting step: 172. Model terms: 139. Terms eligible: 4. Validation error: 0.00312693.\n",
      "Fold: 1. Boosting step: 173. Model terms: 140. Terms eligible: 4. Validation error: 0.00310634.\n",
      "Fold: 1. Boosting step: 174. Model terms: 141. Terms eligible: 4. Validation error: 0.00309559.\n",
      "Fold: 1. Boosting step: 175. Model terms: 142. Terms eligible: 4. Validation error: 0.00308518.\n",
      "Fold: 1. Boosting step: 176. Model terms: 142. Terms eligible: 4. Validation error: 0.00307188.\n",
      "Fold: 1. Boosting step: 177. Model terms: 143. Terms eligible: 4. Validation error: 0.00306155.\n",
      "Fold: 1. Boosting step: 178. Model terms: 144. Terms eligible: 4. Validation error: 0.00304541.\n",
      "Fold: 1. Boosting step: 179. Model terms: 145. Terms eligible: 4. Validation error: 0.00303386.\n",
      "Fold: 1. Boosting step: 180. Model terms: 146. Terms eligible: 4. Validation error: 0.00302386.\n",
      "Fold: 1. Boosting step: 181. Model terms: 146. Terms eligible: 4. Validation error: 0.00300624.\n",
      "Fold: 1. Boosting step: 182. Model terms: 146. Terms eligible: 4. Validation error: 0.00299532.\n",
      "Fold: 1. Boosting step: 183. Model terms: 147. Terms eligible: 4. Validation error: 0.00298513.\n",
      "Fold: 1. Boosting step: 184. Model terms: 148. Terms eligible: 4. Validation error: 0.00297403.\n",
      "Fold: 1. Boosting step: 185. Model terms: 149. Terms eligible: 4. Validation error: 0.00296523.\n",
      "Fold: 1. Boosting step: 186. Model terms: 149. Terms eligible: 4. Validation error: 0.00295627.\n",
      "Fold: 1. Boosting step: 187. Model terms: 150. Terms eligible: 4. Validation error: 0.00294208.\n",
      "Fold: 1. Boosting step: 188. Model terms: 151. Terms eligible: 4. Validation error: 0.00293295.\n",
      "Fold: 1. Boosting step: 189. Model terms: 152. Terms eligible: 4. Validation error: 0.00291804.\n",
      "Fold: 1. Boosting step: 190. Model terms: 153. Terms eligible: 4. Validation error: 0.00291005.\n",
      "Fold: 1. Boosting step: 191. Model terms: 153. Terms eligible: 4. Validation error: 0.00290166.\n",
      "Fold: 1. Boosting step: 192. Model terms: 154. Terms eligible: 4. Validation error: 0.0028936.\n",
      "Fold: 1. Boosting step: 193. Model terms: 155. Terms eligible: 4. Validation error: 0.00288089.\n",
      "Fold: 1. Boosting step: 194. Model terms: 156. Terms eligible: 4. Validation error: 0.00287296.\n",
      "Fold: 1. Boosting step: 195. Model terms: 156. Terms eligible: 4. Validation error: 0.00286053.\n",
      "Fold: 1. Boosting step: 196. Model terms: 157. Terms eligible: 4. Validation error: 0.00285303.\n",
      "Fold: 1. Boosting step: 197. Model terms: 158. Terms eligible: 4. Validation error: 0.00284373.\n",
      "Fold: 1. Boosting step: 198. Model terms: 158. Terms eligible: 4. Validation error: 0.00283634.\n",
      "Fold: 1. Boosting step: 199. Model terms: 158. Terms eligible: 4. Validation error: 0.00282682.\n",
      "Fold: 1. Boosting step: 200. Model terms: 158. Terms eligible: 4. Validation error: 0.00281756.\n",
      "Model terms: 158. Terms available in final boosting step: 4.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from interpret.glassbox import APLRClassifier, ExplainableBoostingClassifier, APLRRegressor\n",
    "from lightgbm import train\n",
    "model_aplr = APLRRegressor(random_state=1, m=200, max_interaction_level=0, verbosity=2, min_observations_in_split=1, v = 0.1, bins=2000, cv_folds=2)\n",
    "# model_aplr = ExplainableBoostingClassifier(random_state=42)\n",
    "model_aplr.fit(dataset[features].values, dataset[\"choice\"], X_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f1ab466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.002832630307732215\n"
     ]
    }
   ],
   "source": [
    "from rumboost.metrics import cross_entropy\n",
    "# preds = model_aplr.predict_class_probabilities(dataset_test[features[0]].values)\n",
    "if n_utility > 1:\n",
    "    preds = model_aplr.predict_proba(dataset_test[features].values)\n",
    "    print(cross_entropy(preds, dataset_test[\"choice\"].astype(int).values))\n",
    "else:\n",
    "    preds = model_aplr.predict(dataset_test[features].values)\n",
    "    mse = np.mean((preds - dataset_test[\"choice\"].values)**2)\n",
    "    print(f\"Mean squared error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f412ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- http://127.0.0.1:7001/1867676515152/ -->\n",
       "<iframe src=\"http://127.0.0.1:7001/1867676515152/\" width=100% height=800 frameBorder=\"0\"></iframe>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from interpret import show\n",
    "show(model_aplr.explain_global())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1318f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "old_rumboost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
